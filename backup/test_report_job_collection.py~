#!/usr/bin/env python
"""@package unittests.reporting.test_report_job_collection
Unit Tests for @ref unapireporting.datamodel.report_job_collection
"""

# For Python 2.6 we use our own copy of unittest2
# For Python 2.7 and later, the normal unittest suffices
try:
    import unittest2 as unittest
except ImportError:
    import unittest

# Standard library
import logging
import shlex
import textwrap
import re
from datetime import datetime

# 3rd party
# For Python 2.6 we use our own copy of unittest2
# For Python 2.7 and later, the normal unittest suffices
from unapiflaskapp import errors

try:
    import unittest2 as unittest
except ImportError:
    import unittest

import mock
import werkzeug.exceptions

# UNAPI Definitions
from unapiflaskapp.datamodel.formats import SPLUNK_DATE_FORMAT
from unapicommon.definitions import JsonKeys, Profiles, LinkRel, TEST_LANG_CODE

# UNAPI / Splunk
import unapireporting.splunk.index_retention
from unapireporting.datamodel import report_job_collection, report_job_item
from unapireporting.datamodel.report_item import ReportIds, DEFAULT_SPLIT_LIMIT
from unapiserverlib import cdndb

# Unit tests
from unittests import fake_report_results
from unittests.fake_reporting import FakeCdnDbForReporting
from unittests.fake_report_results import report_results, NOW
from unittests.reporting.setup_flask_context_for_reporting import SetupFlaskContext
from unittests.reporting.setup_unapi_for_reporting import SetupUnapiFlaskAppForReporting


class TestReportRunBuildCustomerRestriction(SetupUnapiFlaskAppForReporting):
    """Tests for the @ref
    unapireporting.datamodel.report_job_collection.ReportJobCollection._build_customer_restriction method.
    """
    def setUp(self):
        patch = mock.patch.object(report_job_collection.ReportJobCollection, 'get_cdn_db',
                                  new=FakeCdnDbForReporting)
        patch.start()
        self.addCleanup(patch.stop)

    @mock.patch.object(FakeCdnDbForReporting, 'get_orgs_with_sites')
    def test_normal(self, mock_db_method):
        """Normal case where the user has some services, return as an OR list
        """
        # Prepare mock result from Db
        mock_db_method.return_value = [4,5,2]

        # Call code under test
        obj = report_job_collection.ReportJobCollection(report_id='traffic')
        query_parts = obj._build_customer_restriction({})

        # Check result
        self.assertIn('customer_restriction', query_parts)
        restriction = query_parts['customer_restriction']
        self.assertEquals(restriction, '(host=4 OR host=5 OR host=2)')

    @mock.patch.object(FakeCdnDbForReporting, 'get_orgs_with_sites')
    def test_no_orgs_with_sites(self, mock_db_method):
        """User has no services, should return an "always False" clause.

        Must not give the empty string, which wouldn't restrict the results at
        all and give data for all customers accidentally!
        """
        # Prepare mock result from Db
        mock_db_method.return_value = []

        # Call code under test
        obj = report_job_collection.ReportJobCollection(report_id='traffic')
        query_parts = obj._build_customer_restriction({})

        # Check result
        self.assertIn('customer_restriction', query_parts)
        restriction = query_parts['customer_restriction']
        self.assertEquals(restriction, 'host="impossible"')

    @mock.patch.object(FakeCdnDbForReporting, 'get_orgs_with_sites')
    def test_no_roles_for_user(self, mock_db_method):
        """Test a customer restriction for the case where the user has no
        roles for any Org.

        Should give a HTTP 401.
        """
        # Prepare mock result from Db
        mock_db_method.side_effect = cdndb.AuthorizationError

        # Call code under test and check it throws the right error
        with self.assertRaises(werkzeug.exceptions.Unauthorized) as cm:
            obj = report_job_collection.ReportJobCollection(report_id='traffic')
            obj._build_customer_restriction({})
        err = cm.exception
        self.assertEquals(err.code, 401)

    @mock.patch.object(report_job_collection.ReportJobCollection,
                       '_parse_organization_parameters')
    @mock.patch.object(FakeCdnDbForReporting, 'get_orgs_with_sites')
    def test_normal_and_filter(self, mock_db_method, mock_job_collection_method):
        """Normal case where the user has some services and is filtered by
        organisation in the request.
        """
        # Prepare mock result from Db
        mock_db_method.return_value = [4,5,2]

        # Prepare mock result from private method as if filter-by for ids 1 and
        # 2 had been requested.
        mock_job_collection_method.return_value = set([1, 2])

        # Call code under test
        obj = report_job_collection.ReportJobCollection(report_id='traffic')
        query_parts = obj._build_customer_restriction({})

        # Check result
        self.assertIn('customer_restriction', query_parts)
        restriction = query_parts['customer_restriction']
        self.assertEquals(restriction, '(host=2)')

    @mock.patch.object(report_job_collection.ReportJobCollection,
                       '_parse_organization_parameters')
    @mock.patch.object(FakeCdnDbForReporting, 'get_orgs_with_sites')
    def test_no_orgs_with_sites_and_filter(self, mock_db_method,
                                           mock_job_collection_method):
        """User has no services, should return an "always False" clause.

        Must not give the empty string, which wouldn't restrict the results at
        all and give data for all customers accidentally!
        """
        # Prepare mock result from Db
        mock_db_method.return_value = []

        # Prepare mock result from private method as if filter-by for ids 1 and
        # 2 had been requested.
        mock_job_collection_method.return_value = set([1, 2])

        # Call code under test
        obj = report_job_collection.ReportJobCollection(report_id='traffic')
        query_parts = obj._build_customer_restriction({})

        # Check result
        self.assertIn('customer_restriction', query_parts)
        restriction = query_parts['customer_restriction']
        self.assertEquals(restriction, 'host="impossible"')


class TestAddTimeRangeWarning(SetupFlaskContext):
    """Tests for the
    @ref unapireporting.datamodel.report_job_collection.ReportJobCollection._add_time_range_warning method.
    """

    def check_add_time_range_warning(self, peak_query, index_resolution, warning):
        """Helper method to test _add_time_range_warning.
        The time range can be shortened when there is insufficient data in the
        index for the time range. The time range is shortened when either a peak
        or percentile metric is used (both use peak queries) or a medium or high
        resolution time resolution is used. This method checks that the
        appropriate warning test is generated.

        @param peak_query - a peak query could cause the time range to be
                            shortened, a non-peak (summing query) should not.
        @param index_resolution - the index resolution; medium or high could
                                  cause the time range to be shortened, a low
                                  resolution should not.
        @param warning - the expected warning in the job metadata.
        """
        # Create a object of the class to which the method belongs
        obj = report_job_collection.ReportJobCollection(report_id='traffic')
        # Check warning message text is empty before a warning is added
        self.assertEqual(obj.metadata.warnings, [], "Metadata should be an empty list.")
        # Setup the object
        date = datetime(2014,9,26)
        obj._peak_query = peak_query
        # Call code under test
        obj._add_time_range_warning(index_resolution, date)
        # Check warning message text and request ID is as expected
        self.assertEqual(obj.metadata.warnings, [(warning, 'a_request_id')])

    def test_metric_warning(self):
        """Test selected metric causing time range to be shortened."""
        self.check_add_time_range_warning(True,
                                          report_job_collection.LOW_RESOLUTION,
                                          "Your requested time range has been shortened because the selected metric is only available from 9/26/14 12:00 AM.")

    def test_time_resolution_warning(self):
        """Test selected time resolution causing time range to be shortened."""
        self.check_add_time_range_warning(False,
                                          report_job_collection.MEDIUM_RESOLUTION,
                                          "Your requested time range has been shortened because the selected time resolution is only available from 9/26/14 12:00 AM.")

    def test_time_resolution_and_metric_warning(self):
        """Test selected metric and time resolution causing time range to be shortened."""
        self.check_add_time_range_warning(True,
                                          report_job_collection.HIGH_RESOLUTION,
                                          "Your requested time range has been shortened because the selected metric and time resolution is only available from 9/26/14 12:00 AM.")

    def test_report_warning(self):
        """Test an unknown reason caused the time range to be shortened."""
        self.check_add_time_range_warning(False,
                                          report_job_collection.LOW_RESOLUTION,
                                          "Your requested time range has been shortened because the selected report is only available from 9/26/14 12:00 AM.")


class TestDefaultTimeResolution(SetupFlaskContext):
    """Tests which check the correct time resolution is used for each report.
    """

    def check_default_time_resolution(self, report_id, time_resolution, time_split):
        """Helper method to test default time resolution for a report.
        @param report_id - report ID for which to check the default.
        @param time_resolution - expected time resolution
        @param time_split - whether or not a time split is expected
        """

        # Call code under test
        obj = report_job_collection.ReportJobCollection(report_id=report_id)
        # Use a fixed value for now - 28/02/2014 10:38:30 AM
        obj._now = fake_report_results.NOW

        # Setup the member variable for whether a peak query is being use
        obj._peak_query = False

        # Setup the member variable for whether or not the metric and split are
        # in the saved search and summary index.
        obj._metric_in_saved_search = True
        obj._metric_in_summary_index = True
        obj._split_in_saved_search = True

        query_parts = obj._parse_time_parameters({})

        # Check the time resolution selected
        self.assertEqual(query_parts['time_resolution'], time_resolution, 'The returned time_resolution is incorrect')
        # Check whether or not a time split has been selected
        self.assertEqual(obj._time_split, time_split, 'The returned time_split is incorrect')


    def test_traffic_report_default(self):
        """Check the Traffic report has the correct default time resolution."""
        self.check_default_time_resolution(ReportIds.TRAFFIC, '1d', True)

    def test_request_report_default(self):
        """Check the Request report has the correct default time resolution."""
        self.check_default_time_resolution(ReportIds.REQUEST, '1d', True)

    def test_cache_hit_report_default(self):
        """Check the Cache hit report has the correct default time resolution."""
        self.check_default_time_resolution(ReportIds.CACHE_HIT, '1d', False)

    def test_http_status_report_default(self):
        """Check the HTTP status code report has the correct default time resolution."""
        self.check_default_time_resolution(ReportIds.HTTP_STATUS_CODE, '1d', False)

    def test_content_popularity_report_default(self):
        """Check the Content popularity analysis report has the correct default time resolution."""
        self.check_default_time_resolution(ReportIds.CONTENT_POPULARITY, '1d', True)

    def test_tier_analysis_report_default(self):
        """Check the Tier analysis report has the correct default time resolution."""
        self.check_default_time_resolution(ReportIds.TIER_ANALYSIS, '1d', True)


class ParseTimeParametersMixin(SetupFlaskContext):
    """Base class for testing time parsing."""

    @mock.patch.object(report_job_collection, 'saved_search_available')
    def invoke_parse_time_parameters_no_saved_search(self, time_resolution, peak_query,
                                                     start_time, end_time, mock_method):
        # Have saved_search_available() return False
        mock_method.return_value = False

        self.invoke_parse_time_parameters(time_resolution, peak_query, start_time, end_time)


    def invoke_parse_time_parameters(self, time_resolution, peak_query,
                                     start_time, end_time,
                                     metric_in_saved_search=True,
                                     metric_in_summary_index=True,
                                     split_in_saved_search=True):
        """Create a report job collection and call @c _parse_time_parameters.
        This method stores the object and query parts as returned by @c
        _parse_time_parameters in member variables.
        @param time_resolution - parameter for method under test
        @param peak_query - value of member variable @c _peak_query
        @param start_time - parameter for method under test
        @param end_time - parameter for method under test
        @param metric_in_saved_search - value of member variable @c _metric_in_saved_search
        @param metric_in_summary_index - value of member variable @c _metric_in_summary_index
        @param split_in_saved_search - value of member variable @c _split_in_saved_search
        """

        # Create a report job collection object
        self.obj = report_job_collection.ReportJobCollection(report_id='traffic')

        # Use a fixed value for now - 28/02/2014 10:38:30 AM
        self.obj._now = NOW

        # Setup the member variable for whether a peak query is being use
        self.obj._peak_query = peak_query

        # Setup the member variable for whether or not the metric is in the
        # saved search.
        self.obj._metric_in_saved_search = metric_in_saved_search

        # Setup the member variable for whether or not the metric is in the
        # summary index.
        self.obj._metric_in_summary_index = metric_in_summary_index

        # Setup the member variable for whether or not the split is in the
        # saved search.
        self.obj._split_in_saved_search = split_in_saved_search

        # Configure the parameters handled by _parse_time_parameters
        params = {
            JsonKeys.START_TIME: start_time,
            JsonKeys.END_TIME: end_time,
            JsonKeys.TIME_RESOLUTION: time_resolution
        }
        # Call the method under test and store its return value
        self.query_parts = self.obj._parse_time_parameters(params)

        self.snap_to_start_time = self.query_parts[ report_job_collection.SNAP_TO]

        # Check the labels are correct
        self.assertEqual(self.obj.metadata.time_resolution, time_resolution, "Label is incorrect")
        self.assertEqual(self.obj.metadata.start_time, start_time, "Label is incorrect")
        self.assertEqual(self.obj.metadata.end_time, end_time, "Label is incorrect")


    def check_parse_time_parameters(self, time_resolution, time_label, snap_to_start_time,
                                    chart_start_time, chart_end_time,
                                    main_start_time=None, main_end_time=None,
                                    main_start_time2=None, main_end_time2=None,
                                    saved_search_start_time=None, saved_search_end_time=None,
                                    summary_start_time=None, summary_end_time=None):
        """Check the values return by the method @c _parse_time_parameters.
         @param time_resolution - time resolution to be passed to Splunk
         @param time_label - chart label to be passed to Splunk
         @param snap_to_start_time - the start time to which the results should snap to
         @param chart_start_time - overall start time for all queries - should
                be the same as snap_to_start_time but in a different format
         @param chart_end_time - overall end time for all queries
         @param main_start_time - optional start time for the main index query
         @param main_end_time - optional end time for the main index query
         @param main_start_time2 - optional start time for a second main index query
         @param main_end_time2 - optional end time for a second main index query
         @param saved_search_start_time - optional start time for the ssaved search query
         @param saved_search_end_time - optional end time for the saved search query
         @param summary_start_time - optional start time for the summary index query
         @param summary_end_time - optional end time for the summary index query
        """

        # These should always be defined
        self.assertEquals(self.query_parts['chart_start_time'], chart_start_time, "Query part chart_start_time is incorrect")
        self.assertEquals(self.query_parts['chart_end_time'], chart_end_time, "Query part chart_end_time is incorrect")

        # Main index isn't always used
        if main_start_time is None and main_end_time is None:
            # Check member variable correctly sets main index usage
            self.assertFalse(self.obj._use_main_index, "Member variable _use_main_index should be False")
            # Check query parts are absent
            self.assertNotIn('main_start_time', self.query_parts, "Query part main_start_time should not be defined")
            self.assertNotIn('main_end_time', self.query_parts, "Query part main_end_time should not be defined")
        else:
            # Check member variable correctly sets main index usage
            self.assertTrue(self.obj._use_main_index, "Member variable _use_main_index should be True")
            self.assertEquals(self.query_parts['main_start_time'], main_start_time, "Query part main_start_time is incorrect")
            self.assertEquals(self.query_parts['main_end_time'], main_end_time, "Query part main_end_time is incorrect")

        # Main index is sometimes used twice
        if main_start_time2 is None and main_end_time2 is None:
            # Check member variable correctly sets main index usage
            self.assertFalse(self.obj._use_main_index_twice, "Member variable _use_main_index_twice should be False")
            # Check query parts are absent
            self.assertNotIn('main_start_time2', self.query_parts, "Query part main_start_time2 should not be defined")
            self.assertNotIn('main_end_time2', self.query_parts, "Query part main_end_time2 should not be defined")
        else:
            # Check member variable correctly sets main index usage
            self.assertTrue(self.obj._use_main_index_twice, "Member variable _use_main_index_twice should be True")
            self.assertEquals(self.query_parts['main_start_time2'], main_start_time2, "Query part main_start_time2 is incorrect")
            self.assertEquals(self.query_parts['main_end_time2'], main_end_time2, "Query part main_end_time2 is incorrect")

        # Saved search isn't always used
        if saved_search_start_time is None and saved_search_end_time is None:
            # Check member variable correctly sets summary index usage
            self.assertFalse(self.obj._use_saved_search, "Member variable _use_saved_search should be False")
            # Check query parts are absent
            self.assertNotIn('saved_search_start_time', self.query_parts, "Query part saved_search_start_time should not be defined")
            self.assertNotIn('saved_search_end_time', self.query_parts, "Query part saved_search_end_time should not be defined")
        else:
            # Check member variable correctly sets summary index usage
            self.assertTrue(self.obj._use_saved_search, "Member variable _use_saved_search should be True")
            # Convert from epoch time (timestamp) to a datetime as this is what
            # is used in this part of the query.
            query_start_datetime = datetime.fromtimestamp(self.query_parts['saved_search_start_time'])
            query_end_datetime = datetime.fromtimestamp(self.query_parts['saved_search_end_time'])
            # Convert to a datetime for easy comparison
            expected_start_datetime = datetime.strptime(saved_search_start_time, SPLUNK_DATE_FORMAT)
            expected_end_datetime = datetime.strptime(saved_search_end_time, SPLUNK_DATE_FORMAT)
            # Check query parts are present and correct
            self.assertEquals(query_start_datetime, expected_start_datetime, "Query part saved_search_start_time is incorrect")
            self.assertEquals(query_end_datetime, expected_end_datetime, "Query part saved_search_end_time is incorrect")

        # Summary index isn't always used
        if summary_start_time is None and summary_end_time is None:
            # Check member variable correctly sets summary index usage
            self.assertFalse(self.obj._use_summary_index, "Member variable _use_summary_index should be False")
            # Check query parts are absent
            self.assertNotIn('summary_start_time', self.query_parts, "Query part summary_start_time should not be defined")
            self.assertNotIn('summary_end_time', self.query_parts, "Query part summary_end_time should not be defined")
        else:
            # Check member variable correctly sets summary index usage
            self.assertTrue(self.obj._use_summary_index, "Member variable _use_summary_index should be True")
            # Check query parts are present and correct
            self.assertEquals(self.query_parts['summary_start_time'], summary_start_time, "Query part summary_start_time is incorrect")
            self.assertEquals(self.query_parts['summary_end_time'], summary_end_time, "Query part summary_end_time is incorrect")

        # Check remaining query parts
        self.assertEquals(self.query_parts['time_resolution'], time_resolution, "Query part time_resolution is incorrect")
        self.assertEquals(self.query_parts['time_label'], time_label, "Query part time_label is incorrect")

        # Check time resolution used in tstat query is as expected (minimum
        # resolution should be 1 day to ensure buckets are used correctly).
        if time_resolution in ['1w', '1mon', '3mon', '12mon']:
            tstat_time_resolution = '1d'
        else:
            tstat_time_resolution = time_resolution
        self.assertEquals(self.query_parts['tstat_time_resolution'], tstat_time_resolution, "Query part tstat_time_resolution is incorrect")

        # Check the time format; should now *always* be ISO 8601
        self.assertEquals(self.query_parts['time_format'], "%Y-%m-%dT%H:%M:%SZ", "Query part time_format is incorrect")

        # When the time label is expected e.g. to be "Time" or "Date" then the
        # member variable controlling the chart representation should be set to
        # disable splitting by time.
        if time_label is None:
            self.assertFalse(self.obj._time_split, "If a time resolution is defined this should be set to False")
        else:
            self.assertTrue(self.obj._time_split, "If a time resolution is defined this should be set to True")

        # Check the labels are correct
        self.assertEqual(self.obj.metadata.split_label, time_label, "Label is incorrect")

        # Check that the snap time is correct
        self.assertEqual(self.snap_to_start_time, snap_to_start_time, "Snap to start time is incorrect")

    def check_exception(self, exception_context, property_list, decription):
        """Check the contents of a PropertyValueError exception.
        @param exception_context - conetx from with self.assertRaises...
        @param property_list - espected property list in exception
        @param decription - expected description in exception.
        """
        err = exception_context.exception
        self.assertEquals(err.code, 400)
        self.assertEquals(err.error_code, 4004)
        self.assertEquals(err.error_description, decription)
        self.assertEquals(err.error_details['property'], property_list)



class TestParseTimeParameters(ParseTimeParametersMixin):
    """Tests for the generation of the query parts that control the time range.
    This class checks the time handling of the following method:
    - @ref unapireporting.datamodel.report_job_collection.ReportJobCollection._parse_time_parameters

    Most edge cases actually tested by the following classes:
    - @ref TestGenericReportParameterValidation
    - @ref TestCDNReportingReportParameterValidation.
    - @ref TestCDNInsightsReportParameterValidation.

    This suite is mostly to check that the returned values are appropriate for
    passing to Splunk.

    @note These tests use the standard test data which reports the main index as
          having a 45 day retention period.
    """



    def test_low_res_sum_2hours(self):
        """Test low resolution, sum query, for last 2 hours.
        Queries for low time resolutions can have up to three parts. The
        crosses on the following diagram illustrate the expected indexes that
        will be queried:
        @verbatim
                    Summary Index             |Saved Search | Main Index |
        ______________________________________|_____________|xxxxxxxxxxxx|
        <-Indefinite                         -3d           -2h          Now
        @endverbatim
        """
        self.invoke_parse_time_parameters("1day", False, "-2hours", "now")
        self.check_parse_time_parameters('1d', 'Date', '2014-02-28T00:00:00Z',
                                         chart_start_time='02/28/2014:00:00:00', chart_end_time='02/28/2014:10:38:30',
                                         main_start_time='02/28/2014:08:00:00', main_end_time='02/28/2014:10:38:30')

    def test_low_res_sum_2hours_no_saved_search(self):
        """Test low resolution, sum query, for last 2 hours with no saved search.
        Queries for low time resolutions can have up to three parts. The
        crosses on the following diagram illustrate the expected indexes that
        will be queried:
        @verbatim
                    Summary Index             |Saved Search | Main Index |
        ______________________________________|_____________|xxxxxxxxxxxx|
        <-Indefinite                         -3d           -2h          Now
        @endverbatim
        """
        self.invoke_parse_time_parameters_no_saved_search("1day", False, "-2hours", "now")
        self.check_parse_time_parameters('1d', 'Date', '2014-02-28T00:00:00Z',
                                         chart_start_time='02/28/2014:00:00:00', chart_end_time='02/28/2014:10:38:30',
                                         main_start_time='02/28/2014:08:00:00', main_end_time='02/28/2014:10:38:30')

    def test_low_res_sum_3hours(self):
        """Test low resolution, sum query, for last 3 hours.
        Queries for low time resolutions can have up to three parts. The
        crosses on the following diagram illustrate the expected indexes that
        will be queried:
        @verbatim
                    Summary Index             |Saved Search | Main Index |
        ______________________________________|___________xx|xxxxxxxxxx__|
        <-Indefinite                         -3d           -2h          Now
        @endverbatim
        """
        self.invoke_parse_time_parameters("1day", False, "-3hours", "0hour")
        self.check_parse_time_parameters('1d', 'Date', '2014-02-28T00:00:00Z',
                                         chart_start_time='02/28/2014:00:00:00', chart_end_time='02/28/2014:10:00:00',
                                         main_start_time='02/28/2014:08:00:00', main_end_time='02/28/2014:10:00:00',
                                         saved_search_start_time='02/28/2014:07:00:00', saved_search_end_time='02/28/2014:08:00:00')

    def test_low_res_sum_3hours_no_saved_search(self):
        """Test low resolution, sum query, for last 3 hours with no saved search.
        Queries for low time resolutions can have up to two parts if the saved
        search artifacts are not available. The crosses on the following diagram
        illustrate the expected indexes that will be queried:
        @verbatim
                    Summary Index             |               Main Index |
        ______________________________________|___________xxxxxxxxxxxxx__|
        <-Indefinite                         -3d                        Now
        @endverbatim

        If saved searches were available the following indexes would have been
        searched.

        @verbatim
                    Summary Index             |Saved Search | Main Index |
        ______________________________________|___________xx|xxxxxxxxxx__|
        <-Indefinite                         -3d           -2h          Now
        @endverbatim
        """
        self.invoke_parse_time_parameters_no_saved_search("1day", False, "-3hours", "0hour")
        self.check_parse_time_parameters('1d', 'Date', '2014-02-28T00:00:00Z',
                                         chart_start_time='02/28/2014:00:00:00', chart_end_time='02/28/2014:10:00:00',
                                         main_start_time='02/28/2014:07:00:00', main_end_time='02/28/2014:10:00:00')

    def test_low_res_sum_4days(self):
        """Test low resolution, sum query, for last 4 days.
        Queries for low time resolutions can have up to three parts. The
        crosses on the following diagram illustrate the expected indexes that
        will be queried:
        @verbatim
                    Summary Index             |Saved Search | Main Index |
        ____________________________________xx|xxxxxxxxxxxxx|xxxxxxxxxx__|
        <-Indefinite                         -3d           -2h          Now
        @endverbatim
        """
        self.invoke_parse_time_parameters("1month", False, "-4days", "0hour")
        self.check_parse_time_parameters('1mon', 'Date', '2014-02-01T00:00:00Z',
                                         chart_start_time='02/01/2014:00:00:00', chart_end_time='02/28/2014:10:00:00',
                                         main_start_time='02/28/2014:08:00:00', main_end_time='02/28/2014:10:00:00',
                                         saved_search_start_time='02/25/2014:00:00:00', saved_search_end_time='02/28/2014:08:00:00',
                                         summary_start_time='02/24/2014:00:00:00', summary_end_time='02/25/2014:00:00:00')

    def test_low_res_sum_4days_no_saved_search(self):
        """Test low resolution, sum query, for last 4 days.
        Queries for low time resolutions can have up to three parts, but this
        will only have two as the saved search is unavailable:

        @verbatim
                    Summary Index             |        Main Index        |
        ____________________________________xx|xxxxxxxxxxxxxxxxxxxxxxxx__|
        <-Indefinite                         -3d                        Now
        @endverbatim

        If saved searches were available the following indexes would have been
        searched.

        @verbatim
                    Summary Index             |Saved Search | Main Index |
        ____________________________________xx|xxxxxxxxxxxxx|xxxxxxxxxx__|
        <-Indefinite                         -3d           -2h          Now
        @endverbatim
        """
        self.invoke_parse_time_parameters_no_saved_search("1month", False, "-4days", "0hour")
        self.check_parse_time_parameters('1mon', 'Date', '2014-02-01T00:00:00Z',
                                         chart_start_time='02/01/2014:00:00:00', chart_end_time='02/28/2014:10:00:00',
                                         main_start_time='02/25/2014:00:00:00', main_end_time='02/28/2014:10:00:00',
                                         summary_start_time='02/24/2014:00:00:00', summary_end_time='02/25/2014:00:00:00')

    def test_low_res_sum_3days(self):
        """Test low resolution, sum query, for last 3 days - snap to day.
        Queries for low time resolutions can have up to three parts. The
        crosses on the following diagram illustrate the expected indexes that
        will be queried:
        @verbatim
                    Summary Index             |Saved Search | Main Index |
        ______________________________________|xxxxxxxxxxx__|____________|
        <-Indefinite                         -3d           -2h          Now
        @endverbatim
        """
        self.invoke_parse_time_parameters("1week", False, "-3days", "0days")
        self.check_parse_time_parameters('1w', 'Date', '2014-02-24T00:00:00Z',
                                         chart_start_time='02/24/2014:00:00:00', chart_end_time='02/28/2014:00:00:00',
                                         saved_search_start_time='02/25/2014:00:00:00', saved_search_end_time='02/28/2014:00:00:00')

    def test_low_res_sum_7days(self):
        """Test low resolution, sum query, for last 7 days - snap to week.
        Queries for low time resolutions can have up to three parts. The
        crosses on the following diagram illustrate the expected indexes that
        will be queried:
        @verbatim
                    Summary Index             |Saved Search | Main Index |
        __________________________________xxxx|xxxxxxxxxxx__|____________|
        <-Indefinite                         -3d           -2h          Now
        @endverbatim
        """
        self.invoke_parse_time_parameters("1day", False, "-7days", "0days")
        self.check_parse_time_parameters('1d', 'Date', '2014-02-21T00:00:00Z',
                                         chart_start_time='02/21/2014:00:00:00', chart_end_time='02/28/2014:00:00:00',
                                         saved_search_start_time='02/25/2014:00:00:00', saved_search_end_time='02/28/2014:00:00:00',
                                         summary_start_time='02/21/2014:00:00:00', summary_end_time='02/25/2014:00:00:00')

    def test_low_res_sum_7days_no_saved_search(self):
        """Test low resolution, sum query, for last 7 days - snap to week.
        Queries for low time resolutions can have up to two parts if the saved
        search artifacts are not available. The crosses on the following diagram
        illustrate the expected indexes that will be queried:
        @verbatim
                    Summary Index             |               Main Index |
        __________________________________xxxx|xxxx______________________|
        <-Indefinite                         -3d                        Now
        @endverbatim
        """
        self.invoke_parse_time_parameters_no_saved_search("1day", False, "-7days", "0days")
        self.check_parse_time_parameters('1d', 'Date', '2014-02-21T00:00:00Z',
                                         chart_start_time='02/21/2014:00:00:00', chart_end_time='02/28/2014:00:00:00',
                                         main_start_time='02/25/2014:00:00:00', main_end_time='02/28/2014:00:00:00',
                                         summary_start_time='02/21/2014:00:00:00', summary_end_time='02/25/2014:00:00:00')

    def test_low_res_sum_7days_metric_not_in_saved_search(self):
        """Test low resolution, sum query, for last 7 days - snap to week.
        Queries for low time resolutions can have up to two parts if the metric
        is not part of the saved search. The crosses on the following diagram
        illustrate the expected indexes that will be queried:
        @verbatim
                    Summary Index             |               Main Index |
        __________________________________xxxx|xxxx______________________|
        <-Indefinite                         -3d                        Now
        @endverbatim
        """
        self.invoke_parse_time_parameters("1day", False, "-7days", "0days", metric_in_saved_search=False)
        self.check_parse_time_parameters('1d', 'Date', '2014-02-21T00:00:00Z',
                                         chart_start_time='02/21/2014:00:00:00', chart_end_time='02/28/2014:00:00:00',
                                         main_start_time='02/25/2014:00:00:00', main_end_time='02/28/2014:00:00:00',
                                         summary_start_time='02/21/2014:00:00:00', summary_end_time='02/25/2014:00:00:00')

    def test_low_res_sum_7days_split_not_in_saved_search(self):
        """Test low resolution, sum query, for last 7 days - snap to week.
        Queries for low time resolutions can have up to two parts if the split
        is not part of the saved search. The crosses on the following diagram
        illustrate the expected indexes that will be queried:
        @verbatim
                    Summary Index             |               Main Index |
        __________________________________xxxx|xxxx______________________|
        <-Indefinite                         -3d                        Now
        @endverbatim
        """
        self.invoke_parse_time_parameters("1day", False, "-7days", "0days", split_in_saved_search=False)
        self.check_parse_time_parameters('1d', 'Date', '2014-02-21T00:00:00Z',
                                         chart_start_time='02/21/2014:00:00:00', chart_end_time='02/28/2014:00:00:00',
                                         main_start_time='02/25/2014:00:00:00', main_end_time='02/28/2014:00:00:00',
                                         summary_start_time='02/21/2014:00:00:00', summary_end_time='02/25/2014:00:00:00')

    def test_low_res_sum_7days_metric_and_split_not_in_saved_search(self):
        """Test low resolution, sum query, for last 7 days - snap to week.
        Queries for low time resolutions can have up to two parts if the split
        and metric are not part of the saved search. The crosses on the
        following diagram illustrate the expected indexes that will be queried:
        @verbatim
                    Summary Index             |               Main Index |
        __________________________________xxxx|xxxx______________________|
        <-Indefinite                         -3d                        Now
        @endverbatim
        """
        self.invoke_parse_time_parameters("1day", False, "-7days", "0days",
                                          metric_in_saved_search=False, split_in_saved_search=False)
        self.check_parse_time_parameters('1d', 'Date', '2014-02-21T00:00:00Z',
                                         chart_start_time='02/21/2014:00:00:00', chart_end_time='02/28/2014:00:00:00',
                                         main_start_time='02/25/2014:00:00:00', main_end_time='02/28/2014:00:00:00',
                                         summary_start_time='02/21/2014:00:00:00', summary_end_time='02/25/2014:00:00:00')

    def test_low_res_sum_7_to_14days(self):
        """Test low resolution, sum query, for last 7 to 14 days - snap to month.
        Queries for low time resolutions can have up to three parts. The
        crosses on the following diagram illustrate the expected indexes that
        will be queried:
        @verbatim
                    Summary Index             |Saved Search | Main Index |
        __________________________xxxxxx______|_____________|____________|
        <-Indefinite                         -3d           -2h          Now
        @endverbatim
        """
        self.invoke_parse_time_parameters("1month", False, "-14days", "-7days")
        self.check_parse_time_parameters('1mon', 'Date', '2014-02-01T00:00:00Z',
                                         chart_start_time='02/01/2014:00:00:00', chart_end_time='02/21/2014:00:00:00',
                                         summary_start_time='02/14/2014:00:00:00', summary_end_time='02/21/2014:00:00:00')

    def test_low_res_sum_7_to_14days_no_saved_search(self):
        """Test low resolution, sum query, for last 7 to 14 days - snap to month.
        Queries for low time resolutions can have up to two parts if the saved
        search artifacts are not available. The crosses on the following diagram
        illustrate the expected indexes that will be queried:
        @verbatim
                    Summary Index             |               Main Index |
        __________________________xxxxxx______|__________________________|
        <-Indefinite                         -3d                        Now
        @endverbatim
        """
        self.invoke_parse_time_parameters_no_saved_search("1month", False, "-14days", "-7days")
        self.check_parse_time_parameters('1mon', 'Date', '2014-02-01T00:00:00Z',
                                         chart_start_time='02/01/2014:00:00:00', chart_end_time='02/21/2014:00:00:00',
                                         summary_start_time='02/14/2014:00:00:00', summary_end_time='02/21/2014:00:00:00')

    def test_low_res_sum_365days(self):
        """Test low resolution, sum query, for last 365 days - snap to quarter.
        Queries for low time resolutions can have up to three parts. The
        crosses on the following diagram illustrate the expected indexes that
        will be queried:
        @verbatim
                    Summary Index             |Saved Search | Main Index |
        xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx|xxxxxxxxxxx__|____________|
        <-Indefinite                         -3d           -2h          Now
        @endverbatim
        """
        self.invoke_parse_time_parameters("1quarter", False, "-365days", "0days")
        self.check_parse_time_parameters('3mon', 'Date', '2013-01-01T00:00:00Z',
                                         chart_start_time='01/01/2013:00:00:00', chart_end_time='02/28/2014:00:00:00',
                                         saved_search_start_time='02/25/2014:00:00:00', saved_search_end_time='02/28/2014:00:00:00',
                                         summary_start_time='02/28/2013:00:00:00', summary_end_time='02/25/2014:00:00:00')

    def test_low_res_sum_3years(self):
        """Test low resolution, sum query, for last 3 years - snap to year.
        Queries for low time resolutions can have up to three parts. The
        crosses on the following diagram illustrate the expected indexes that
        will be queried:
        @verbatim
                    Summary Index             |Saved Search | Main Index |
        xxxxxxxxxxxx__________________________|_____________|____________|
        <-Indefinite                         -3d           -2h          Now
        @endverbatim
        """
        self.invoke_parse_time_parameters("1year", False, "-3years", "0years")
        self.check_parse_time_parameters('12mon', 'Date', '2011-01-01T00:00:00Z',
                                         chart_start_time='01/01/2011:00:00:00', chart_end_time='01/01/2014:00:00:00',
                                         summary_start_time='01/01/2011:00:00:00', summary_end_time='01/01/2014:00:00:00')


    def test_medium_res_sum_2hours(self):
        """Test medium resolution, sum query, for last 2 hours.
        Queries for medium time resolutions can have up to three parts. The
        crosses on the following diagram illustrate the expected indexes that
        will be queried:

        @verbatim
         N/A |     Main Index        |     Saved Search     | Main Index |
        _____|_______________________|______________________|xxxxxxxxxxxx|
           -45d                    -7d                     -2h          Now
        @endverbatim
        """
        self.invoke_parse_time_parameters("1hour", False, "-2hours", "now")
        self.check_parse_time_parameters('1h', 'Time', '2014-02-28T08:00:00Z',
                                         chart_start_time='02/28/2014:08:00:00', chart_end_time='02/28/2014:10:38:30',
                                         main_start_time='02/28/2014:08:00:00', main_end_time='02/28/2014:10:38:30')

    def test_medium_res_sum_2hours_no_saved_search(self):
        """Test medium resolution, sum query, for last 2 hours.
        Queries for medium time resolutions can only use the main index if
        saved search artifacts are not available. The crosses on the following
        diagram illustrate the expected indexes that will be queried:

        @verbatim
         N/A |                       Main Index                          |
        _____|_______________________________________________xxxxxxxxxxxx|
           -45d                                                         Now
        @endverbatim
        """
        self.invoke_parse_time_parameters_no_saved_search("1hour", False, "-2hours", "now")
        self.check_parse_time_parameters('1h', 'Time', '2014-02-28T08:00:00Z',
                                         chart_start_time='02/28/2014:08:00:00', chart_end_time='02/28/2014:10:38:30',
                                         main_start_time='02/28/2014:08:00:00', main_end_time='02/28/2014:10:38:30')

    def test_medium_res_sum_3hours(self):
        """Test medium resolution, sum query, for last 3 hours.
        Queries for medium time resolutions can have up to three parts. The
        crosses on the following diagram illustrate the expected indexes that
        will be queried:

        @verbatim
         N/A |     Main Index        |     Saved Search     | Main Index |
        _____|_______________________|____________________xx|xxxxxxxxxx__|
           -45d                    -7d                     -2h          Now
        @endverbatim
        """
        self.invoke_parse_time_parameters("1hour", False, "-3hours", "0hours")
        self.check_parse_time_parameters('1h', 'Time', '2014-02-28T07:00:00Z',
                                         chart_start_time='02/28/2014:07:00:00', chart_end_time='02/28/2014:10:00:00',
                                         main_start_time='02/28/2014:08:00:00', main_end_time='02/28/2014:10:00:00',
                                         saved_search_start_time='02/28/2014:07:00:00', saved_search_end_time='02/28/2014:08:00:00')

    def test_medium_res_sum_1_to_6_days(self):
        """Test medium resolution, sum query, for last 1 to 6 days.
        Queries for medium time resolutions can have up to three parts but this
        query only uses the saved search:

        @verbatim
         N/A |     Main Index        |     Saved Search     | Main Index |
        _____|_______________________|__xxxxxxxxxxxxxxxxxx__|____________|
           -45d                    -7d                     -2h          Now
        @endverbatim
        """
        self.invoke_parse_time_parameters("1hour", False, "-6days", "-1day")
        self.check_parse_time_parameters('1h', 'Time', '2014-02-22T00:00:00Z',
                                         chart_start_time='02/22/2014:00:00:00', chart_end_time='02/27/2014:00:00:00',
                                         saved_search_start_time='02/22/2014:00:00:00', saved_search_end_time='02/27/2014:00:00:00')

    def test_medium_res_sum_1_to_6_days_no_saved_search(self):
        """Test medium resolution, sum query, for last 1 to 6 days.
        Queries for medium time resolutions can have up to three parts but this
        query only uses the main index as the saved search is unavailable:

        @verbatim
         N/A |                     Main Index                            |
        _____|__________________________xxxxxxxxxxxxxxxxxx_______________|
           -45d                                                         Now
        @endverbatim

        If saved searches were available the following indexes would have been
        searched.

        @verbatim
         N/A |     Main Index        |     Saved Search     | Main Index |
        _____|_______________________|__xxxxxxxxxxxxxxxxxx__|____________|
           -45d                    -7d                     -2h          Now
        @endverbatim
        """
        self.invoke_parse_time_parameters_no_saved_search("1hour", False, "-6days", "-1day")
        self.check_parse_time_parameters('1h', 'Time', '2014-02-22T00:00:00Z',
                                         chart_start_time='02/22/2014:00:00:00', chart_end_time='02/27/2014:00:00:00',
                                         main_start_time='02/22/2014:00:00:00', main_end_time='02/27/2014:00:00:00')

    def test_medium_res_sum_8days(self):
        """Test medium resolution, sum query, for last 8 days.
        Queries for medium time resolutions can have up to three parts. The
        crosses on the following diagram illustrate the expected indexes that
        will be queried:

        @verbatim
         N/A |     Main Index        |     Saved Search     | Main Index |
        _____|_____________________xx|xxxxxxxxxxxxxxxxxxxxxx|xxxxxxxxxx__|
           -45d                    -7d                     -2h          Now
        @endverbatim
        """
        self.invoke_parse_time_parameters("1hour", False, "-8days", "0hours")
        self.check_parse_time_parameters('1h', 'Time', '2014-02-20T00:00:00Z',
                                         chart_start_time='02/20/2014:00:00:00', chart_end_time='02/28/2014:10:00:00',
                                         main_start_time='02/28/2014:08:00:00', main_end_time='02/28/2014:10:00:00',
                                         saved_search_start_time='02/21/2014:00:00:00', saved_search_end_time='02/28/2014:08:00:00',
                                         main_start_time2='02/20/2014:00:00:00', main_end_time2='02/21/2014:00:00:00')

    def test_medium_res_sum_46days(self):
        """Test medium resolution, sum query, for last 46 days.
        Queries for medium time resolutions can have up to three parts. The
        crosses on the following diagram illustrate the expected indexes that
        will be queried:

        @verbatim
         N/A |     Main Index        |     Saved Search     | Main Index |
        _____|xxxxxxxxxxxxxxxxxxxxxxx|xxxxxxxxxxxxxxxxxxxxxx|xxxxxxxxxx__|
           -45d                    -7d                     -2h          Now
        @endverbatim

        @note Anything beyond 45 days should be excluded from the query.
        """
        self.invoke_parse_time_parameters("1hour", False, "-46days", "0hours")
        self.check_parse_time_parameters('1h', 'Time', '2014-01-13T10:00:00Z',
                                         chart_start_time='01/13/2014:10:00:00', chart_end_time='02/28/2014:10:00:00',
                                         main_start_time='02/28/2014:08:00:00', main_end_time='02/28/2014:10:00:00',
                                         saved_search_start_time='02/21/2014:00:00:00', saved_search_end_time='02/28/2014:08:00:00',
                                         main_start_time2='01/13/2014:10:38:30', main_end_time2='02/21/2014:00:00:00')

    def test_medium_res_sum_46days_no_saved_search(self):
        """Test medium resolution, sum query, for last 46 days.
        Queries for medium time resolutions can only use the main index if saved
        search artifacts are not available. The crosses on the following diagram
        illustrate the expected indexes that will be queried:

        @verbatim
         N/A |                     Main Index                            |
        _____|xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx__|
           -45d                                                         Now
        @endverbatim

        If saved searches were available the following indexes would have been
        searched.

        @verbatim
         N/A |     Main Index        |     Saved Search     | Main Index |
        _____|xxxxxxxxxxxxxxxxxxxxxxx|xxxxxxxxxxxxxxxxxxxxxx|xxxxxxxxxx__|
           -45d                    -7d                     -2h          Now
        @endverbatim

        @note Anything beyond 45 days should be excluded from the query.
        """
        self.invoke_parse_time_parameters_no_saved_search("1hour", False, "-46days", "0hours")
        self.check_parse_time_parameters('1h', 'Time', '2014-01-13T10:00:00Z',
                                         chart_start_time='01/13/2014:10:00:00', chart_end_time='02/28/2014:10:00:00',
                                         main_start_time='01/13/2014:10:38:30', main_end_time='02/28/2014:10:00:00')

    def test_medium_res_sum_2hours_to_7days(self):
        """Test medium resolution, sum query, from 7 days ago until 2 hours ago.
        Queries for medium time resolutions can have up to three parts. The
        crosses on the following diagram illustrate the expected indexes that
        will be queried:

        @verbatim
         N/A |     Main Index        |     Saved Search     | Main Index |
        _____|_______________________|xxxxxxxxxxxxxxxxxxxxxx|____________|
           -45d                    -7d                     -2h          Now
        @endverbatim
        """
        self.invoke_parse_time_parameters("1hour", False, "-7days", "-2hours")
        self.check_parse_time_parameters('1h', 'Time', '2014-02-21T00:00:00Z',
                                         chart_start_time='02/21/2014:00:00:00', chart_end_time='02/28/2014:08:00:00',
                                         saved_search_start_time='02/21/2014:00:00:00', saved_search_end_time='02/28/2014:08:00:00')

    def test_medium_res_sum_10days(self):
        """Test medium resolution, sum query, for last 10 whole days.
        Queries for medium time resolutions can have up to three parts. The
        crosses on the following diagram illustrate the expected indexes that
        will be queried:

        @verbatim
         N/A |     Main Index        |     Saved Search     | Main Index |
        _____|___________________xxxx|xxxxxxxxxxxxxxxxxxx___|____________|
           -45d                    -7d                     -2h          Now
        @endverbatim
        """
        self.invoke_parse_time_parameters("1hour", False, "-10days", "-0day")
        self.check_parse_time_parameters('1h', 'Time', '2014-02-18T00:00:00Z',
                                         chart_start_time='02/18/2014:00:00:00', chart_end_time='02/28/2014:00:00:00',
                                         saved_search_start_time='02/21/2014:00:00:00', saved_search_end_time='02/28/2014:00:00:00',
                                         main_start_time='02/18/2014:00:00:00', main_end_time='02/21/2014:00:00:00')

    def test_medium_res_sum_10days_no_saved_search(self):
        """Test medium resolution, sum query, for last 10 whole days.
        Queries for medium time resolutions can have up to three parts but this
        only uses one as the saved search is unavailable:

        @verbatim
         N/A |                           Main Index                      |
        _____|___________________xxxxxxxxxxxxxxxxxxxxxxxx________________|
           -45d                                                         Now
        @endverbatim

        If saved searches were available the following indexes would have been
        searched.

        @verbatim
         N/A |     Main Index        |     Saved Search     | Main Index |
        _____|___________________xxxx|xxxxxxxxxxxxxxxxxxx___|____________|
           -45d                    -7d                     -2h          Now
        @endverbatim
        """
        self.invoke_parse_time_parameters_no_saved_search("1hour", False, "-10days", "-0day")
        self.check_parse_time_parameters('1h', 'Time', '2014-02-18T00:00:00Z',
                                         chart_start_time='02/18/2014:00:00:00', chart_end_time='02/28/2014:00:00:00',
                                         main_start_time='02/18/2014:00:00:00', main_end_time='02/28/2014:00:00:00')

    def test_medium_res_sum_7_to_45days(self):
        """Test medium resolution, sum query, for last 7 to 45 days.
        Queries for medium time resolutions can have up to three parts. The
        crosses on the following diagram illustrate the expected indexes that
        will be queried:

        @verbatim
         N/A |     Main Index        |     Saved Search     | Main Index |
        _____|xxxxxxxxxxxxxxxxxxxxxxx|______________________|____________|
           -45d                    -7d                     -2h          Now
        @endverbatim
        """
        self.invoke_parse_time_parameters("1hour", False, "-45days", "-7days")
        self.check_parse_time_parameters('1h', 'Time', '2014-01-14T00:00:00Z',
                                         chart_start_time='01/14/2014:00:00:00', chart_end_time='02/21/2014:00:00:00',
                                         main_start_time='01/14/2014:00:00:00', main_end_time='02/21/2014:00:00:00')



    def test_high_res_sum_7days(self):
        """Test high resolution, sum query, for last 7 days.
        This checks that only the main index is used when a high time resolution
        is selected, when normally both indexes would be used.
        """
        self.invoke_parse_time_parameters("1minute", False, "-7days", "0days")

        # Main index times should cover all of the last 7 days
        # Summary index time should not be present
        self.check_parse_time_parameters('1min', 'Time', '2014-02-21T00:00:00Z',
                                         chart_start_time='02/21/2014:00:00:00', chart_end_time='02/28/2014:00:00:00',
                                         main_start_time='02/21/2014:00:00:00', main_end_time='02/28/2014:00:00:00')

    def test_high_res_sum_7days_no_saved_search(self):
        """Test high resolution, sum query, for last 7 days.
        This checks that only the main index is used when a high time resolution
        is selected, when normally both indexes would be used.
        """
        self.invoke_parse_time_parameters_no_saved_search("1minute", False, "-7days", "0days")

        # Main index times should cover all of the last 7 days
        # Summary index time should not be present
        self.check_parse_time_parameters('1min', 'Time', '2014-02-21T00:00:00Z',
                                         chart_start_time='02/21/2014:00:00:00', chart_end_time='02/28/2014:00:00:00',
                                         main_start_time='02/21/2014:00:00:00', main_end_time='02/28/2014:00:00:00')

    def test_high_res_sum_7_to_14days(self):
        """Test high resolution, sum query, for last 7 to 14 days.
        This checks that only the main index is used when a high time resolution
        is selected, when normally the summary index would be used.
        """
        self.invoke_parse_time_parameters("5minutes", False, "-14days", "-7days")

        # Main index time should cover the last 7 to 14 days
        # Summary index time should not be present
        self.check_parse_time_parameters('5min', 'Time', '2014-02-14T00:00:00Z',
                                         chart_start_time='02/14/2014:00:00:00', chart_end_time='02/21/2014:00:00:00',
                                         main_start_time='02/14/2014:00:00:00', main_end_time='02/21/2014:00:00:00')

    def test_high_res_sum_1_to_45days(self):
        """Test high resolution, sum query, for last 1 to 45 days.
        This checks that a 45 day start time is unaffected by the boundary.
        """
        self.invoke_parse_time_parameters("15minutes", False, "-45days", "-1day")

        # Main index time should cover the last 1 to 45 days
        # Summary index time should not be present
        self.check_parse_time_parameters('15min', 'Time', '2014-01-14T00:00:00Z',
                                         chart_start_time='01/14/2014:00:00:00', chart_end_time='02/27/2014:00:00:00',
                                         main_start_time='01/14/2014:00:00:00', main_end_time='02/27/2014:00:00:00')

    def test_high_res_sum_44_to_46days(self):
        """Test high resolution, sum query, for last 44 to 46 days.
        This checks that the last day in the main index can be searched and that
        the snap time for 30 minutes works ok.
        """
        self.invoke_parse_time_parameters("30minutes", False, "-46days", "-44days")

        # Main index time should cover the last 44 to 45 days
        # Summary index time should not be present
        self.check_parse_time_parameters('30min', 'Time', '2014-01-13T10:30:00Z',
                                         chart_start_time='01/13/2014:10:30:00', chart_end_time='01/15/2014:00:00:00',
                                         main_start_time='01/13/2014:10:38:30', main_end_time='01/15/2014:00:00:00')

    def test_high_res_sum_30_to_50days(self):
        """Test high resolution, sum query, for last 30 to 50 days.
        This checks that only the main index time is limited to the last 45
        days and that the snap time for 15 minutes works ok.
        """
        self.invoke_parse_time_parameters("15minutes", False, "-50days", "-30days")

        # Main index time should cover the last 7 to 45 days
        # Summary index time should not be present
        self.check_parse_time_parameters('15min', 'Time', '2014-01-13T10:30:00Z',
                                         chart_start_time='01/13/2014:10:30:00', chart_end_time='01/29/2014:00:00:00',
                                         main_start_time='01/13/2014:10:38:30', main_end_time='01/29/2014:00:00:00')

    def test_high_res_sum_7_to_90days(self):
        """Test high resolution, sum query, for last 7 to 90 days.
        This checks that only the main index time is limited to the last 45
        days and that the snap time for 5 minutes works ok.
        """
        self.invoke_parse_time_parameters("5minutes", False, "-90days", "-7days")

        # Main index time should cover the last 7 to 45 days
        # Summary index time should not be present
        self.check_parse_time_parameters('5min', 'Time', '2014-01-13T10:35:00Z',
                                         chart_start_time='01/13/2014:10:35:00', chart_end_time='02/21/2014:00:00:00',
                                         main_start_time='01/13/2014:10:38:30', main_end_time='02/21/2014:00:00:00')

    def test_high_res_sum_7_to_50days(self):
        """Test high resolution, sum query, for last 7 to 50 days.
        This checks that only the main index time is limited to the last 45
        days and that the snap time for 1 minute works ok.
        """
        self.invoke_parse_time_parameters("1minute", False, "-50days", "-7days")

        # Main index time should cover the last 7 to 45 days
        # Summary index time should not be present
        self.check_parse_time_parameters('1min', 'Time', '2014-01-13T10:38:00Z',
                                         chart_start_time='01/13/2014:10:38:00', chart_end_time='02/21/2014:00:00:00',
                                         main_start_time='01/13/2014:10:38:30', main_end_time='02/21/2014:00:00:00')

    @mock.patch.object(unapireporting.splunk.index_retention.IndexRetention, 'mainIndexAbsolute')
    def test_high_res_sum_no_index_metadata(self, main_index_absolute):
        """Test high resolution without having index metadata.
        This checks that only the main index time is not limited when the index
        metadata cannot be retrieved.
        """
        main_index_absolute.return_value = None

        # We expect errors to be logged when the index retention times are
        # accessed so temporarily stop logging so output does not look like a
        # failure occurred.
        logging.disable(logging.ERROR)
        self.invoke_parse_time_parameters("30minutes", False, "-50days", "-7days")
        logging.disable(logging.WARNING)

        # Main index time should cover the last 7 to 45 days
        # Summary index time should not be present
        self.check_parse_time_parameters('30min', 'Time', '2014-01-09T00:00:00Z',
                                         chart_start_time='01/09/2014:00:00:00', chart_end_time='02/21/2014:00:00:00',
                                         main_start_time='01/09/2014:00:00:00', main_end_time='02/21/2014:00:00:00')

    def test_low_res_peak_7days(self):
        """Test low resolution, peak query, for last 7 days.
        This checks that only the main index is used when a peak query is
        selected, when normally both indexes would be used.
        """
        self.invoke_parse_time_parameters("1day", True, "-7days", "0days")

        # Main index times should cover all of the last 7 days
        # Summary index time should not be present
        self.check_parse_time_parameters('1d', 'Date', '2014-02-21T00:00:00Z',
                                         chart_start_time='02/21/2014:00:00:00', chart_end_time='02/28/2014:00:00:00',
                                         main_start_time='02/21/2014:00:00:00', main_end_time='02/28/2014:00:00:00')

    def test_high_res_peak_7days(self):
        """Test high resolution, peak query, for last 7 days.
        This checks that only the main index is used when a peak query is needed
        and a high time resolution is selected, when normally both indexes would
        be used.
        """
        self.invoke_parse_time_parameters("1minute", True, "-7days", "0days")

        # Main index times should cover all of the last 7 days
        # Summary index time should not be present
        self.check_parse_time_parameters('1min', 'Time', '2014-02-21T00:00:00Z',
                                         chart_start_time='02/21/2014:00:00:00', chart_end_time='02/28/2014:00:00:00',
                                         main_start_time='02/21/2014:00:00:00', main_end_time='02/28/2014:00:00:00')

    def test_no_time_res_sum_7days(self):
        """Test low resolution, sum query, for last 7 days.
        Queries for low time resolutions can have up to three parts. The
        crosses on the following diagram illustrate the expected indexes that
        will be queried:
        @verbatim
                    Summary Index             |Saved Search | Main Index |
        __________________________________xxxx|xxxxxxxxxxx__|____________|
        <-Indefinite                         -3d           -2h          Now
        @endverbatim
        """
        self.invoke_parse_time_parameters("noTimeSplit", False, "-7days", "0days")
        self.check_parse_time_parameters('1d', None, '2014-02-21T00:00:00Z',
                                         chart_start_time='02/21/2014:00:00:00', chart_end_time='02/28/2014:00:00:00',
                                         saved_search_start_time='02/25/2014:00:00:00', saved_search_end_time='02/28/2014:00:00:00',
                                         summary_start_time='02/21/2014:00:00:00', summary_end_time='02/25/2014:00:00:00')

    def test_high_res_sum_no_summary_index(self):
        """Test high resolution, sum query, for last 7 days, no summary index.
        If a metric has no summary index data (such as the acquiry log data)
        then time resolution is ignored and only the main index is used,
        always.

        """
        self.invoke_parse_time_parameters(
            "30minutes", False, "-50days", "-7days",
            metric_in_summary_index=False)

        self.check_parse_time_parameters(
            '30min', 'Time', '2014-01-13T10:30:00Z', '01/13/2014:10:30:00',
            '02/21/2014:00:00:00', main_start_time='01/13/2014:10:38:30',
            main_end_time='02/21/2014:00:00:00')

    def test_low_res_sum_no_summary_index(self):
        """Test low resolution, sum query, for last 2 hours, no summary index.
        If a metric has no summary index data (such as the acquiry log data)
        then time resolution is ignored and only the main index is used,
        always.

        """
        self.invoke_parse_time_parameters(
            "1day", False, "-2hours", "now", metric_in_summary_index=False)

        self.check_parse_time_parameters(
            '1d', 'Date', '2014-02-28T00:00:00Z', '02/28/2014:00:00:00',
            '02/28/2014:10:38:30', main_start_time='02/28/2014:08:00:00',
            main_end_time='02/28/2014:10:38:30')



class TestTimeParsingErrors(ParseTimeParametersMixin):
    """Tests for the errors raised when the time range is not supported.
    This class checks the time handling of the following method:
    - @ref unapireporting.datamodel.report_job_collection.ReportJobCollection._parse_time_parameters

    @note These tests use the standard test data which reports the main index as
          having a 45 day retention period.
    """

    def test_unsupported_medium_res_sum(self):
        """Test time warning for a summing metric with a medium time resolution.
        This checks that when the start time is adjusted to be the same as the
        end time and exception is raised as the report will have no results.
        """
        with self.assertRaises(errors.PropertyValueError) as cm:
            self.invoke_parse_time_parameters("1hour", False,"-47days", "-46days")
        self.check_exception(cm, [JsonKeys.TIME_RESOLUTION],
                             "This time resolution is not available for this time range")

    def test_unsupported_high_res_sum(self):
        """Test time warning for a summing metric with a high time resolution.
        This checks that when the start time is adjusted to be the after the end
        time and exception is raised as the report will have no results.
        """
        with self.assertRaises(errors.PropertyValueError) as cm:
            self.invoke_parse_time_parameters("30minutes", False, "-90days", "-60days")
        self.check_exception(cm, [JsonKeys.TIME_RESOLUTION],
                             "This time resolution is not available for this time range")

    def test_unsupported_high_res_peak(self):
        """Test metric warning for a peak metric with a high time resolution.
        This checks that when the start time is adjusted to be the same as the
        end time and exception is raised as the report will have no results.
        """
        with self.assertRaises(errors.PropertyValueError) as cm:
            self.invoke_parse_time_parameters("30minutes", True, "-47days", "-46days")
        self.check_exception(cm, [JsonKeys.METRIC],
                             "This metric is not available for this time range")

    def test_unsupported_low_res_peak(self):
        """Test metric warning for a peak metric with a low time resolution.
        This checks that when the start time is adjusted to be the after the end
        time and exception is raised as the report will have no results.
        """
        with self.assertRaises(errors.PropertyValueError) as cm:
            self.invoke_parse_time_parameters("1day", True, "-90days", "-60days")
        self.check_exception(cm, [JsonKeys.METRIC],
                             "This metric is not available for this time range")

    def test_unsupported_low_res_main_index_only(self):
        """Test metric warning for a metric which only uses the main index.
        This checks that when the start time is adjusted to be the after the end
        time and exception is raised as the report will have no results.
        """
        with self.assertRaises(errors.PropertyValueError) as cm:
            self.invoke_parse_time_parameters("1day", False, "-90days", "-60days",
                                              metric_in_summary_index=False)
        self.check_exception(cm, [JsonKeys.METRIC],
                             "This metric is not available for this time range")

    def test_unsupported_medium_main_index_only(self):
        """Test metric warning for a metric which only uses the main index.
        This checks that when the start time is adjusted to be the same as the
        end time and exception is raised as the report will have no results.
        """
        with self.assertRaises(errors.PropertyValueError) as cm:
            self.invoke_parse_time_parameters("1hour", False, "-47days", "-46days",
                                              metric_in_summary_index=False)
        self.check_exception(cm, [JsonKeys.METRIC],
                             "This metric is not available for this time range")

    def test_unsupported_high_main_index_only(self):
        """Test metric warning for a metric which only uses the main index.
        This checks that when the start time is adjusted to be the after the end
        time and exception is raised as the report will have no results.
        """
        with self.assertRaises(errors.PropertyValueError) as cm:
            self.invoke_parse_time_parameters("30minutes", False, "-90days", "-60days",
                                              metric_in_summary_index=False)
        self.check_exception(cm, [JsonKeys.METRIC],
                             "This metric is not available for this time range")

    def test_invalid_start_time(self):
        """Test exception raised when an invalid start time is given."""
        with self.assertRaises(errors.PropertyValueError) as cm:
            self.invoke_parse_time_parameters("1d", False, "days", "0days")
        self.check_exception(cm, [JsonKeys.START_TIME], "Invalid time is given")

    def test_invalid_end_time(self):
        """Test exception raised when an invalid end time is given."""
        with self.assertRaises(errors.PropertyValueError) as cm:
            self.invoke_parse_time_parameters("1d", False, "1days", "days")
        self.check_exception(cm, [JsonKeys.END_TIME], "Invalid time is given")

    def test_invalid_start_time_later_than_end_time(self):
        """Test exception raised when the start time is later than the end time."""
        with self.assertRaises(errors.PropertyValueError) as cm:
            self.invoke_parse_time_parameters("1d", False, "1days", "5days")
        self.check_exception(cm, [JsonKeys.START_TIME], "The start time is later than the end time")

    def test_invalid_start_time_of_now(self):
        """Test exception raised when an invalid start time of "now" is given."""
        with self.assertRaises(errors.PropertyValueError) as cm:
            self.invoke_parse_time_parameters("1d", False, "now", "0days")
        self.check_exception(cm, [JsonKeys.START_TIME], "The start time cannot be now")

    def test_invalid_start_time_in_future(self):
        """Test exception raised when the start time is in the future."""
        with self.assertRaises(errors.PropertyValueError) as cm:
            self.invoke_parse_time_parameters("1d", False, "2014-03-01T00:00:00Z", "2014-03-02T00:00:00Z")
        self.check_exception(cm, [JsonKeys.START_TIME], "The start time is in the future")

    def test_invalid_start_and_end_time_the_same(self):
        """Test exception raised when the start time is the same as the end time."""
        with self.assertRaises(errors.PropertyValueError) as cm:
            self.invoke_parse_time_parameters("1d", False, "1days", "1days")
        self.check_exception(cm, [JsonKeys.START_TIME], "The start time is the same as the end time")

    def test_invalid_time_resolution(self):
        """Test exception raised when an invalid time resolution is given."""
        with self.assertRaises(errors.PropertyValueError) as cm:
            self.invoke_parse_time_parameters("unknown", False, "1week", "now")
        self.check_exception(cm, [JsonKeys.TIME_RESOLUTION], "This time resolution is not supported")



class TestReportRunParseOrganizationParameters(SetupUnapiFlaskAppForReporting):
    """Tests for the
    @ref unapireporting.datamodel.report_job_collection.ReportJobCollection._parse_organization_parameters method.
    """
    def setUp(self):
        patch = mock.patch.object(report_job_collection.ReportJobCollection, 'get_cdn_db',
                                  new=FakeCdnDbForReporting)
        patch.start()
        self.addCleanup(patch.stop)

    def test_no_params(self):
        """Test defaults when no organizations are given
        """
        params = {}
        # Call code under test
        obj = report_job_collection.ReportJobCollection(report_id='traffic')
        org_ids = obj._parse_organization_parameters(params)

        # Check the org IDs are as expected
        self.assertEquals(org_ids, set([]))

    def test_two_organizations(self):
        """Test when a list of organizations is given in filter-by
        """
        params = {
            JsonKeys.LINKS: {
                LinkRel.FILTER_BY: [
                    {
                        JsonKeys.LINK_RESOURCE_ID: {'org_id': 1}
                    },
                    {
                        JsonKeys.LINK_RESOURCE_ID: {'org_id': 2}
                    },
                ]
             }
        }

        # Call code under test
        obj = report_job_collection.ReportJobCollection(report_id='traffic')
        org_ids = obj._parse_organization_parameters(params)

        # Check the org IDs are as expected
        self.assertEquals(org_ids, set([1,2]))

    def test_filter_by_owns(self):
        """Test when an organization is given in filter-by-owns
        """
        params = {
            JsonKeys.LINKS: {
                LinkRel.FILTER_BY_OWNS: {
                    JsonKeys.LINK_RESOURCE_ID: {'org_id': 1}
                },
             }
        }

        # Call code under test
        obj = report_job_collection.ReportJobCollection(report_id='traffic')
        org_ids = obj._parse_organization_parameters(params)

        # Check the org IDs are as expected
        self.assertEquals(org_ids, set([3]))

    def test_missing_resource_id(self):
        """Test when a a link is missing the resource ID.
        This should not occur operationally but good to be robust.
        """
        params = {
            JsonKeys.LINKS: {
                LinkRel.FILTER_BY: [
                    {
                        JsonKeys.LINK_HREF: '/content/orgs/1'
                    },
                    {
                        JsonKeys.LINK_RESOURCE_ID: {'org_id': 2}
                    },
                ]
             }
        }

        obj = report_job_collection.ReportJobCollection(report_id='traffic')

        # Temporarily disable the logging so we have clean output
        logging.disable(logging.ERROR)

        # Call code under test
        org_ids = obj._parse_organization_parameters(params)

        # Return logging to it's previous level
        logging.disable(logging.WARNING)

        # Check the org IDs are as expected
        self.assertEquals(org_ids, set([2]))

    def test_missing_organization_id(self):
        """Test when a a link is missing the organization ID.
        This should not occur operationally but good to be robust.
        """
        params = {
            JsonKeys.LINKS: {
                LinkRel.FILTER_BY: [
                    {
                        JsonKeys.LINK_HREF: '/content/orgs/1',
                        JsonKeys.LINK_RESOURCE_ID: {'org_id': 1}
                    },
                    {
                        JsonKeys.LINK_HREF: '/content/orgs/2',
                        JsonKeys.LINK_RESOURCE_ID: {'report_id': 2}
                    },
                ]
             }
        }

        obj = report_job_collection.ReportJobCollection(report_id='traffic')

        # Temporarily disable the logging so we have clean output
        logging.disable(logging.ERROR)

        # Call code under test
        org_ids = obj._parse_organization_parameters(params)

        # Return logging to it's previous level
        logging.disable(logging.WARNING)

        # Check the org IDs are as expected
        self.assertEquals(org_ids, set([1]))


class ReportRunningMixin(SetupUnapiFlaskAppForReporting):
    """Bases class for checking report running and results."""

    def check_report_results(self, actual_results, language, split_type, time_resolution, metric):
        """Check that a report result match those expected.
        @todo Do a proper check of the split label.
        @param actual_results - results returned
        @param language - expected language in results
        @param split_type - split type
        @param time_resolution - time resolution

        @retval None"""
        self.expected_results = report_results(language=language, split_type=split_type, time_resolution=time_resolution, metric=metric)

        self.assertEqual(actual_results[JsonKeys.RESULT_FIELDS], self.expected_results[JsonKeys.RESULT_FIELDS], "Report results don't match")
        self.assertEqual(actual_results[JsonKeys.RESULT_COLUMNS], self.expected_results[JsonKeys.RESULT_COLUMNS], "Report results don't match")
        self.assertEqual(actual_results[JsonKeys.RESULT_PERCENTAGE_DONE], self.expected_results[JsonKeys.RESULT_PERCENTAGE_DONE], "Report results don't match")
        self.assertEqual(actual_results[JsonKeys.RESULT_IS_DONE], self.expected_results[JsonKeys.RESULT_IS_DONE], "Report results don't match")

        # Do a simple check to ensure the properties are there
        self.assertIn(JsonKeys.START_TIME, actual_results)
        self.assertIn(JsonKeys.END_TIME, actual_results)
        self.assertIn(JsonKeys.TIME_RESOLUTION, actual_results)
        self.assertIn(JsonKeys.SPLIT_BY, actual_results)
        self.assertIn(JsonKeys.METRIC, actual_results)
        self.assertIn(JsonKeys.METRIC_LABEL, actual_results)


    def check_report_run(self, parameters={}, language=None, report_id='traffic'):
        """Check that a report can be run and handles job not ready correctly.
        @param parameters - report run parameters used in POST.
        @param language - optional language to localise report results.
        @param report_id - report to test, defaults to the @c traffic report.
        @returns The decoded JSON response body of the report job item.
        """

        # Check we can run the report and get a result
        _, report_job_json = self.http_post(Profiles.REPORT_JOB_ITEM,
                                         '/reporting/reports/%s/jobs' %report_id,
                                         parameters,
                                         language=language,
                                         expected_max_age=60 * 60 * 24 * 365)

        metric = parameters.get(JsonKeys.METRIC)

        split_type = None
        split = parameters.get(JsonKeys.SPLIT_BY)
        if split:
            if JsonKeys.SPLIT_BY_ORG in split:
                split_type = split.get(JsonKeys.SPLIT_BY_ORG)
            elif JsonKeys.SPLIT_BY_HTTP_STATUS in split:
                split_type = split.get(JsonKeys.SPLIT_BY_HTTP_STATUS)
            elif JsonKeys.SPLIT_BY_CACHE_STATUS in split:
                split_type = split.get(JsonKeys.SPLIT_BY_CACHE_STATUS)
            elif JsonKeys.SPLIT_BY_CONTENT in split:
                split_type = split.get(JsonKeys.SPLIT_BY_CONTENT)
            elif JsonKeys.SPLIT_BY_NETWORK in split:
                split_type = split.get(JsonKeys.SPLIT_BY_NETWORK)
            elif JsonKeys.SPLIT_BY_NETWORK_RESPONSE in split:
                split_type = split.get(JsonKeys.SPLIT_BY_NETWORK_RESPONSE)

        time_resolution = parameters.get(JsonKeys.TIME_RESOLUTION)

        self.check_report_results(report_job_json, language, split_type, time_resolution, metric)

        return report_job_json



class TestReportRunning(ReportRunningMixin):
    """Test Report Running at the API level"""

    def check_invalid_report_run(self, parameters, err_code, err_msg, err_details):
        """Check that invalid report run parameters are handled correctly.
        @param parameters - the given input parameters to POST
        @param err_code - the error code that is expected to be
        found in the error response that comes back from UNAPI.
        @param err_msg - a string that is expected to be found within the error
                         message that comes back from UNAPI.
        @param err_details - the error details that are expected to be found in
                             the error response that comes back from UNAPI.
        """
        self.check_bad_request_post(Profiles.REPORT_JOB_ITEM,
                                    '/reporting/reports/traffic/jobs',
                                    parameters,
                                    err_code,
                                    err_msg,
                                    err_details)

    def check_default_report_run(self, language=None):
        json_props = self.check_report_run(language=language)

        # Check we have the correct parameter values
        self.assertEqual(json_props[JsonKeys.METRIC], JsonKeys.METRIC_BYTES_DELIVERED)
        self.assertIsNotNone(json_props[JsonKeys.START_TIME])
        self.assertIsNotNone(json_props[JsonKeys.END_TIME])
        self.assertEqual(json_props[JsonKeys.TIME_RESOLUTION], '1day')
        self.assertEqual(json_props[JsonKeys.SPLIT_BY], {})
        self.assertNotIn(LinkRel.FILTER_BY, json_props[JsonKeys.LINKS], "There shouldn't be an organization filter")
        self.assertEqual(json_props[JsonKeys.FILTER_BY], {})

        if language == TEST_LANG_CODE:
            self.assertEqual(json_props[JsonKeys.METRIC_LABEL], '!Bytes delivered')
        else:
            self.assertEqual(json_props[JsonKeys.METRIC_LABEL], 'Bytes delivered')

    def test_report_job_collection(self):
        """Check that a report can be run and handles job not ready correctly."""
        self.check_default_report_run()

    def test_report_job_collection_localised(self):
        """Check that a report can be run and handles job not ready correctly."""
        self.check_default_report_run(language=TEST_LANG_CODE)

    def test_invalid_param(self):
        """Check the JSON schema is working correctly for report run."""
        self.check_invalid_report_run({'badger':'cheese'},
            4002,
            "Additional properties are not allowed ('badger' was unexpected)",
            {u'property': [u'badger']})

    def test_invalid_split(self):
        """Check the JSON schema is working correctly for report run."""
        params = {
            JsonKeys.SPLIT_BY: {
                'badger': 'cheese'
            }
        }
        self.check_invalid_report_run(params,
            4002,
            "Additional properties are not allowed ('badger' was unexpected)",
            {u'property': [u'splitBy', u'badger']} )

    def test_invalid_multiple_split(self):
        """Check the JSON schema is working correctly for report run."""
        params = {
            JsonKeys.SPLIT_BY: {
                JsonKeys.SPLIT_BY_ORG:          JsonKeys.SPLIT_BY_ORG_SERVICE,
                JsonKeys.SPLIT_BY_HTTP_STATUS:  JsonKeys.SPLIT_BY_HTTP_STATUS_CODE,
                JsonKeys.SPLIT_BY_CACHE_STATUS: JsonKeys.SPLIT_BY_CACHE_STATUS_DISPOSITION
            }
        }

        # Printing the dictionary in the final string due to the iteration order is going to be the same that the  one
        # used to generate the json response
        self.check_invalid_report_run(params,
            4004,
            '{0} has too many properties'.format(
                {'org': 'orgService', 'httpStatus': 'httpStatusCode', 'cacheStatus': 'cacheStatusDisposition'}
            ),
            {u'property': [u'splitBy']} )

    def test_invalid_org_subsplit(self):
        """Check the JSON schema is working correctly for organization splits."""
        params = {
            JsonKeys.SPLIT_BY: {
                JsonKeys.SPLIT_BY_ORG: 'badger'
            }
        }
        self.check_invalid_report_run(params,
            4004,
            "'badger' is not one of ['orgService']",
            {u'property': [u'splitBy', u'org']} )

    def test_invalid_http_status_subsplit(self):
        """Check the JSON schema is working correctly for HTTP Status splits."""
        params = {
            JsonKeys.SPLIT_BY: {
                JsonKeys.SPLIT_BY_HTTP_STATUS: 'badger'
            }
        }
        self.check_invalid_report_run(params,
            4004,
            "'badger' is not one of ['httpStatusSuccessFailure', 'httpStatusCodeClass', 'httpStatusCode']",
            {u'property': [u'splitBy', u'httpStatus']} )

    def test_invalid_cache_status_subsplit(self):
        """Check the JSON schema is working correctly for Cache Status splits."""
        params = {
            JsonKeys.SPLIT_BY: {
                JsonKeys.SPLIT_BY_CACHE_STATUS: 'badger'
            }
        }
        self.check_invalid_report_run(params,
            4004,
            "'badger' is not one of ['cacheStatusHitMiss', 'cacheStatusDisposition']",
            {u'property': [u'splitBy', u'cacheStatus']} )

    def test_invalid_content_subsplit(self):
        """Check the JSON schema is working correctly for Content splits."""
        params = {
            JsonKeys.SPLIT_BY: {
                JsonKeys.SPLIT_BY_CONTENT: 'badger'
            }
        }
        self.check_invalid_report_run(params,
            4004,
            "'badger' is not one of ['contentObject', 'contentAsset']",
            {u'property': [u'splitBy', u'content']} )

    def test_invalid_network_subsplit(self):
        """Check the JSON schema is working correctly for Network splits."""
        params = {
            JsonKeys.SPLIT_BY: {
                JsonKeys.SPLIT_BY_NETWORK: 'badger'
            }
        }
        self.check_invalid_report_run(params,
            4004,
            "'badger' is not one of ['networkDeliveryAppliance', 'networkTier', "
                "'networkSendingApplianceHostname', 'networkSendingApplianceIpAddress', "
                "'networkReceivingApplianceHostname', 'networkReceivingApplianceIpAddress']",
            {u'property': [u'splitBy', u'network']} )

    def test_invalid_network_response_subsplit(self):
        """Check the JSON schema is working correctly for Network Response splits."""
        params = {
            JsonKeys.SPLIT_BY: {
                JsonKeys.SPLIT_BY_NETWORK_RESPONSE: 'badger'
            }
        }
        self.check_invalid_report_run(
            params, 4004, "'badger' is not one of {0!r}".format([
                'networkResponseSuccessFailure', 'networkResponseCodeClass', 'networkResponseCode']),
            {u'property': [u'splitBy', u'networkResponse']})

    def test_valid_empty_split(self):
        """Check that we can run a report with an empty split."""
        params = {
            JsonKeys.SPLIT_BY: {}
        }
        json_props = self.check_report_run(parameters=params, language=TEST_LANG_CODE)
        # Check the report job contains the correct parameters
        self.assertEqual(json_props[JsonKeys.SPLIT_BY], {})

    def test_valid_org_filter_cdn_reporting(self):
        """Check that we can filter by organization when running one of the CDN Reporting reports."""
        self.check_valid_org_filter(report_id='traffic')

    def test_valid_org_filter_cdn_insights(self):
        """Check that we can filter by organization when running one of the CDN Insights reports."""
        # Must specify the split since the mocked results are derived from the given split
        self.check_valid_org_filter(report_id='tier-analysis',
            parameters={ JsonKeys.SPLIT_BY: { JsonKeys.SPLIT_BY_NETWORK:
                                              JsonKeys.SPLIT_BY_NETWORK_TIER } })

    def check_valid_org_filter(self, parameters=None, report_id='traffic'):
        """Check that we can successfully filter by organization when running a report."""
        if parameters is None:
            parameters = {}
        parameters.update({
            JsonKeys.LINKS: {
                LinkRel.FILTER_BY: [
                    {
                        JsonKeys.LINK_HREF: "/content/orgs/2",
                        JsonKeys.LINK_PROFILE: Profiles.ORGANIZATION_ITEM
                    },
                    {
                        JsonKeys.LINK_HREF: "/content/orgs/3",
                        JsonKeys.LINK_PROFILE: Profiles.ORGANIZATION_ITEM
                    }
                ],
                LinkRel.FILTER_BY_OWNS: {
                    JsonKeys.LINK_HREF: "/content/orgs/4",
                    JsonKeys.LINK_PROFILE: Profiles.ORGANIZATION_ITEM
                }
             }
        })
        json_props = self.check_report_run(parameters=parameters, report_id=report_id)
        # Check the report job contains the correct parameters.
        # Filter-by first
        org_names = []
        links = json_props[JsonKeys.LINKS][LinkRel.FILTER_BY]
        self.assertTrue(isinstance(links, list), "Expecting a list of filter-by links")
        for link in links:
            org_names.append(link[JsonKeys.LINK_NAME])
        self.assertEqual(org_names, ["2", "3"])
        # Now filter-by-owns
        link = json_props[JsonKeys.LINKS][LinkRel.FILTER_BY_OWNS]
        self.assertTrue(isinstance(link, dict), "Expecting a single link dict")
        self.assertEqual(link[JsonKeys.LINK_NAME], "4")

    def test_valid_http_filter(self):
        """Check that we can successfully apply a HTTP Status filter."""
        params = {
            JsonKeys.FILTER_BY: {
                JsonKeys.FILTER_BY_HTTP_STATUS: [100,"101","2xx","failure"]
            }
        }
        json_props = self.check_report_run(parameters=params)
        # Check the report job contains the correct parameters
        self.assertEqual(json_props[JsonKeys.FILTER_BY][JsonKeys.FILTER_BY_HTTP_STATUS], [100,"101","2xx","failure"])

    def test_valid_empty_http_filter(self):
        """Check that we can run a report with an empty HTTP Status filter."""
        params = {
            JsonKeys.FILTER_BY: { JsonKeys.FILTER_BY_HTTP_STATUS: [] }
        }
        json_props = self.check_report_run(parameters=params)
        # Check the report job contains the correct parameters
        self.assertEqual(json_props[JsonKeys.FILTER_BY], {})

    def test_valid_split_limit(self):
        """Check that we can run a report with a split limit parameter."""
        params = {
            JsonKeys.SPLIT_LIMIT: 13
        }
        json_props = self.check_report_run(parameters=params)
        # Check the report job contains the given split limit
        self.assertEqual(json_props[JsonKeys.SPLIT_LIMIT], 13)

    def test_default_split_limit(self):
        """Check that the split limit parameter is returned when it is not given."""
        params = {}
        json_props = self.check_report_run(parameters=params)
        # Check the report job contains the correct split limit
        self.assertEqual(json_props[JsonKeys.SPLIT_LIMIT], DEFAULT_SPLIT_LIMIT)

    def test_valid_cache_disposition_filter(self):
        """Check that we can successfully apply a Cache Disposition filter."""
        params = {
            JsonKeys.FILTER_BY: {
                JsonKeys.FILTER_BY_CACHE_DISPOSITION: [JsonKeys.FILTER_BY_CACHE_HIT,
                                                       JsonKeys.FILTER_BY_CACHE_VXICP_HIT,
                                                       JsonKeys.FILTER_BY_CACHE_DISPOSITION_MISS]
            }
        }
        json_props = self.check_report_run(parameters=params)
        # Check the report job contains the correct parameters
        self.assertEqual(json_props[JsonKeys.FILTER_BY][JsonKeys.FILTER_BY_CACHE_DISPOSITION], [
                                                        JsonKeys.FILTER_BY_CACHE_HIT,
                                                        JsonKeys.FILTER_BY_CACHE_VXICP_HIT,
                                                        JsonKeys.FILTER_BY_CACHE_DISPOSITION_MISS])

    def test_valid_empty_cache_disposition_filter(self):
        """Check that we can run a report with an empty Cache Disposition filter."""
        params = {
            JsonKeys.FILTER_BY: { JsonKeys.FILTER_BY_CACHE_DISPOSITION: [] }
        }
        json_props = self.check_report_run(parameters=params, language=TEST_LANG_CODE)
        # Check the report job contains the correct parameters
        self.assertEqual(json_props[JsonKeys.FILTER_BY], {})

    def test_valid_empty_filter(self):
        """Check that we can run a report with an empty filter."""
        params = {
            JsonKeys.FILTER_BY: {}
        }
        json_props = self.check_report_run(parameters=params, language=TEST_LANG_CODE)
        # Check the report job contains the correct parameters
        self.assertEqual(json_props[JsonKeys.FILTER_BY], {})


    def test_valid_absolute_start_and_end_times(self):
        """Check that we can successfully set start and end times when running a report."""
        params = {
            JsonKeys.START_TIME: '1978-06-30T12:30:05Z',
            JsonKeys.END_TIME: '2078-06-30T12:30:05Z'
        }

        json_props = self.check_report_run(parameters=params)
        self.assertEqual(json_props[JsonKeys.START_TIME], '1978-06-30T12:30:05Z')
        self.assertEqual(json_props[JsonKeys.END_TIME], '2078-06-30T12:30:05Z')

    def test_valid_relative_start_and_end_times(self):
        """Check that we can successfully set start and end times when running a report."""
        params = {
            JsonKeys.START_TIME: '7days',
            JsonKeys.END_TIME: '0days'
        }

        json_props = self.check_report_run(parameters=params)
        self.assertEqual(json_props[JsonKeys.START_TIME], '7days')
        self.assertEqual(json_props[JsonKeys.END_TIME], '0days')



class TestReportRunTimeResolutions(ReportRunningMixin):
    """Test each of the supported time resolutions by running a report."""

    def check_time_resolution(self, time_resolution, language=None):
        """Check POSTing with a time resolution
        @param time_resolution - which time resolution to use
        @param language - optional language
        @return JSON properties
        """
        params = {
            JsonKeys.TIME_RESOLUTION: time_resolution
        }

        json_props = self.check_report_run(parameters=params, language=language)
        # Check the report job contains the correct label
        self.assertEqual(json_props[JsonKeys.TIME_RESOLUTION], time_resolution)
        return json_props

    def test_valid_time_resolution(self):
        """Test each supported time resolution.
        Check that each resolution is accepted when creating a report job and
        returned when reading a report job."""
        for resolution in report_job_item.ReportJobItem.json_schema["properties"][JsonKeys.TIME_RESOLUTION]['enum']:
            self.check_time_resolution(resolution)

    def test_valid_time_resolution_localised(self):
        """Test each supported time resolution localised.
        Check that each resolution is accepted when creating a report job and
        returned when reading a report job."""
        for resolution in report_job_item.ReportJobItem.json_schema["properties"][JsonKeys.TIME_RESOLUTION]['enum']:
            self.check_time_resolution(resolution, language=TEST_LANG_CODE)



class ReportRunSplitMixin(object):
    """The tests to invoke to run a report for any given split.
    Any derived class must define @c check_split which calls @c
    check_time_resolution_and_split"""

    def check_time_resolution_and_split(self, time_resolution, split,
                                        split_label=None,
                                        language=None,
                                        report_id='traffic'):
        """Helper method for checking a time resolution with a split.
        @param time_resolution - time resolution to use (None for default)
        @param split - split dictionary
        @param split_label - split label if not "Time" or "Date"
        @param language - optional language to localise report results.
        @param report_id - report to test, defaults to the @c traffic report.
        """

        # If the time resolution provided is none then use the default, which
        # should be one day.
        if time_resolution is None:
            params = {
                JsonKeys.SPLIT_BY: split
            }
            time_resolution = JsonKeys.TIME_RESOLUTION_1_DAY
        # Otherwise use the time resolution provided.
        else:
            params = {
                JsonKeys.TIME_RESOLUTION: time_resolution,
                JsonKeys.SPLIT_BY: split
            }
        json_props = self.check_report_run(parameters=params,
                                           language=language,
                                           report_id=report_id)

        # There are some default expectations for the split label when a time
        # resolution is defined.
        if split_label is None:
            if time_resolution in [JsonKeys.TIME_RESOLUTION_1_MINUTE,
                                   JsonKeys.TIME_RESOLUTION_5_MINUTES,
                                   JsonKeys.TIME_RESOLUTION_15_MINUTES,
                                   JsonKeys.TIME_RESOLUTION_30_MINUTES,
                                   JsonKeys.TIME_RESOLUTION_1_HOUR]:
                if language is None:
                    split_label = "Time"
                else:
                    split_label = "!Time"
            elif time_resolution in [JsonKeys.TIME_RESOLUTION_1_DAY,
                                     JsonKeys.TIME_RESOLUTION_1_WEEK,
                                     JsonKeys.TIME_RESOLUTION_1_MONTH,
                                     JsonKeys.TIME_RESOLUTION_1_QUARTER,
                                     JsonKeys.TIME_RESOLUTION_1_YEAR]:
                if language is None:
                    split_label = "Date"
                else:
                    split_label = "!Date"

        # Check the report job contains the correct parameters
        self.assertEqual(json_props[JsonKeys.TIME_RESOLUTION], time_resolution)
        self.assertEqual(json_props[JsonKeys.SPLIT_BY], split)
        self.assertEqual(json_props[JsonKeys.SPLIT_LABEL], split_label)
        # Despite the internal filtering that may go on there should not be any
        # addressable filters listed e.g organisation.
        self.assertNotIn(LinkRel.FILTER_BY, json_props[JsonKeys.LINKS],
                         "There shouldn't be any addressable filters")


    def test_default_time_resolution_and_split(self):
        """Check the default time resolution with appropriate split"""
        self.check_split(None)

    def test_default_time_resolution_and_split_localised(self):
        """Check the default time resolution with appropriate split"""
        self.check_split(None, language=TEST_LANG_CODE)

    def test_1minute_resolution_and_split(self):
        """Check 1 minute time resolution with appropriate split"""
        self.check_split(JsonKeys.TIME_RESOLUTION_1_MINUTE)

    def test_1minute_resolution_and_split_localised(self):
        """Check 1 minute time resolution with appropriate split"""
        self.check_split(JsonKeys.TIME_RESOLUTION_1_MINUTE, language=TEST_LANG_CODE)

    def test_5minutes_resolution_and_split(self):
        """Check 5 minutes time resolution with appropriate split"""
        self.check_split(JsonKeys.TIME_RESOLUTION_5_MINUTES)

    def test_5minutes_resolution_and_split_localised(self):
        """Check 5 minutes time resolution with appropriate split"""
        self.check_split(JsonKeys.TIME_RESOLUTION_5_MINUTES, language=TEST_LANG_CODE)

    def test_15minutes_resolution_and_split(self):
        """Check 15 minutes time resolution with appropriate split"""
        self.check_split(JsonKeys.TIME_RESOLUTION_15_MINUTES)

    def test_15minutes_resolution_and_split_localised(self):
        """Check 15 minutes time resolution with appropriate split"""
        self.check_split(JsonKeys.TIME_RESOLUTION_15_MINUTES, language=TEST_LANG_CODE)

    def test_30minutes_resolution_and_split(self):
        """Check 30 minutes time resolution with appropriate split"""
        self.check_split(JsonKeys.TIME_RESOLUTION_30_MINUTES)

    def test_30minutes_resolution_and_split_localised(self):
        """Check 30 minutes time resolution with appropriate split"""
        self.check_split(JsonKeys.TIME_RESOLUTION_30_MINUTES, language=TEST_LANG_CODE)

    def test_1hour_resolution_and_split(self):
        """Check 1 hour time resolution with appropriate split"""
        self.check_split(JsonKeys.TIME_RESOLUTION_1_HOUR)

    def test_1hour_resolution_and_split_localised(self):
        """Check 1 hour time resolution with appropriate split"""
        self.check_split(JsonKeys.TIME_RESOLUTION_1_HOUR, language=TEST_LANG_CODE)

    def test_1day_resolution_and_split(self):
        """Check 1 day time resolution with appropriate split"""
        self.check_split(JsonKeys.TIME_RESOLUTION_1_DAY)

    def test_1day_resolution_and_split_localised(self):
        """Check 1 day time resolution with appropriate split"""
        self.check_split(JsonKeys.TIME_RESOLUTION_1_DAY, language=TEST_LANG_CODE)

    def test_1week_resolution_and_split(self):
        """Check 1 week time resolution with appropriate split"""
        self.check_split(JsonKeys.TIME_RESOLUTION_1_WEEK)

    def test_1week_resolution_and_split_localised(self):
        """Check 1 week time resolution with appropriate split"""
        self.check_split(JsonKeys.TIME_RESOLUTION_1_WEEK, language=TEST_LANG_CODE)

    def test_1month_resolution_and_split(self):
        """Check 1 month time resolution with appropriate split"""
        self.check_split(JsonKeys.TIME_RESOLUTION_1_MONTH)

    def test_1month_resolution_and_split_localised(self):
        """Check 1 month time resolution with appropriate split"""
        self.check_split(JsonKeys.TIME_RESOLUTION_1_MONTH, language=TEST_LANG_CODE)

    def test_1quarter_resolution_and_split(self):
        """Check 1 quarter time resolution with appropriate split"""
        self.check_split(JsonKeys.TIME_RESOLUTION_1_QUARTER)

    def test_1quarter_resolution_and_split_localised(self):
        """Check 1 quarter time resolution with appropriate split"""
        self.check_split(JsonKeys.TIME_RESOLUTION_1_QUARTER, language=TEST_LANG_CODE)

    def test_1year_resolution_and_split(self):
        """Check 1 year time resolution with appropriate split"""
        self.check_split(JsonKeys.TIME_RESOLUTION_1_YEAR)

    def test_1year_resolution_and_split_localised(self):
        """Check 1 year time resolution with appropriate split"""
        self.check_split(JsonKeys.TIME_RESOLUTION_1_YEAR, language=TEST_LANG_CODE)

    def test_no_time_resolution_and_split(self):
        """Check without a time resolution with appropriate split"""
        self.check_split(JsonKeys.TIME_RESOLUTION_NONE,
                         split_label=self.split_label)

    def test_no_time_resolution_and_split_localised(self):
        """Check without a time resolution with appropriate split"""
        split_label = '!' + self.split_label
        self.check_split(JsonKeys.TIME_RESOLUTION_NONE,
                         split_label=split_label,
                         language=TEST_LANG_CODE)



class TestReportRunWithOrgServiceSplit(ReportRunSplitMixin, ReportRunningMixin):
    """Test report run with Organization Service split.
    This tests the split with each of the supported time resolutions and also
    checks the localised results. The tests run can be found in the @ref
    ReportRunSplitMixin."""

    ## Unlocalised split label used when there isn't a time split
    split_label = 'Service'

    def check_split(self, time_resolution, split_label=None, language=None):
        """Helper method for checking a time resolution with an organization service split.
        @param time_resolution - time resolution to use
        @param split_label - split label if not "Time" or "Date"
        @param language - optional language
        """
        self.check_time_resolution_and_split(time_resolution,
                                             {JsonKeys.SPLIT_BY_ORG: JsonKeys.SPLIT_BY_ORG_SERVICE},
                                             split_label=split_label,
                                             language=language)



class TestReportRunWithContentObjectSplit(ReportRunSplitMixin, ReportRunningMixin):
    """Test report run with Content Object split.
    This tests the split with each of the supported time resolutions and also
    checks the localised results. The tests run can be found in the @ref
    ReportRunSplitMixin."""

    ## Unlocalised split label used when there isn't a time split
    split_label = 'Object'

    def check_split(self, time_resolution, split_label=None, language=None):
        """Helper method for checking a time resolution with a content object split.
        @param time_resolution - time resolution to use
        @param split_label - split label if not "Time" or "Date"
        @param language - optional language
        """
        self.check_time_resolution_and_split(time_resolution,
                                             {JsonKeys.SPLIT_BY_CONTENT: JsonKeys.SPLIT_BY_CONTENT_OBJECT},
                                             split_label=split_label,
                                             language=language)



class TestReportRunWithContentAssetSplit(ReportRunSplitMixin, ReportRunningMixin):
    """Test report run with a Content Asset split.
    This tests the split with each of the supported time resolutions and also
    checks the localised results. The tests run can be found in the @ref
    ReportRunSplitMixin."""

    ## Unlocalised split label used when there isn't a time split
    split_label = 'Asset'

    def check_split(self, time_resolution, split_label=None, language=None):
        """Helper method for checking a time resolution with a content asset split.
        @param time_resolution - time resolution to use
        @param split_label - split label if not "Time" or "Date"
        @param language - optional language
        """
        self.check_time_resolution_and_split(time_resolution,
                                             {JsonKeys.SPLIT_BY_CONTENT: JsonKeys.SPLIT_BY_CONTENT_ASSET},
                                             split_label=split_label,
                                             language=language)



class TestReportRunWithNetworkApplianceSplit(ReportRunSplitMixin, ReportRunningMixin):
    """Test report run with a Network Appliance split.
    This tests the split with each of the supported time resolutions and also
    checks the localised results. The tests run can be found in the @ref
    ReportRunSplitMixin."""

    ## Unlocalised split label used when there isn't a time split
    split_label = 'Delivery Appliance'

    def check_split(self, time_resolution, split_label=None, language=None):
        """Helper method for checking a time resolution with a network delivery appliance split.
        @param time_resolution - time resolution to use
        @param split_label - split label if not "Time" or "Date"
        @param language - optional language
        """
        self.check_time_resolution_and_split(time_resolution,
                                             {JsonKeys.SPLIT_BY_NETWORK: JsonKeys.SPLIT_BY_NETWORK_DELIVERY_APPLIANCE},
                                             split_label=split_label,
                                             language=language)



class TestReportRunWithNetworkTierSplit(ReportRunSplitMixin, ReportRunningMixin):
    """Test report run with a Network Tier split.
    This tests the split with each of the supported time resolutions and also
    checks the localised results. The tests run can be found in the @ref
    ReportRunSplitMixin."""

    ## Unlocalised split label used when there isn't a time split
    split_label = 'Tier'

    def check_split(self, time_resolution, split_label=None, language=None):
        """Helper method for checking a time resolution with a network delivery appliance split.
        @param time_resolution - time resolution to use
        @param split_label - split label if not "Time" or "Date"
        @param language - optional language
        """
        self.check_time_resolution_and_split(time_resolution,
                                             {JsonKeys.SPLIT_BY_NETWORK: JsonKeys.SPLIT_BY_NETWORK_TIER},
                                             split_label=split_label,
                                             language=language,
                                             report_id='tier-analysis')


class TestReportRunWithSendingHostnameSplit(ReportRunSplitMixin, ReportRunningMixin):
    """Test report run with a Sending Appliance Hostname split.
    This tests the split with each of the supported time resolutions and also
    checks the localised results. The tests run can be found in the @ref
    ReportRunSplitMixin."""

    ## Unlocalised split label used when there isn't a time split
    split_label = 'Sending Appliance Hostname'

    def check_split(self, time_resolution, split_label=None, language=None):
        """Helper method for checking a time resolution with a network delivery appliance split.
        @param time_resolution - time resolution to use
        @param split_label - split label if not "Time" or "Date"
        @param language - optional language
        """
        self.check_time_resolution_and_split(time_resolution,
                                             {JsonKeys.SPLIT_BY_NETWORK: JsonKeys.SPLIT_BY_NETWORK_SENDING_APPLIANCE_HOSTNAME},
                                             split_label=split_label,
                                             language=language,
                                             report_id='tier-analysis')


class TestReportRunWithSendingIpAddressSplit(ReportRunSplitMixin, ReportRunningMixin):
    """Test report run with a Sending Appliance IP Address split.
    This tests the split with each of the supported time resolutions and also
    checks the localised results. The tests run can be found in the @ref
    ReportRunSplitMixin."""

    ## Unlocalised split label used when there isn't a time split
    split_label = 'Sending Appliance IP Address'

    def check_split(self, time_resolution, split_label=None, language=None):
        """Helper method for checking a time resolution with a network delivery appliance split.
        @param time_resolution - time resolution to use
        @param split_label - split label if not "Time" or "Date"
        @param language - optional language
        """
        self.check_time_resolution_and_split(time_resolution,
                                             {JsonKeys.SPLIT_BY_NETWORK: JsonKeys.SPLIT_BY_NETWORK_SENDING_APPLIANCE_IP_ADDRESS},
                                             split_label=split_label,
                                             language=language,
                                             report_id='tier-analysis')


class TestReportRunWithReceivingHostnameSplit(ReportRunSplitMixin, ReportRunningMixin):
    """Test report run with a Receiving Appliance Hostname split.
    This tests the split with each of the supported time resolutions and also
    checks the localised results. The tests run can be found in the @ref
    ReportRunSplitMixin."""

    ## Unlocalised split label used when there isn't a time split
    split_label = 'Receiving Appliance Hostname'

    def check_split(self, time_resolution, split_label=None, language=None):
        """Helper method for checking a time resolution with a network delivery appliance split.
        @param time_resolution - time resolution to use
        @param split_label - split label if not "Time" or "Date"
        @param language - optional language
        """
        self.check_time_resolution_and_split(time_resolution,
                                             {JsonKeys.SPLIT_BY_NETWORK: JsonKeys.SPLIT_BY_NETWORK_RECEIVING_APPLIANCE_HOSTNAME},
                                             split_label=split_label,
                                             language=language,
                                             report_id='tier-analysis')


class TestReportRunWithReceivingIpAddressSplit(ReportRunSplitMixin, ReportRunningMixin):
    """Test report run with a Receiving Appliance IP Address split.
    This tests the split with each of the supported time resolutions and also
    checks the localised results. The tests run can be found in the @ref
    ReportRunSplitMixin."""

    ## Unlocalised split label used when there isn't a time split
    split_label = 'Receiving Appliance IP Address'

    def check_split(self, time_resolution, split_label=None, language=None):
        """Helper method for checking a time resolution with a network delivery appliance split.
        @param time_resolution - time resolution to use
        @param split_label - split label if not "Time" or "Date"
        @param language - optional language
        """
        self.check_time_resolution_and_split(time_resolution,
                                             {JsonKeys.SPLIT_BY_NETWORK: JsonKeys.SPLIT_BY_NETWORK_RECEIVING_APPLIANCE_IP_ADDRESS},
                                             split_label=split_label,
                                             language=language,
                                             report_id='tier-analysis')



class TestReportRunWithHttpStatusSplit(ReportRunSplitMixin, ReportRunningMixin):
    """Test report run with an HTTP Status split.
    This tests the split with each of the supported time resolutions and also
    checks the localised results. The tests run can be found in the @ref
    ReportRunSplitMixin."""

    ## Unlocalised split label used when there isn't a time split
    split_label = 'HTTP Success or Failure'

    def check_split(self, time_resolution, split_label=None, language=None):
        """Helper method for checking a time resolution with an HTTP status success / failure split.
        @param time_resolution - time resolution to use
        @param split_label - split label if not "Time" or "Date"
        @param language - optional language
        """
        self.check_time_resolution_and_split(time_resolution,
                                             {JsonKeys.SPLIT_BY_HTTP_STATUS: JsonKeys.SPLIT_BY_HTTP_STATUS_SUCCESS_FAILURE},
                                             split_label=split_label,
                                             language=language)



class TestReportRunWithHttpClassSplit(ReportRunSplitMixin, ReportRunningMixin):
    """Test report run with an HTTP Class split.
    This tests the split with each of the supported time resolutions and also
    checks the localised results. The tests run can be found in the @ref
    ReportRunSplitMixin."""

    ## Unlocalised split label used when there isn't a time split
    split_label = 'HTTP Status Class'

    def check_split(self, time_resolution, split_label=None, language=None):
        """Helper method for checking a time resolution with an HTTP status code class split.
        @param time_resolution - time resolution to use
        @param split_label - split label if not "Time" or "Date"
        @param language - optional language
        """
        self.check_time_resolution_and_split(time_resolution,
                                             {JsonKeys.SPLIT_BY_HTTP_STATUS: JsonKeys.SPLIT_BY_HTTP_STATUS_CODE_CLASS},
                                             split_label=split_label,
                                             language=language)


class TestReportRunWithNetworkResponseStatusSplit(ReportRunSplitMixin,
                                                  ReportRunningMixin):
    """Test report run with an Network Response Status Code split.
    This tests the split with each of the supported time resolutions and also
    checks the localised results. The tests run can be found in the @ref
    ReportRunSplitMixin."""

    ## Unlocalised split label used when there isn't a time split
    split_label = 'Network Response Success or Failure'

    def check_split(self, time_resolution, split_label=None, language=None):
        """Helper method for checking a time resolution with a network response
           status success / failure split.
        @param time_resolution - time resolution to use
        @param split_label - split label if not "Time" or "Date"
        @param language - optional language
        """
        self.check_time_resolution_and_split(
            time_resolution,
            {JsonKeys.SPLIT_BY_NETWORK_RESPONSE:
             JsonKeys.SPLIT_BY_NETWORK_RESPONSE_SUCCESS_FAILURE},
            split_label=split_label, language=language,
            report_id='tier-analysis')


class TestReportRunWithNetworkResponseClassSplit(ReportRunSplitMixin,
                                                 ReportRunningMixin):
    """Test report run with a Network Response Class split.
    This tests the split with each of the supported time resolutions and also
    checks the localised results. The tests run can be found in the @ref
    ReportRunSplitMixin."""

    ## Unlocalised split label used when there isn't a time split
    split_label = 'Network Response Class'

    def check_split(self, time_resolution, split_label=None, language=None):
        """Helper method for checking a time resolution with a network response
           status code class split.
        @param time_resolution - time resolution to use
        @param split_label - split label if not "Time" or "Date"
        @param language - optional language
        """
        self.check_time_resolution_and_split(
            time_resolution,
            {JsonKeys.SPLIT_BY_NETWORK_RESPONSE:
             JsonKeys.SPLIT_BY_NETWORK_RESPONSE_CODE_CLASS},
            split_label=split_label, language=language,
            report_id='tier-analysis')


class TestReportRunWithNetworkResponseCodeSplit(ReportRunSplitMixin,
                                                ReportRunningMixin):
    """Test report run with a Network Response Code split.
    This tests the split with each of the supported time resolutions and also
    checks the localised results. The tests run can be found in the @ref
    ReportRunSplitMixin."""

    ## Unlocalised split label used when there isn't a time split
    split_label = 'Network Response Code'

    def check_split(self, time_resolution, split_label=None, language=None):
        """Helper method for checking a time resolution with a network response
           status code class split.
        @param time_resolution - time resolution to use
        @param split_label - split label if not "Time" or "Date"
        @param language - optional language
        """
        self.check_time_resolution_and_split(
            time_resolution,
            {JsonKeys.SPLIT_BY_NETWORK_RESPONSE:
             JsonKeys.SPLIT_BY_NETWORK_RESPONSE_CODE},
            split_label=split_label, language=language,
            report_id='tier-analysis')


class TestReportRunWithHttpStatusCodeSplit(ReportRunSplitMixin, ReportRunningMixin):
    """Test report run with an HTTP Status Code split.
    This tests the split with each of the supported time resolutions and also
    checks the localised results. The tests run can be found in the @ref
    ReportRunSplitMixin."""

    ## Unlocalised split label used when there isn't a time split
    split_label = 'HTTP Status Code'

    def check_split(self, time_resolution, split_label=None, language=None):
        """Helper method for checking a time resolution with an HTTP status code split.
        @param time_resolution - time resolution to use
        @param split_label - split label if not "Time" or "Date"
        @param language - optional language
        """
        self.check_time_resolution_and_split(time_resolution,
                                             {JsonKeys.SPLIT_BY_HTTP_STATUS: JsonKeys.SPLIT_BY_HTTP_STATUS_CODE},
                                             split_label=split_label,
                                             language=language)


class TestReportRunWithCacheHitStatusSplit(ReportRunSplitMixin, ReportRunningMixin):
    """Test report run with a Cache Hit Status split.
    This tests the split with each of the supported time resolutions and also
    checks the localised results. The tests run can be found in the @ref
    ReportRunSplitMixin."""

    ## Unlocalised split label used when there isn't a time split
    split_label = 'Cache Hit or Miss'

    def check_split(self, time_resolution, split_label=None, language=None):
        """Helper method for checking a time resolution with a cache status hit / miss split.
        @param time_resolution - time resolution to use
        @param split_label - split label if not "Time" or "Date"
        @param language - optional language
        """
        self.check_time_resolution_and_split(time_resolution,
                                             {JsonKeys.SPLIT_BY_CACHE_STATUS: JsonKeys.SPLIT_BY_CACHE_STATUS_HIT},
                                             split_label=split_label,
                                             language=language)



class TestReportRunWithCacheStatusDispositionSplit(ReportRunSplitMixin, ReportRunningMixin):
    """Test report run with a Cache Dispostion split.
    This tests the split with each of the supported time resolutions and also
    checks the localised results. The tests run can be found in the @ref
    ReportRunSplitMixin."""

    ## Unlocalised split label used when there isn't a time split
    split_label = 'Cache Status'

    def check_split(self, time_resolution, split_label=None, language=None):
        """Helper method for checking a time resolution with a cache status disposition split.
        @param time_resolution - time resolution to use
        @param split_label - split label if not "Time" or "Date"
        @param language - optional language
        """
        self.check_time_resolution_and_split(time_resolution,
                                             {JsonKeys.SPLIT_BY_CACHE_STATUS: JsonKeys.SPLIT_BY_CACHE_STATUS_DISPOSITION},
                                             split_label=split_label,
                                             language=language)



class TestReportRunTimeRangeWarnings(ReportRunningMixin):
    """Test Time Range warnings when running a report."""

    def check_time_range_warning(self, start_time, time_resolution, metric, warning=None, language=None):
        """Check POSTing with a metric
        @param start_time - start time
        @param time_resolution - time resolution to use, medium and high
                                 resolutions should constrain the time.
        @param metric - metric to use, peak metrics should constrain the time.
        @param warning - expected warning text
        @param language - optional language
        @return JSON properties
        """
        params = {
            JsonKeys.START_TIME: start_time,
            JsonKeys.END_TIME: "now",
            JsonKeys.TIME_RESOLUTION: time_resolution,
            JsonKeys.METRIC: metric
        }

        json_props = self.check_report_run(parameters=params, language=language)
        # Check the report job contains the correct label

        if warning is None:
            self.assertFalse(json_props.has_key('messages'),
                             "There should not be a messages section.")
        else:
            self.assertEqual(len(json_props['messages']), 1, "One error message is expected")
            message = json_props['messages'][0]
            self.assertEqual(message['summary'], warning, "Message summary is incorrect")
            self.assertEqual(message['severity'], "warning", "Message severity is incorrect")
            self.assertIsNotNone(message['id'], "ID could be any hex string")



    def test_high_res_time_range_warning(self):
        """Test time range warning for a high time resolution.
        Time Resolution: High
        Metric: Non peak
        Start Time: Before main index faked start of 2014-01-13T10:38:30Z
        Expected Outcome: Warning given.
        """
        self.check_time_range_warning("2014-01-13T10:38:29Z",
                                      JsonKeys.TIME_RESOLUTION_30_MINUTES,
                                      JsonKeys.METRIC_BYTES_DELIVERED,
                                      warning = "Your requested time range has been shortened because the selected time resolution is only available from 1/13/14 10:38 AM.")

    def test_high_res_time_range_warning_localised(self):
        """Test time range warning for a high time resolution (localised).
        Time Resolution: High
        Metric: Non peak
        Start Time: Before main index faked start of 2014-01-13T10:38:30Z
        Expected Outcome: Warning given.
        """
        self.check_time_range_warning("2014-01-13T10:38:29Z",
                                      JsonKeys.TIME_RESOLUTION_30_MINUTES,
                                      JsonKeys.METRIC_BYTES_DELIVERED,
                                      warning = "!Your requested time range has been shortened because the selected !time resolution is only available from 2014-01-13 10.38.",
                                      language=TEST_LANG_CODE)

    def test_high_res_time_range_no_warning(self):
        """Test time range warning for a high time resolution.
        Time Resolution: High
        Metric: Non peak
        Start Time: Same as main index faked start of 2014-01-13T10:38:30Z
        Expected Outcome: No warning given.
        """
        self.check_time_range_warning("2014-01-13T10:38:30Z",
                                      JsonKeys.TIME_RESOLUTION_30_MINUTES,
                                      JsonKeys.METRIC_BYTES_DELIVERED)

    def test_medium_res_time_range_warning(self):
        """Test time range warning for a medium time resolution.
        Time Resolution: High
        Metric: Non peak
        Start Time: Before main index faked start of 2014-01-13T10:38:30Z
        Expected Outcome: Warning given.
        """
        self.check_time_range_warning("2014-01-13T00:00:00Z",
                                      JsonKeys.TIME_RESOLUTION_1_HOUR,
                                      JsonKeys.METRIC_REQUESTS_DELIVERED,
                                      warning = "Your requested time range has been shortened because the selected time resolution is only available from 1/13/14 10:38 AM.")

    def test_medium_res_time_range_warning_localised(self):
        """Test time range warning for a medium time resolution (localised).
        Time Resolution: High
        Metric: Non peak
        Start Time: Before main index faked start of 2014-01-13T10:38:30Z
        Expected Outcome: Warning given.
        """
        self.check_time_range_warning("2014-01-13T00:00:00Z",
                                      JsonKeys.TIME_RESOLUTION_1_HOUR,
                                      JsonKeys.METRIC_REQUESTS_DELIVERED,
                                      warning = "!Your requested time range has been shortened because the selected !time resolution is only available from 2014-01-13 10.38.",
                                      language=TEST_LANG_CODE)

    def test_medium_res_time_range_no_warning(self):
        """Test time range warning for a medium time resolution.
        Time Resolution: High
        Metric: Non peak
        Start Time: After main index faked start of 2014-01-13T10:38:30Z
        Expected Outcome: No warning given.
        """
        self.check_time_range_warning("2014-01-14T00:00:00Z",
                                      JsonKeys.TIME_RESOLUTION_1_HOUR,
                                      JsonKeys.METRIC_REQUESTS_DELIVERED)

    def test_low_res_time_range_no_warning(self):
        """Test that there is not a time range warning for a low time resolution.
        Time Resolution: Low
        Metric: Non peak
        Start Time: Before main index faked start of 2014-01-13T10:38:30Z
        Expected Outcome: No warning given.
        """
        self.check_time_range_warning("2013-01-01T00:00:00Z",
                                      JsonKeys.TIME_RESOLUTION_1_DAY,
                                      JsonKeys.METRIC_BYTES_DELIVERED)

    def test_peak_metric_time_range_warning(self):
        """Test time range warning for peak metrics.
        Time Resolution: Low
        Metric: Peak
        Start Time: Before main index faked start of 2014-01-13T10:38:30Z
        Expected Outcome: Warning given.
        """
        self.check_time_range_warning("2014-01-13T10:38:29Z",
                                      JsonKeys.TIME_RESOLUTION_1_DAY,
                                      JsonKeys.METRIC_PEAK_DELIVERED,
                                      warning = "Your requested time range has been shortened because the selected metric is only available from 1/13/14 10:38 AM.")

    def test_peak_metric_time_range_warning_localised(self):
        """Test time range warning for peak metrics (localised).
        Time Resolution: Low
        Metric: Peak
        Start Time: Before main index faked start of 2014-01-13T10:38:30Z
        Expected Outcome: Warning given.
        """
        self.check_time_range_warning("2014-01-13T10:38:29Z",
                                      JsonKeys.TIME_RESOLUTION_1_DAY,
                                      JsonKeys.METRIC_PEAK_DELIVERED,
                                      warning = "!Your requested time range has been shortened because the selected !metric is only available from 2014-01-13 10.38.",
                                      language=TEST_LANG_CODE)

    def test_peak_metric_time_range_no_warning(self):
        """Test time range warning for peak metrics.
        Time Resolution: Low
        Metric: Peak
        Start Time: Same as main index faked start of 2014-01-13T10:38:30Z
        Expected Outcome: No warning given.
        """
        self.check_time_range_warning("2014-01-13T10:38:30Z",
                                      JsonKeys.TIME_RESOLUTION_1_DAY,
                                      JsonKeys.METRIC_PEAK_DELIVERED)

    def test_high_time_res_and_peak_metric_time_range_warning(self):
        """Test time range warning for high time resolution and peak metrics.
        Time Resolution: High
        Metric: Peak
        Start Time: Before main index faked start of 2014-01-13T10:38:30Z
        Expected Outcome: Warning given.
        """
        self.check_time_range_warning("2014-01-13T10:38:29Z",
                                      JsonKeys.TIME_RESOLUTION_5_MINUTES,
                                      JsonKeys.METRIC_PEAK_REQUEST_RATE,
                                      warning = "Your requested time range has been shortened because the selected metric and time resolution is only available from 1/13/14 10:38 AM.")

    def test_high_time_res_and_peak_metric_time_range_warning_localised(self):
        """Test time range warning for high time resolution and peak metrics (localised).
        Time Resolution: High
        Metric: Peak
        Start Time: Before main index faked start of 2014-01-13T10:38:30Z
        Expected Outcome: Warning given.
        """
        self.check_time_range_warning("2014-01-13T10:38:29Z",
                                      JsonKeys.TIME_RESOLUTION_5_MINUTES,
                                      JsonKeys.METRIC_PEAK_REQUEST_RATE,
                                      warning = "!Your requested time range has been shortened because the selected !metric and time resolution is only available from 2014-01-13 10.38.",
                                      language=TEST_LANG_CODE)

    def test_high_time_res_and_peak_metric_time_range_no_warning(self):
        """Test time range warning for high time resolution and peak metrics.
        Time Resolution: High
        Metric: Peak
        Start Time: Same as main index faked start of 2014-01-13T10:38:30Z
        Expected Outcome: No warning given.
        """
        self.check_time_range_warning("2014-01-13T10:38:30Z",
                                      JsonKeys.TIME_RESOLUTION_5_MINUTES,
                                      JsonKeys.METRIC_PEAK_REQUEST_RATE)



class TestReportRunMetrics(ReportRunningMixin):
    """Test Time Range warnings when running a report."""

    def check_metric(self, metric, language=None, split=False, time=True, report_id='traffic'):
        """Check POSTing with a metric.
        This method also checks that the samples size of peak metrics is handled
        appropriately.

        @param metric - which metric to use
        @param language - optional language
        @param split - set to True to run a report with a split
        @param time - set to False to disable splitting by time
        @param report_id - report to test, defaults to the @c traffic report.
        @return JSON properties
        """
        params = {
            JsonKeys.METRIC: metric
        }

        # Add a split which will change the the sample size results processed.
        if split:
            params.update({
                JsonKeys.SPLIT_BY: {
                    JsonKeys.SPLIT_BY_NETWORK: JsonKeys.SPLIT_BY_NETWORK_DELIVERY_APPLIANCE
                }
            })
        else:
            params.update({ JsonKeys.SPLIT_BY: {} })


        # Turn off the time split which will change the the sample size results processed.
        if not time:
            params.update({
                JsonKeys.TIME_RESOLUTION: JsonKeys.TIME_RESOLUTION_NONE
            })

        json_props = self.check_report_run(parameters=params,
                                           language=language,
                                           report_id=report_id)

        # Check the report job contains the correct metric
        self.assertEqual(json_props[JsonKeys.METRIC], metric)

        # If the expected results have samples check they are correct otherwise
        # check that there aren't any samples in the results.
        try:
            expected_samples = self.expected_results[JsonKeys.RESULT_SAMPLES]
        except KeyError:
            self.assertFalse(json_props.has_key(JsonKeys.RESULT_SAMPLES), "Only peak metrics should have result samples")
        else:
            self.assertEqual(json_props[JsonKeys.RESULT_SAMPLES], expected_samples, "Peak metric samples not as expected")

        return json_props

    def test_valid_metric_bytes(self):
        json_props = self.check_metric(JsonKeys.METRIC_BYTES_DELIVERED)
        self.assertEqual(json_props[JsonKeys.METRIC_LABEL], 'Bytes delivered')
        self.assertEqual(json_props[JsonKeys.METRIC_UNITS], {JsonKeys.METRIC_UNITS_SHORT: u'B', JsonKeys.METRIC_UNITS_LONG: u'bytes'})

    def test_valid_metric_bytes_localised(self):
        json_props = self.check_metric(JsonKeys.METRIC_BYTES_DELIVERED, language=TEST_LANG_CODE)
        # Check the report job contains the correct label
        self.assertEqual(json_props[JsonKeys.METRIC_LABEL], '!Bytes delivered')
        self.assertEqual(json_props[JsonKeys.METRIC_UNITS], {JsonKeys.METRIC_UNITS_SHORT: u'!B', JsonKeys.METRIC_UNITS_LONG: u'!bytes'})


    def test_valid_metric_count(self):
        json_props = self.check_metric(JsonKeys.METRIC_REQUESTS_DELIVERED)
        self.assertEqual(json_props[JsonKeys.METRIC_LABEL], 'Request count')
        self.assertEqual(json_props[JsonKeys.METRIC_UNITS], {})

    def test_valid_metric_count_localised(self):
        json_props = self.check_metric(JsonKeys.METRIC_REQUESTS_DELIVERED, language=TEST_LANG_CODE)
        # Check the report job contains the correct label
        self.assertEqual(json_props[JsonKeys.METRIC_LABEL], '!Request count')
        self.assertEqual(json_props[JsonKeys.METRIC_UNITS], {})


    def test_valid_metric_peak_throughput(self):
        json_props = self.check_metric(JsonKeys.METRIC_PEAK_DELIVERED)
        self.assertEqual(json_props[JsonKeys.METRIC_LABEL], "Peak throughput (bps)")
        self.assertEqual(json_props[JsonKeys.METRIC_UNITS], {JsonKeys.METRIC_UNITS_SHORT: u'bps', JsonKeys.METRIC_UNITS_LONG: u'bits per second'})

    def test_valid_metric_peak_throughput_localised(self):
        json_props = self.check_metric(JsonKeys.METRIC_PEAK_DELIVERED, language=TEST_LANG_CODE)
        self.assertEqual(json_props[JsonKeys.METRIC_LABEL], "!Peak throughput (bps)")
        self.assertEqual(json_props[JsonKeys.METRIC_UNITS], {JsonKeys.METRIC_UNITS_SHORT: u'!bps', JsonKeys.METRIC_UNITS_LONG: u'!bits per second'})


    def test_valid_metric_peak_throughput_95th_percentile(self):
        json_props = self.check_metric(JsonKeys.METRIC_95TH_PERCENTILE_DELIVERED)
        self.assertEqual(json_props[JsonKeys.METRIC_LABEL], "95th percentile throughput (bps)")
        self.assertEqual(json_props[JsonKeys.METRIC_UNITS], {JsonKeys.METRIC_UNITS_SHORT: u'bps', JsonKeys.METRIC_UNITS_LONG: u'bits per second'})

    def test_valid_metric_peak_throughput_95th_percentile_localised(self):
        json_props = self.check_metric(JsonKeys.METRIC_95TH_PERCENTILE_DELIVERED, language=TEST_LANG_CODE)
        self.assertEqual(json_props[JsonKeys.METRIC_LABEL], "!95th percentile throughput (bps)")
        self.assertEqual(json_props[JsonKeys.METRIC_UNITS], {JsonKeys.METRIC_UNITS_SHORT: u'!bps', JsonKeys.METRIC_UNITS_LONG: u'!bits per second'})


    def test_valid_metric_peak_request_rate(self):
        json_props = self.check_metric(JsonKeys.METRIC_PEAK_REQUEST_RATE)
        self.assertEqual(json_props[JsonKeys.METRIC_LABEL], "Peak request rate (rps)")
        self.assertEqual(json_props[JsonKeys.METRIC_UNITS], {JsonKeys.METRIC_UNITS_SHORT: u'rps', JsonKeys.METRIC_UNITS_LONG: u'requests per second'})

    def test_valid_metric_peak_request_rate_localised(self):
        json_props = self.check_metric(JsonKeys.METRIC_PEAK_REQUEST_RATE, language=TEST_LANG_CODE)
        self.assertEqual(json_props[JsonKeys.METRIC_LABEL], "!Peak request rate (rps)")
        self.assertEqual(json_props[JsonKeys.METRIC_UNITS], {JsonKeys.METRIC_UNITS_SHORT: u'!rps', JsonKeys.METRIC_UNITS_LONG: u'!requests per second'})


    def test_valid_metric_unique_ip_address(self):
        json_props = self.check_metric(JsonKeys.METRIC_UNIQUE_IP_ADDRESS)
        self.assertEqual(json_props[JsonKeys.METRIC_LABEL], 'Unique IP address')
        self.assertEqual(json_props[JsonKeys.METRIC_UNITS], {})

    def test_valid_metric_unique_ip_address_localised(self):
        json_props = self.check_metric(JsonKeys.METRIC_UNIQUE_IP_ADDRESS, language=TEST_LANG_CODE)
        # Check the report job contains the correct label
        self.assertEqual(json_props[JsonKeys.METRIC_LABEL], '!Unique IP address')
        self.assertEqual(json_props[JsonKeys.METRIC_UNITS], {})

    def test_valid_metric_bytes_transferred(self):
        json_props = self.check_metric(JsonKeys.METRIC_BYTES_TRANSFERRED, report_id='tier-analysis')
        self.assertEqual(json_props[JsonKeys.METRIC_LABEL], 'Bytes transferred')
        self.assertEqual(json_props[JsonKeys.METRIC_UNITS], {JsonKeys.METRIC_UNITS_SHORT: u'B', JsonKeys.METRIC_UNITS_LONG: u'bytes'})

    def test_valid_metric_bytes_transferred_localised(self):
        json_props = self.check_metric(JsonKeys.METRIC_BYTES_TRANSFERRED, language=TEST_LANG_CODE, report_id='tier-analysis')
        # Check the report job contains the correct label
        self.assertEqual(json_props[JsonKeys.METRIC_LABEL], '!Bytes transferred')
        self.assertEqual(json_props[JsonKeys.METRIC_UNITS], {JsonKeys.METRIC_UNITS_SHORT: u'!B', JsonKeys.METRIC_UNITS_LONG: u'!bytes'})

    def test_valid_metric_peak_bytes_transferred(self):
        json_props = self.check_metric(JsonKeys.METRIC_PEAK_TRANSFERRED, report_id='tier-analysis')
        self.assertEqual(json_props[JsonKeys.METRIC_LABEL], 'Peak throughput (bps)')
        self.assertEqual(json_props[JsonKeys.METRIC_UNITS], {JsonKeys.METRIC_UNITS_SHORT: u'bps', JsonKeys.METRIC_UNITS_LONG: u'bits per second'})

    def test_valid_metric_peak_bytes_transferred_localised(self):
        json_props = self.check_metric(JsonKeys.METRIC_PEAK_TRANSFERRED, language=TEST_LANG_CODE, report_id='tier-analysis')
        # Check the report job contains the correct label
        self.assertEqual(json_props[JsonKeys.METRIC_LABEL], '!Peak throughput (bps)')
        self.assertEqual(json_props[JsonKeys.METRIC_UNITS], {JsonKeys.METRIC_UNITS_SHORT: u'!bps', JsonKeys.METRIC_UNITS_LONG: u'!bits per second'})


    def test_valid_metric_95th_percentile_bytes_transferred(self):
        json_props = self.check_metric(JsonKeys.METRIC_95TH_PERCENTILE_TRANSFERRED, report_id='tier-analysis')
        self.assertEqual(json_props[JsonKeys.METRIC_LABEL], '95th percentile throughput (bps)')
        self.assertEqual(json_props[JsonKeys.METRIC_UNITS], {JsonKeys.METRIC_UNITS_SHORT: u'bps', JsonKeys.METRIC_UNITS_LONG: u'bits per second'})

    def test_valid_metric_95th_percentile_bytes_transferred_localised(self):
        json_props = self.check_metric(JsonKeys.METRIC_95TH_PERCENTILE_TRANSFERRED, language=TEST_LANG_CODE, report_id='tier-analysis')
        # Check the report job contains the correct label
        self.assertEqual(json_props[JsonKeys.METRIC_LABEL], '!95th percentile throughput (bps)')
        self.assertEqual(json_props[JsonKeys.METRIC_UNITS], {JsonKeys.METRIC_UNITS_SHORT: u'!bps', JsonKeys.METRIC_UNITS_LONG: u'!bits per second'})

    def test_valid_metric_requests_transferred(self):
        json_props = self.check_metric(JsonKeys.METRIC_REQUESTS_TRANSFERRED, report_id='tier-analysis')
        self.assertEqual(json_props[JsonKeys.METRIC_LABEL], 'Request count')
        self.assertEqual(json_props[JsonKeys.METRIC_UNITS], {})

    def test_valid_metric_requests_transferred_localised(self):
        json_props = self.check_metric(JsonKeys.METRIC_REQUESTS_TRANSFERRED, language=TEST_LANG_CODE, report_id='tier-analysis')
        self.assertEqual(json_props[JsonKeys.METRIC_LABEL], '!Request count')
        self.assertEqual(json_props[JsonKeys.METRIC_UNITS], {})

    def test_peak_metric_time_no_splits(self):
        """Check the peak metric sample size handling with split by time."""
        json_props = self.check_metric(JsonKeys.METRIC_PEAK_DELIVERED, split=False, time=True)
        self.assertEqual(json_props[JsonKeys.METRIC_LABEL], "Peak throughput (bps)")

    def test_peak_metric_time_and_split(self):
        """Check the peak metric sample size handling with split by time and antoher dimension."""
        json_props = self.check_metric(JsonKeys.METRIC_95TH_PERCENTILE_DELIVERED, split=True)
        self.assertEqual(json_props[JsonKeys.METRIC_LABEL], "95th percentile throughput (bps)")

    def test_peak_metric_no_time_but_split(self):
        """Check the peak metric sample size handling with split by a dimensions other than time."""
        json_props = self.check_metric(JsonKeys.METRIC_PEAK_REQUEST_RATE, split=True, time=False)
        self.assertEqual(json_props[JsonKeys.METRIC_LABEL], "Peak request rate (rps)")

    def test_peak_metric_no_splits(self):
        """Check the peak metric sample size handling not split at all."""
        json_props = self.check_metric(JsonKeys.METRIC_PEAK_DELIVERED, split=False, time=False)
        self.assertEqual(json_props[JsonKeys.METRIC_LABEL], "Peak throughput (bps)")



class ReportParameterValidationMixin:
    """Support for API Report Parameters tests."""

    report_path = None  # concrete tests must set the report path

    def check_invalid_params(self, params, expected_error_code,
                             expected_error_msg, expected_error_details=None):
        """Check that posting an invalid body results in
        appropriate error.

        @param params - the JSON request body to POST to UNAPI server.
        @param expected_error_code - the error code that is expected to be
        found in the error response that comes back from UNAPI.
        @param expected_error_msg - a string that is expected to be found
        within the error message that comes back from UNAPI.
        @param expected_error_details - the error details that are expected to
        be found in the error response that comes back from UNAPI.
        """
        self.check_bad_request_post(
            Profiles.REPORT_JOB_ITEM, self.report_path, params,
            expected_error_code, expected_error_msg, expected_error_details)

    def check_valid_params(self, params):
        """Check that posting a valid body does not result in error"""
        # Try POSTing these params and ensure that the UNAPI
        # server doesn't fail to process them.
        _, response_json = self.http_post(
            Profiles.REPORT_JOB_ITEM, self.report_path, params)
        self.assertIn(JsonKeys.RESULT_FIELDS, response_json)
        self.assertIn(JsonKeys.RESULT_COLUMNS, response_json)
        self.assertIn(JsonKeys.RESULT_PERCENTAGE_DONE, response_json)
        self.assertIn(JsonKeys.RESULT_IS_DONE, response_json)
        self.assertIn('_links', response_json)
        return response_json


class TestGenericReportParameterValidation(
        ReportParameterValidationMixin, SetupUnapiFlaskAppForReporting):
    """Test that Report Parameters are validated at the API level."""

    report_path = '/reporting/reports/traffic/jobs'

    def test_invalid_num(self):
        self.check_invalid_params(1, 2001, 'JSON is not an object literal')

    def test_invalid_str(self):
        self.check_invalid_params('foo', 2001, 'JSON is not an object literal')

    def test_invalid_bool(self):
        self.check_invalid_params(True, 2001, 'JSON is not an object literal')

    def test_invalid_none(self):
        self.check_invalid_params(None, 2001, 'JSON is not an object literal')

    def test_invalid_parameters(self):
        self.check_invalid_params(
            {
                'foo': 'bar'
            },
            4002, # Unrecognised property error
            "Additional properties are not allowed ('foo' was unexpected)",
            {u'property': [u'foo']}
        )

    def test_invalid_start_time(self):
        self.check_invalid_params(
            {
                'startTime': 'foo'
            },
            4004, # Value error
            "'foo' is not valid under any of the given schemas",
            {u'property': [u'startTime']}
        )

    def test_invalid_end_time(self):
        self.check_invalid_params(
            {
                'endTime': 'foo',
            },
            4004, # Value error
            "'foo' is not valid under any of the given schemas",
            {u'property': [u'endTime']}
        )

    def test_invalid_month(self):
        """Test validation of an invalid month (13th month)
        Part of testlink testcase id 1.0-10768.
        The rest of the test cases for date-times with invalid parts are covered
        by the vxformats unittests. This single test is enough to ensure that validation
        failures of date-times in such format are exposed from the UNAPI in the
        right way. No point exhaustively repeating all the cases here.
        """
        self.check_invalid_params(
            {
                'startTime': '2012-13-01T12:30:05Z'
            },
            4004, # Value error
            "'2012-13-01T12:30:05Z' is not valid under any of the given schemas"
        )

    def test_invalid_future_start_time(self):
        self.check_invalid_params(
            {
                'startTime': '2078-06-30T12:30:05Z'
            },
            4004, # Value error
            "The start time is later than the end time",
            {u'property': [u'startTime']}
        )

    def test_invalid_start_time_of_now(self):
        self.check_invalid_params(
            {
                'startTime': 'now',
                'endTime': '2078-06-30T12:30:05Z'
            },
            4004, # Value error
            "The start time cannot be now",
            {u'property': [u'startTime']}
        )

    def test_start_time_later_than_end_time(self):
        """End time before start time.
        Part of testlink testcase id 1.0-10768.
        """
        self.check_invalid_params(
            {
                'startTime': '2013-06-30T12:30:05Z',
                'endTime': '2013-06-28T12:30:05Z'
            },
            4004, # Value error
            "The start time is later than the end time"
        )

    def test_same_start_and_end_time(self):
        """Same start and end time.
        Part of testlink testcase id 1.0-10768.
        """
        self.check_invalid_params(
            {
                'startTime': '2013-02-05T11:23:58Z',
                'endTime':   '2013-02-05T11:23:58Z',
            },
            4004, # Value error
            "The start time is the same as the end time",
            {u'property': [u'startTime']}
        )

    def test_valid_absolute_start_time(self):
        self.check_valid_params({
            'startTime': '1978-06-30T12:30:05Z'
        })

    def test_valid_absolute_end_time_of_now(self):
        self.check_valid_params({
            'endTime': 'now'
        })

    def test_valid_relative_times_days(self):
        self.check_valid_params({
            'startTime': '7days',
            'endTime': '1day'
        })

    def test_valid_relative_times_minutes(self):
        self.check_valid_params({
            'startTime': '10minutes',
            'endTime': '1minute'
        })

    def test_valid_relative_times_hours(self):
        self.check_valid_params({
            'startTime': '12hours',
            'endTime': '1hour'
        })

    def test_valid_relative_times_weeks(self):
        self.check_valid_params({
            'startTime': '2weeks',
            'endTime': '1week'
        })

    def test_valid_relative_times_months(self):
        self.check_valid_params({
            'startTime': '6months',
            'endTime': '1month'
        })

    def test_valid_relative_times_querters(self):
        self.check_valid_params({
            'startTime': '3quarters',
            'endTime': '1quarter'
        })

    def test_valid_relative_times_years(self):
        self.check_valid_params({
            'startTime': '5years',
            'endTime': '1year'
        })


    def test_invalid_end_time_before_start_time(self):
        self.check_invalid_params(
            {
                'endTime': '1978-06-30T12:30:05Z'
            },
            4004, # Value error
            "The start time is later than the end time",
            {u'property': [u'startTime']}
        )

    def test_valid_absolute_start_and_end_time(self):
        self.check_valid_params({
            'startTime': '1978-06-30T12:30:05Z',
            'endTime': '2078-06-30T12:30:05Z'
        })

    def test_missing_relative_start_time(self):
        """Only relative end date given"""
        self.check_valid_params({
                'endTime':   '0days',
        })

    def test_valid_organization_link(self):
        self.check_valid_params({
            JsonKeys.LINKS: {
                LinkRel.FILTER_BY: {
                    JsonKeys.LINK_HREF : '/content/orgs/1',
                    JsonKeys.LINK_PROFILE : Profiles.ORGANIZATION_ITEM
                }
            }
        })

    def test_valid_time_resolution_none(self):
        self.check_valid_params({
            'timeResolution': 'noTimeSplit'
        })

    def test_valid_time_resolution_1minute(self):
        self.check_valid_params({
            'timeResolution': '1minute'
        })

    def test_valid_time_resolution_5minutes(self):
        self.check_valid_params({
            'timeResolution': '5minutes'
        })

    def test_valid_time_resolution_15minutes(self):
        self.check_valid_params({
            'timeResolution': '15minutes'
        })

    def test_valid_time_resolution_30minutes(self):
        self.check_valid_params({
            'timeResolution': '30minutes'
        })

    def test_valid_time_resolution_1day(self):
        self.check_valid_params({
            'timeResolution': '1day'
        })

    def test_valid_time_resolution_1week(self):
        self.check_valid_params({
            'timeResolution': '1week'
        })

    def test_valid_time_resolution_1month(self):
        self.check_valid_params({
            'timeResolution': '1month'
        })

    def test_valid_time_resolution_1quarter(self):
        self.check_valid_params({
            'timeResolution': '1quarter'
        })

    def test_valid_time_resolution_1year(self):
        self.check_valid_params({
            'timeResolution': '1year'
        })

    def test_invalid_time_resolution(self):
        self.check_invalid_params(
            {
                'timeResolution': '2hours'
            },
            4004, # Value error
            "'2hours' is not one of",
            {u'property': [u'timeResolution']}
        )

    def test_valid_organization_links(self):
        self.check_valid_params({
            JsonKeys.LINKS: {
                LinkRel.FILTER_BY: [
                    {
                        JsonKeys.LINK_HREF : '/content/orgs/1',
                        JsonKeys.LINK_PROFILE : Profiles.ORGANIZATION_ITEM
                    },
                    {
                        JsonKeys.LINK_HREF : '/content/orgs/2',
                        JsonKeys.LINK_PROFILE : Profiles.ORGANIZATION_ITEM
                    },
                ]
             }
        })

    def test_invalid_link_rel(self):
        self.check_invalid_params({
            JsonKeys.LINKS: {
                LinkRel.SELF: {
                    JsonKeys.LINK_HREF : '/content/orgs/1',
                    JsonKeys.LINK_PROFILE : Profiles.ORGANIZATION_ITEM
                }
            }
        },
        3002, # Unrecognised relationship
        "Link relationship is not allowed for this resource, path: '_links/self'")

    def test_invalid_link_href(self):
        self.check_invalid_params({
            JsonKeys.LINKS: {
                LinkRel.FILTER_BY: {
                    JsonKeys.LINK_HREF : '/content/orgs/666',
                    JsonKeys.LINK_PROFILE : Profiles.ORGANIZATION_ITEM
                }
            }
        },
        3101, # HREF does not exist
        "The following link href does not exist: '/content/orgs/666', path: '_links/{0}'".format(
            LinkRel.FILTER_BY))

    def test_invalid_link_href_syntax(self):
        """JSON Schema validation should fail to validate this as a "uri"
        format.
        """
        self.check_invalid_params({
            JsonKeys.LINKS: {
                LinkRel.FILTER_BY: {
                    JsonKeys.LINK_HREF : u'\u00a3\u20ac',
                    JsonKeys.LINK_PROFILE : Profiles.ORGANIZATION_ITEM
                }
            }
        },
        4004, # Value error
        "is not valid under any of the given schemas")

    def test_invalid_link_profile(self):
        self.check_invalid_params({
            JsonKeys.LINKS: {
                LinkRel.FILTER_BY: {
                    JsonKeys.LINK_HREF : '/content/orgs/1',
                    JsonKeys.LINK_PROFILE : Profiles.ORGANIZATION_COLLECTION
                }
            }
        },
        3201, # Profile is wrong for link relationship
        "Profile is not valid for this relationship, expected: '{0}', path: '_links/{1}'".format(
            Profiles.ORGANIZATION_ITEM, LinkRel.FILTER_BY))

    def test_valid_filter_by_owns_link(self):
        self.check_valid_params({
            JsonKeys.LINKS: {
                LinkRel.FILTER_BY_OWNS: {
                    JsonKeys.LINK_HREF : '/content/orgs/1',
                    JsonKeys.LINK_PROFILE : Profiles.ORGANIZATION_ITEM
                }
            }
        })


class TestCDNReportingReportParameterValidation(
        ReportParameterValidationMixin, SetupUnapiFlaskAppForReporting):
    """Test that Report Parameters are validated at the API level.

    These tests are specific to the CDN Reporting pack reports.

    """
    report_path = '/reporting/reports/traffic/jobs'

    FILTER_BY_CACHE_DISPOSITIONS = [JsonKeys.FILTER_BY_CACHE_DISPOSITION_HIT,
                                    JsonKeys.FILTER_BY_CACHE_MEM_HIT,
                                    JsonKeys.FILTER_BY_CACHE_HIT,
                                    JsonKeys.FILTER_BY_CACHE_VXICP_HIT,
                                    JsonKeys.FILTER_BY_CACHE_REVALIDATED_MEM_HIT,
                                    JsonKeys.FILTER_BY_CACHE_REVALIDATED_HIT,
                                    JsonKeys.FILTER_BY_CACHE_DISPOSITION_MISS,
                                    JsonKeys.FILTER_BY_CACHE_MISS,
                                    JsonKeys.FILTER_BY_CACHE_REVALIDATED_MISS,
                                    JsonKeys.FILTER_BY_CACHE_BACKGROUND_FILL,
                                    JsonKeys.FILTER_BY_CACHE_BAD_REQUEST,
                                    JsonKeys.FILTER_BY_CACHE_SERV_FAIL,
                                    JsonKeys.FILTER_BY_CACHE_UNKNOWN]

    def test_non_numeric_split_limit(self):
        self.check_invalid_params(
            {
                'splitLimit': '365f'
            },
            4003, # Value error
            "'365f' is not of type 'integer'",
            {u'property': [u'splitLimit']}
        )

    def test_negative_split_limit(self):
        self.check_invalid_params(
            {
                'splitLimit': -365
            },
            4004, # Value error
            # The following message is brought to you by json-schema
            "-365.0 is less than the minimum of 0",
            {u'property': [u'splitLimit']}
        )

    def test_invalid_network_tier_split(self):
        """Only Network split available is Delivery Appliance split"""
        self.check_invalid_params(
            {'splitBy': {
                JsonKeys.SPLIT_BY_NETWORK: JsonKeys.SPLIT_BY_NETWORK_TIER}},
            4004, # Value error
            "The split level must be one of",
            {u'property': [u'splitBy', u'network']}
        )

    def test_valid_network_delivery_appliance_split(self):
        """Show that the network delivery appliance split works"""
        self.check_valid_params({
            'splitBy': { JsonKeys.SPLIT_BY_NETWORK : JsonKeys.SPLIT_BY_NETWORK_DELIVERY_APPLIANCE}
        })

    def test_invalid_network_response_split(self):
        """No Network Response splits are available"""
        self.check_invalid_params(
            {
                'splitBy': {
                    JsonKeys.SPLIT_BY_NETWORK_RESPONSE:
                    JsonKeys.SPLIT_BY_NETWORK_RESPONSE_SUCCESS_FAILURE
                }
            },
            4002,  # Unrecognised property
            "The split dimension must be one of 'cacheStatus', 'content', "
            "'httpStatus', 'network', 'org'.",
            {u'property': [u'splitBy']}
        )

    def test_valid_metric_bytes(self):
        self.check_valid_params({
            'metric': 'bytesDelivered'
        })

    def test_valid_metric_count(self):
        self.check_valid_params({
            'metric': 'requestsDelivered'
        })

    def test_valid_metric_peak_throughput(self):
        self.check_valid_params({
            'metric': 'peakBytesDelivered'
        })

    def test_valid_metric_peak_request_rate(self):
        self.check_valid_params({
            'metric': 'peakRequestRate'
        })

    def test_valid_metric_unique_ip_address(self):
        self.check_valid_params({
            'metric': 'uniqueIpAddress'
        })

    def test_invalid_metric(self):
        self.check_invalid_params(
            {
                'metric': 'cheese'
            },
            4004, # Value error
            "'cheese' is not one of",
            {u'property': [u'metric']}
        )

    def test_invalid_filter(self):
        """Check the JSON schema is working correctly for filters."""
        self.check_invalid_params(
            {
                'filterBy': "invalid"
            },
            4003, # Type error
            "'invalid' is not of type 'object'",
            {'property': ['filterBy']}
        )

    def test_unknown_filter(self):
        """Check the JSON schema is working correctly for filters."""
        self.check_invalid_params(
            {
                'filterBy': { 'unknownFilter' : [] }
            },
            4002, # Unrecognised property
            "Additional properties are not allowed ('unknownFilter' was unexpected)",
            {'property': ['filterBy', 'unknownFilter']}
        )

    def test_valid_http_filter(self):
        """Check the http filter accepts the correct type."""
        self.check_valid_params({
            'filterBy': { 'httpStatus' : [] }
        })

    def test_invalid_http_filter_type(self):
        """Check the http filter returns an error if given an incorrect value."""
        self.check_invalid_params(
            {
                'filterBy': { 'httpStatus' : 'unknown' }
            },
            4003, # Type error
            "'unknown' is not of type 'array'",
            {'property': ['filterBy', 'httpStatus']}
        )

    def test_invalid_http_filter_enum(self):
        """Check the http filter returns an error if given an incorrect value."""
        self.check_invalid_params(
            {
                'filterBy': { 'httpStatus' : ['unknown'] }
            },
            4004, # Value error
            "'unknown' is not valid under any of the given schemas",
            {'property': ['filterBy', 'httpStatus', 0]}
        )

    def test_invalid_http_filter_pattern(self):
        """Check the http filter returns an error if given an incorrect value."""
        self.check_invalid_params(
            {
                'filterBy': { 'httpStatus' : ['1xx', '6xx'] }
            },
            4004, # Value error
            "'6xx' is not valid under any of the given schemas",
            {'property':['filterBy', 'httpStatus', 1]}
        )

    def test_invalid_http_filter_range(self):
        """Check the http filter returns an error if given an incorrect value."""
        self.check_invalid_params(
            {
                'filterBy': { 'httpStatus' : [99] }
            },
            4004, # Value error
            "99 is not valid under any of the given schemas",
            {'property': ['filterBy', 'httpStatus', 0]}
        )


    def test_valid_cache_disposition_filter(self):
        """Check the Cache Disposition filter accepts the correct type."""
        self.check_valid_params({
            'filterBy': { JsonKeys.FILTER_BY_CACHE_DISPOSITION : [] }
        })

    def test_invalid_cache_disposition_filter_type(self):
        """Check the Cache Disposition filter returns an error if given an incorrect value."""
        self.check_invalid_params(
            {
                JsonKeys.FILTER_BY: { JsonKeys.FILTER_BY_CACHE_DISPOSITION : 'unknown' }
            },
            4003, # Type error
            "'unknown' is not of type 'array'",
            {'property': [JsonKeys.FILTER_BY, JsonKeys.FILTER_BY_CACHE_DISPOSITION]}
        )

    def test_invalid_cache_disposition_filter_enum(self):
        """Check the Cache Disposition filter returns an error if given an incorrect value."""
        self.check_invalid_params(
            {
                JsonKeys.FILTER_BY: { JsonKeys.FILTER_BY_CACHE_DISPOSITION : ['unknown'] }
            },
            4004, # Value error
            "'unknown' is not one of {0}".format(self.FILTER_BY_CACHE_DISPOSITIONS),
            {'property': [JsonKeys.FILTER_BY, JsonKeys.FILTER_BY_CACHE_DISPOSITION, 0]}
        )


class TestCDNInsightsReportParameterValidation(
        ReportParameterValidationMixin, SetupUnapiFlaskAppForReporting):
    """Test that Report Parameters are validated at the API level.

    These tests are specific to the CDN Insights pack reports.

    """
    report_path = '/reporting/reports/tier-analysis/jobs'

    def test_invalid_metric_peak_throughput(self):
        """No metrics are supported, so this standard metric isn't either"""
        self.check_invalid_params(
            {'metric': 'peakBytesDelivered'}, 4004,  # Value error
            '"Peak throughput (bps)" metric is not supported for this report',
            {u'property': [u'metric']}
        )

    def test_invalid_cache_disposition_filter(self):
        """Only Organisation filter is supported, so this standard one isn't"""
        self.check_invalid_params(
            {'filterBy': {
                JsonKeys.FILTER_BY_CACHE_DISPOSITION: ['cacheHit']}},
            4002,  # Property error
            "The filter must be one of",
            {u'property': [u'filterBy']}
        )

    def test_invalid_network_response_filter(self):
        """Network Response filter is supported, but 'unknown' is not valid item"""
        self.check_invalid_params(
            {'filterBy': {
                JsonKeys.FILTER_BY_NETWORK_RESPONSE: ['unknown']}},
            4004,  # Value error
            "'unknown' is not valid under any of the given schemas",
            {u'property': [u'filterBy', u'networkResponse', 0]}
        )

    def test_valid_network_response_filter(self):
        """Show that the Network Response filter works"""
        json_props = self.check_valid_params({
            'filterBy': {
                JsonKeys.FILTER_BY_NETWORK_RESPONSE: [
                    '2xx', JsonKeys.FILTER_BY_NETWORK_RESPONSE_FAILURE]}
        })
        # Check the report job contains the correct parameters
        self.assertEqual(json_props[JsonKeys.FILTER_BY],
                         {u'networkResponse': [u'2xx', u'failure']})

    def test_valid_empty_network_response_filter(self):
        """Check that we can run a report with an empty Network Response filter."""
        json_props = self.check_valid_params({
            'filterBy': { JsonKeys.FILTER_BY_NETWORK_RESPONSE: []}
        })
        # Check the report job contains the correct parameters
        self.assertEqual(json_props[JsonKeys.FILTER_BY], {})

    def test_invalid_org_service_split(self):
        """Only Network Tier split is supported, so this Organization split isn't"""
        self.check_invalid_params(
            {'splitBy': {
                JsonKeys.SPLIT_BY_ORG: JsonKeys.SPLIT_BY_ORG_SERVICE}},
            4002, # Property error
            "The split dimension must be one of",
            {u'property': [u'splitBy']}
        )

    def test_invalid_network_delivery_appliance_split(self):
        """Only Network split available is Tier split"""
        self.check_invalid_params(
            {'splitBy': {
                JsonKeys.SPLIT_BY_NETWORK: JsonKeys.SPLIT_BY_NETWORK_DELIVERY_APPLIANCE}},
            4004, # Value error
            "The split level must be one of",
            {u'property': [u'splitBy', u'network']}
        )

    def test_valid_network_tier_split(self):
        """Show that the network tier split works"""
        self.check_valid_params({
            'splitBy': { JsonKeys.SPLIT_BY_NETWORK : JsonKeys.SPLIT_BY_NETWORK_TIER}
        })


    def test_tier_analysis_default_metric(self):
        """Show that the default metric works"""
        self.check_valid_params({})

    def test_tier_analysis_bytes_transferred(self):
        """Show that the bytes transferred metric works"""
        self.check_valid_params({'metric': JsonKeys.METRIC_BYTES_TRANSFERRED})


class TestGenerateSplitLimit(SetupUnapiFlaskAppForReporting):
    """Test the method ReportJobCollection._generate_split_limit"""

    def check_working_split_limit(self, report_id,
                                  expected_split_limit,
                                  given_split_limit=None):
        """Helper method to test split limit generation.
        @param report_id - the report to test for
        @param expected_split_limit - expected return value.
        @param given_split_limit - parameter for _generate_metric method,
                                   if None no split_limit parameter is given.
        """

        # Call code under test
        obj = report_job_collection.ReportJobCollection(report_id=report_id)
        if given_split_limit is None:
            query_parts = obj._generate_split_limit({})
        else:
            query_parts = obj._generate_split_limit({JsonKeys.SPLIT_LIMIT: given_split_limit})

        # Check the generated query parameter
        self.assertEqual(query_parts['split_limit'], expected_split_limit, 'The returned split_limit is incorrect')

    def test_traffic_report_no_split_limit_given(self):
        """Normal case where no split limit is given as parameter, the default is returned then"""
        self.check_working_split_limit(ReportIds.TRAFFIC, DEFAULT_SPLIT_LIMIT)

    def test_traffic_report_split_limit_given(self):
        """Normal case where a split limit is given as parameter, this given value is returned then"""
        self.check_working_split_limit(ReportIds.TRAFFIC, 18, 18)

    def test_request_report_no_split_limit_given(self):
        """Normal case where no split limit is given as parameter, the default is returned then"""
        self.check_working_split_limit(ReportIds.REQUEST, DEFAULT_SPLIT_LIMIT)

    def test_cache_hit_report_no_split_limit_given(self):
        """Normal case where no split limit is given as parameter, the default is returned then"""
        self.check_working_split_limit(ReportIds.CACHE_HIT, 0)

    def test_http_status_code_report_no_split_limit_given(self):
        """Normal case where no split limit is given as parameter, the default is returned then"""
        self.check_working_split_limit(ReportIds.HTTP_STATUS_CODE, DEFAULT_SPLIT_LIMIT)

    def test_content_popularity_report_no_split_limit_given(self):
        """Normal case where no split limit is given as parameter, the default is returned then"""
        self.check_working_split_limit(ReportIds.CONTENT_POPULARITY, DEFAULT_SPLIT_LIMIT)



class TestGenerateMetric(SetupFlaskContext):
    """Test the method ReportJobCollection._generate_metric"""

    def check_working_metric(self, metric,
                             metric_label,
                             stat_func,
                             tstat_access_metric,
                             tstat_acquiry_metric,
                             summary_metric,
                             unit_scale,
                             peak_query,
                             metric_in_saved_search=True,
                             metric_in_summary_index=True,
                             use_access_source_type = True,
                             use_acquisition_source_type = False,
                             sample_metric='',
                             sample_id='',
                             report_id='traffic'):
        """Helper method to test working metrics.
        @param metric - parameter to use with the _generate_metric method.
        @param metric_label - metric / axis label
        @param stat_func - expected query part
        @param tstat_access_metric - expected query part
        @param tstat_acquiry_metric - expected query part
        @param summary_metric - expected query part
        @param unit_scale - expected query part
        @param peak_query - expected value of member variable
        @param metric_in_saved_search - expected value of member variable
        @param metric_in_summary_index - expected value of member variable
        @param use_access_source_type - whether or not the sourcetype should be used.
        @param use_acquisition_source_type - whether or not the sourcetype should be used.
        @param sample_metric - expected query part
        @param sample_id - expected query part
        @param report_id - report to test, defaults to the @c traffic report.
        """

        # Call code under test
        obj = report_job_collection.ReportJobCollection(report_id=report_id)
        query_parts = obj._generate_metric({JsonKeys.METRIC: metric})

        query_per_sourcetype = {}
        for query in query_parts['tstats_queries']:
            query_per_sourcetype.setdefault(query['mi_sourcetype'], []).append(
                query)

        # Check the generated search query and member variables
        self.assertEqual(query_parts['metric_label'], metric_label, 'The returned metric_label is incorrect')
        self.assertEqual(query_parts['stat_func'], stat_func, 'The returned stat_func is incorrect')
        self.assertEqual(query_parts['summary_metric'], summary_metric, 'The returned summary_metric is incorrect')
        self.assertEqual(query_parts['unit_scale'], unit_scale, 'The returned unit_scale is incorrect')
        self.assertEqual(query_parts['sample_id'], sample_id, 'The returned sample_id is incorrect')
        self.assertEqual(obj._peak_query, peak_query, '_peak_query member variable is incorrect')
        self.assertEqual(
            obj._metric_in_saved_search, metric_in_saved_search,
            '_metric_in_saved_search member variable is incorrect')
        self.assertEqual(
            obj._metric_in_summary_index, metric_in_summary_index,
            '_metric_in_summary_index member variable is incorrect')

        if 'cdn_del_access' in query_per_sourcetype:
            first_query = query_per_sourcetype['cdn_del_access'][0]
            self.assertEqual(
                first_query['mi_metric'], tstat_access_metric,
                'The returned tstats metric is incorrect')
            self.assertEqual(
                first_query['mi_zones'], ('external',),
                'Zone usage is inappropriate for source type')
            self.assertEqual(
                first_query['mi_sample_metric'], sample_metric,
                'The returned sample_metric is incorrect')
        else:
            self.assertFalse(
                tstat_access_metric, 'No access tstats query returned')
            self.assertFalse(
                use_access_source_type,
                'Zone usage is inappropriate for source type')

        if 'cdn_del_acquisition' in query_per_sourcetype:
            first_query = query_per_sourcetype['cdn_del_acquisition'][0]
            self.assertEqual(
                first_query['mi_metric'], tstat_acquiry_metric,
                'The returned tstats metric is incorrect')
            self.assertEqual(
                first_query['mi_zones'], ('cdn-origin', 'external', 'cdn'),
                'Zone usage is inappropriate for source type')
            self.assertEqual(
                first_query['mi_sample_metric'], sample_metric,
                'The returned sample_metric is incorrect')
        else:
            self.assertFalse(
                tstat_acquiry_metric, 'No access tstats query returned')
            self.assertFalse(
                use_acquisition_source_type,
                'Zone usage is inappropriate for source type')

    def test_bytes_metric(self):
        """Normal case where a bytes metric is given"""
        self.check_working_metric(JsonKeys.METRIC_BYTES_DELIVERED,
                                  'Bytes delivered',
                                  'sum',
                                  'sum(sc_bytes)',
                                  '',
                                  'sum(sc_bytes)',
                                  '',
                                  False)

    def test_count_metric(self):
        """Normal case where a count metric is given"""
        self.check_working_metric(JsonKeys.METRIC_REQUESTS_DELIVERED,
                                  'Request count',
                                  'sum',
                                  'count',
                                  '',
                                  'sum(requests)',
                                  '',
                                  False)

    def test_peak_throughput_metric(self):
        """Normal case where a peak throughput metric is given"""
        self.check_working_metric(JsonKeys.METRIC_PEAK_DELIVERED,
                                  'Peak throughput (bps)',
                                  'max',
                                  'sum(sc_bytes)',
                                  '',
                                  '',
                                  '*8/300',
                                  True,
                                  sample_metric='count as s',
                                  sample_id='s')

    def test_peak_throughput_95th_percentile_metric(self):
        """Normal case where a Peak throughput 95th percentile metric is given"""
        self.check_working_metric(JsonKeys.METRIC_95TH_PERCENTILE_DELIVERED,
                                  '95th percentile throughput (bps)',
                                  'perc95',
                                  'sum(sc_bytes)',
                                  '',
                                  '',
                                  '*8/300',
                                  True,
                                  sample_metric='count as s',
                                  sample_id='s')

    def test_peak_request_rate_metric(self):
        """Normal case where a peak request rate metric is given"""
        self.check_working_metric(JsonKeys.METRIC_PEAK_REQUEST_RATE,
                                  'Peak request rate (rps)',
                                  'max',
                                  'count',
                                  '',
                                  '',
                                  '/300',
                                  True,
                                  sample_id='x')

    def test_unique_ip_address_metric(self):
        """Normal case where an unique ip address metric is given"""
        self.check_working_metric(JsonKeys.METRIC_UNIQUE_IP_ADDRESS,
                                  'Unique IP address',
                                  'dc',
                                  'values(c_ip)',
                                  '',
                                  '',
                                  '',
                                  False,
                                  metric_in_saved_search=False,
                                  metric_in_summary_index=False)

    def test_bytes_transferred_metric(self):
        """Normal case where a bytes transferred metric is given"""
        self.check_working_metric(JsonKeys.METRIC_BYTES_TRANSFERRED,
                                  'Bytes transferred',
                                  'sum',
                                  'sum(sc_bytes)',
                                  'sum(rs_bytes)',
                                  '',
                                  '',
                                  False,
                                  metric_in_saved_search=False,
                                  metric_in_summary_index = False,
                                  use_acquisition_source_type=True,
                                  report_id=ReportIds.TIER_ANALYSIS)

    def test_peak_bytes_transferred_metric(self):
        """Normal case where a peak bytes transferred metric is given"""
        self.check_working_metric(JsonKeys.METRIC_PEAK_TRANSFERRED,
                                  'Peak throughput (bps)',
                                  'max',
                                  'sum(sc_bytes)',
                                  'sum(rs_bytes)',
                                  '',
                                  '*8/300',
                                  True,
                                  sample_metric='count as s',
                                  sample_id='s',
                                  use_acquisition_source_type=True,
                                  report_id=ReportIds.TIER_ANALYSIS)

    def test_95th_percentile_bytes_transferred_metric(self):
        """Normal case where a 95th percentile bytes transferred metric is given"""
        self.check_working_metric(JsonKeys.METRIC_95TH_PERCENTILE_TRANSFERRED,
                                  '95th percentile throughput (bps)',
                                  'perc95',
                                  'sum(sc_bytes)',
                                  'sum(rs_bytes)',
                                  '',
                                  '*8/300',
                                  True,
                                  sample_metric='count as s',
                                  sample_id='s',
                                  use_acquisition_source_type=True,
                                  report_id=ReportIds.TIER_ANALYSIS)

    def test_requests_transferred_metric(self):
        """CDN Insights pack: requests transferred (request count) metric"""
        self.check_working_metric(JsonKeys.METRIC_REQUESTS_TRANSFERRED,
                                  'Request count',
                                  'sum',
                                  'count',
                                  'count',
                                  '',
                                  '',
                                  False,
                                  metric_in_saved_search=False,
                                  metric_in_summary_index=False,
                                  sample_metric='',
                                  sample_id='',
                                  use_acquisition_source_type=True,
                                  report_id=ReportIds.TIER_ANALYSIS)

    def check_default_metric(self, report_id, metric_label):
        """Helper method to test default metric for a report.
        @param report_id - report ID for which to check the default.
        @param metric_label - expected metric / axis label
        """

        # Call code under test
        obj = report_job_collection.ReportJobCollection(report_id=report_id)
        query_parts = obj._generate_metric({})

        # Check the label which will depend of the metric chosen for the report
        self.assertEqual(query_parts['metric_label'], metric_label, 'The returned metric_label is incorrect')

    def test_traffic_report_default(self):
        """Check the Traffic report has the correct default metric."""
        self.check_default_metric(ReportIds.TRAFFIC, 'Bytes delivered')

    def test_request_report_default(self):
        """Check the Request report has the correct default metric."""
        self.check_default_metric(ReportIds.REQUEST, 'Request count')

    def test_cache_hit_report_default(self):
        """Check the Cache hit report has the correct default metric."""
        self.check_default_metric(ReportIds.CACHE_HIT, 'Bytes delivered')

    def test_http_status_report_default(self):
        """Check the HTTP status code report has the correct default metric."""
        self.check_default_metric(ReportIds.HTTP_STATUS_CODE, 'Request count')

    def test_content_popularity_report_default(self):
        """Check the Content popularity analysis report has the correct default metric."""
        self.check_default_metric(ReportIds.CONTENT_POPULARITY, 'Bytes delivered')

    def check_invalid_parameters(self, parameters):
        """Helper method to test invalid metric parameters"""
        # Call code under test
        with self.assertRaises(errors.PropertyValueError) as cm:
            obj = report_job_collection.ReportJobCollection(report_id='traffic')
            obj._generate_metric(parameters)
        err = cm.exception
        self.assertEquals(err.code, 400)
        self.assertEquals(err.error_code, 4004)

    def test_invalid_metric(self):
        """Error case where an invalid metric is given"""
        self.check_invalid_parameters({JsonKeys.METRIC: "cheese"})


class TestGenerateSplit(SetupFlaskContext):
    """Test the method ReportJobCollection._generate_split"""
    def setUp(self):  # noqa
        # Patch external systems
        super(TestGenerateSplit, self).setUp()

        patch = mock.patch.object(report_job_collection.ReportJobCollection, 'get_cdn_db',
                                  new=FakeCdnDbForReporting)
        patch.start()
        self.addCleanup(patch.stop)

    def check_no_split(self, report_id):
        """Check the query parameters when no split is defined.
        @param report_id - report ID for which to check the query parts.
        """
        # Call code under test
        obj = report_job_collection.ReportJobCollection(report_id=report_id)
        split_config, split_parts = obj._generate_split({JsonKeys.SPLIT_BY: {}})

        self.assertFalse(
            split_config,
            'The returned split contains tstat queries')
        self.assertEqual(
            '', split_parts['chart_split'],
            'The returned chart split is not an empty string')
        self.assertEqual(
            '', split_parts['summary_chart_split'],
            'The returned summary chart split is not an empty string')
        self.assertEqual(
            '', split_parts['split_lookup'],
            'The returned split_lookup entry is not an empty string')
        self.assertEqual(
            '', split_parts['split_label'],
            'The returned split_label entry is not an empty string')
        self.assertEqual(
            4, len(split_parts),
            'Unexpected number of items returned')

        self.assertTrue(
            obj._split_in_saved_search,
            "With no split selected the saved search should be available")

    def test_traffic_report_no_split(self):
        """Test when splits are explicity disabled."""
        self.check_no_split(ReportIds.TRAFFIC)

    def test_request_report_no_split(self):
        """Test when splits are explicity disabled."""
        self.check_no_split(ReportIds.REQUEST)

    def test_cache_hit_report_no_split(self):
        """Test when splits are explicity disabled."""
        self.check_no_split(ReportIds.CACHE_HIT)

    def test_http_status_report_no_split(self):
        """Test when splits are explicity disabled."""
        self.check_no_split(ReportIds.HTTP_STATUS_CODE)

    def test_content_popularity_report_no_split(self):
        """Test when splits are explicity disabled."""
        self.check_no_split(ReportIds.CONTENT_POPULARITY)

    def test_tier_analysis_report_no_split(self):
        """Test when splits are explicity disabled."""
        self.check_no_split(ReportIds.TIER_ANALYSIS)

    def check_working_split(self, report_id, split, subsplit,
                            tstats_access_splits, tstats_acquiry_splits,
                            chart_split, summary_chart_split,
                            split_lookup, split_label,
                            access_field_grouping='',
                            acquiry_field_grouping='',
                            split_in_saved_search=True):
        """Helper method to test working splits
        @param report_id - report ID for which to check the query parts.
        @param split - split group report parameter (may be None)
        @param subsplit - split dimension report parameter (may be None)
        @param tstats_access_split - expected tstats split value for access logs
        @param tstats_acquiry_split - expected tstats split value for acquiry logs
        @param chart_split - expected chart split value
        @param summary_chart_split - expected chart split for the summary index value
        @param split_lookup - expected split lookup value
        @param split_label - expected split label value
        @param access_field_grouping - expected field grouping for access logs
        @param acquiry_field_grouping - expected field grouping for acquiry logs
        @param split_in_saved_search - expected value for _split_in_saved_search
        """

        # Call code under test
        obj = report_job_collection.ReportJobCollection(report_id=report_id)

        # If a split it not provided then call method under test with an empty
        # dictionary.
        if split and subsplit:
            parameters = {JsonKeys.SPLIT_BY: {split: subsplit}}
        else:
            parameters = {}

        split_config, split_parts = obj._generate_split(parameters)

        # Check result
        if not split_config:
            for entry in (tstats_access_splits, access_field_grouping,
                          tstats_acquiry_splits, acquiry_field_grouping):
                self.assertFalse(entry, 'no split set')

        tstats_access_splits = iter(tstats_access_splits)
        tstats_acquiry_splits = iter(tstats_acquiry_splits)
        for query in split_config:
            if query['mi_sourcetype'] == 'cdn_del_access':
                expected_split = next(tstats_access_splits, [])
                self.assertEqual(
                    expected_split, query['mi_split'],
                    'The returned tstats_access_split is incorrect')
                self.assertEqual(
                    access_field_grouping, query.get('mi_field_grouping', ''),
                    'The returned access_field_grouping is incorrect')

            elif query['mi_sourcetype'] == 'cdn_del_acquisition':
                expected_split = next(tstats_acquiry_splits, [])
                self.assertEqual(
                    expected_split, query['mi_split'],
                    'The returned tstats_acquiry_split is incorrect')
                self.assertEqual(
                    acquiry_field_grouping, query.get('mi_field_grouping', ''),
                    'The returned acquiry_field_grouping is incorrect')

            else:
                self.fail('Unsupported or missing sourcetype in split')

        self.assertEqual(
            chart_split, split_parts['chart_split'],
            'The returned chart_split is incorrect')
        self.assertEqual(
            summary_chart_split, split_parts['summary_chart_split'],
            'The returned summary_chart_split item is incorrect')
        self.assertEqual(
            split_lookup, split_parts['split_lookup'],
            'The returned split_lookup item is incorrect')
        self.assertEqual(
            split_label, split_parts['split_label'],
            'The returned split_label item is incorrect')

        self.assertEqual(
            4, len(split_parts), 'Unexpected number of items returned')
        self.assertEqual(
            obj._split_in_saved_search, split_in_saved_search,
            "Unexpected value for whether or not this split should use the saved search")

    def test_org_service_split(self):
        """Normal case where an orgService split is given under org"""
        self.check_working_split(ReportIds.TRAFFIC,
                                 JsonKeys.SPLIT_BY_ORG,
                                 JsonKeys.SPLIT_BY_ORG_SERVICE,
                                 [['host']], [], 'host', 'orig_host', '',
                                 'Service')

    def test_http_status_code_success_split(self):
        """Normal case where a split is done for http status code success"""
        self.check_working_split(ReportIds.TRAFFIC,
                                 JsonKeys.SPLIT_BY_HTTP_STATUS,
                                 JsonKeys.SPLIT_BY_HTTP_STATUS_SUCCESS_FAILURE,
                                 [['sc_status']], [], 'status_ok', 'status_ok',
                                 report_job_collection.HTTP_STATUS_LOOKUP,
                                 'HTTP Success or Failure')

    def test_http_status_code_class_split(self):
        """Normal case where a split is done for http status code class"""
        self.check_working_split(ReportIds.TRAFFIC,
                                 JsonKeys.SPLIT_BY_HTTP_STATUS,
                                 JsonKeys.SPLIT_BY_HTTP_STATUS_CODE_CLASS,
                                 [['sc_status']], [], 'status_type',
                                 'status_type',
                                 report_job_collection.HTTP_STATUS_LOOKUP,
                                 'HTTP Status Class')

    def test_http_status_code_split(self):
        """Normal case where a split is done for http status code"""
        self.check_working_split(ReportIds.TRAFFIC,
                                 JsonKeys.SPLIT_BY_HTTP_STATUS,
                                 JsonKeys.SPLIT_BY_HTTP_STATUS_CODE,
                                 [['sc_status']], [], 'sc_status', 'sc_status',
                                 '', 'HTTP Status Code')

    def test_cache_status_hit_split(self):
        """Normal case where a split is done for cache status success"""
        self.check_working_split(ReportIds.TRAFFIC,
                                 JsonKeys.SPLIT_BY_CACHE_STATUS,
                                 JsonKeys.SPLIT_BY_CACHE_STATUS_HIT,
                                 [['s_cachestatus']], [], 'cache_hit',
                                 'cache_hit',
                                 report_job_collection.CACHE_STATUS_LOOKUP,
                                 'Cache Hit or Miss')

    def test_cache_status_disposition_split(self):
        """Normal case where a split is done for cache status disposition"""
        self.check_working_split(ReportIds.TRAFFIC,
                                 JsonKeys.SPLIT_BY_CACHE_STATUS,
                                 JsonKeys.SPLIT_BY_CACHE_STATUS_DISPOSITION,
                                 [['s_cachestatus']], [], 's_cachestatus',
                                 's_cachestatus', '', 'Cache Status')

    def test_content_object_split(self):
        """Normal case where a split is done for content object"""
        self.check_working_split(ReportIds.TRAFFIC,
                                 JsonKeys.SPLIT_BY_CONTENT,
                                 JsonKeys.SPLIT_BY_CONTENT_OBJECT,
                                 [['x_vx_serial']], [], 'x_vx_serial',
                                 'x_vx_serial', '', 'Object')

    def test_content_asset_split(self):
        """Normal case where a split is done for content asset"""
        # This split dimension is not supported by the saved search due to the
        # number of unique values for cs_uri which is used to calculate the
        # Asset.
        self.check_working_split(ReportIds.TRAFFIC,
                                 JsonKeys.SPLIT_BY_CONTENT,
                                 JsonKeys.SPLIT_BY_CONTENT_ASSET,
                                 [['cs_uri', 'x_vx_serial']], [], 'asset',
                                 'asset',
                                 report_job_collection.CONTENT_ASSET_LOOKUP,
                                 'Asset',
                                 split_in_saved_search=False)

    def test_network_delivery_appliance_split(self):
        """Normal case where a split is done for network delivery appliance"""
        self.check_working_split(ReportIds.TRAFFIC,
                                 JsonKeys.SPLIT_BY_NETWORK,
                                 JsonKeys.SPLIT_BY_NETWORK_DELIVERY_APPLIANCE,
                                 [['s_dns']], [['s_dns']], 's_dns', 's_dns', '',
                                 'Delivery Appliance')

    def test_network_tier_split(self):
        """Normal case where a split is done for network tier"""
        self.check_working_split(ReportIds.TIER_ANALYSIS,
                                 JsonKeys.SPLIT_BY_NETWORK,
                                 JsonKeys.SPLIT_BY_NETWORK_TIER,
                                 [['c_vx_zone']], [['r_vx_zone']], 'c_vx_zone',
                                 'c_vx_zone', '', 'Tier',
                                 split_in_saved_search=False,
                                 acquiry_field_grouping='| eval c_vx_zone = if(r_vx_zone == "cdn", "cdn", "acquiry")')

    def test_network_sending_appliance_hostname(self):
        """Normal case of a split by sending appliance hostname."""
        self.check_working_split(ReportIds.TIER_ANALYSIS,
                                 JsonKeys.SPLIT_BY_NETWORK,
                                 JsonKeys.SPLIT_BY_NETWORK_SENDING_APPLIANCE_HOSTNAME,
                                 [['s_dns', 's_ip']], [['r_dns', 'r_ip']],
                                 'hostname', 'hostname', '',
                                 'Sending Appliance Hostname',
                                 split_in_saved_search=False,
                                 access_field_grouping='| eval hostname=if(s_dns=="None", s_ip, s_dns)',
                                 acquiry_field_grouping='| eval hostname=if(r_dns=="None", r_ip, r_dns)')

    def test_network_sending_appliance_ip(self):
        """Normal case of a split by sending appliance IP address."""
        self.check_working_split(ReportIds.TIER_ANALYSIS,
                                 JsonKeys.SPLIT_BY_NETWORK,
                                 JsonKeys.SPLIT_BY_NETWORK_SENDING_APPLIANCE_IP_ADDRESS,
                                 [['s_ip']], [['r_ip']], 's_ip', 's_ip', '',
                                 'Sending Appliance IP Address',
                                 split_in_saved_search=False,
                                 acquiry_field_grouping='| eval s_ip=r_ip')

    def test_network_receiving_appliance_hostname(self):
        """Normal case of a split by receiving appliance hostname."""
        self.check_working_split(ReportIds.TIER_ANALYSIS,
                                 JsonKeys.SPLIT_BY_NETWORK,
                                 JsonKeys.SPLIT_BY_NETWORK_RECEIVING_APPLIANCE_HOSTNAME,
                                 [['c_ip']], [['s_dns', 's_ip']], 'hostname',
                                 'hostname', '', 'Receiving Appliance Hostname',
                                 split_in_saved_search=False,
                                 access_field_grouping='| eval hostname=c_ip',
                                 acquiry_field_grouping='| eval hostname=if(s_dns=="None", s_ip, s_dns)')

    def test_network_receiving_appliance_ip(self):
        """Normal case of a split by receiving appliance IP address."""
        self.check_working_split(ReportIds.TIER_ANALYSIS,
                                 JsonKeys.SPLIT_BY_NETWORK,
                                 JsonKeys.SPLIT_BY_NETWORK_RECEIVING_APPLIANCE_IP_ADDRESS,
                                 [['c_ip']], [['s_ip']], 's_ip', 's_ip', '',
                                 'Receiving Appliance IP Address',
                                 split_in_saved_search=False,
                                 access_field_grouping='| eval s_ip=c_ip')

    def test_network_response_success_failure(self):
        """Split by network response codes - success or failure."""
        self.check_working_split(
            ReportIds.TIER_ANALYSIS,
            JsonKeys.SPLIT_BY_NETWORK_RESPONSE,
            JsonKeys.SPLIT_BY_NETWORK_RESPONSE_SUCCESS_FAILURE,
            [['sc_status']], [['s_flags', 'rs_status'], ['rs_status']],
            'status_ok', 'status_ok',
            report_job_collection.NETWORK_STATUS_LOOKUP,
            'Network Response Success or Failure',
            split_in_saved_search=False)

    def test_network_response_code_class(self):
        """Split by network response codes - code class."""
        self.check_working_split(
            ReportIds.TIER_ANALYSIS,
            JsonKeys.SPLIT_BY_NETWORK_RESPONSE,
            JsonKeys.SPLIT_BY_NETWORK_RESPONSE_CODE_CLASS,
            [['sc_status']], [['s_flags', 'rs_status'], ['rs_status']],
            'status_type', 'status_type',
            report_job_collection.NETWORK_STATUS_LOOKUP,
            'Network Response Class',
            split_in_saved_search=False)

    def test_traffic_report_default(self):
        """Check the Traffic report has the correct default split."""
        self.check_working_split(
            ReportIds.TRAFFIC, None, None,
            [], [], '', '', '', '')

    def test_request_report_default(self):
        """Check the request report has the correct default split."""
        self.check_working_split(
            ReportIds.REQUEST, None, None,
            [], [], '', '', '', '')

    def test_cache_hit_report_default(self):
        """Check the cache hit report has the correct default split."""
        self.check_working_split(
            ReportIds.CACHE_HIT, None, None,
            [['s_cachestatus']], [], 'cache_hit', 'cache_hit',
            report_job_collection.CACHE_STATUS_LOOKUP,
            'Cache Hit or Miss')

    def test_http_status_report_default(self):
        """Check the http status code report has the correct default split."""
        self.check_working_split(
            ReportIds.HTTP_STATUS_CODE, None, None,
            [['sc_status']], [], 'sc_status', 'sc_status', '',
            'HTTP Status Code')

    def test_content_popularity_report_default(self):
        """Check the content popularity report has the correct default split."""
        self.check_working_split(
            ReportIds.CONTENT_POPULARITY, None, None,
            [['x_vx_serial']], [], 'x_vx_serial', 'x_vx_serial', '',
            'Object')

    def test_tier_analysis_report_default(self):
        """Check the content popularity report has the correct default split."""
        self.check_working_split(
            ReportIds.TIER_ANALYSIS, None, None,
            [['c_vx_zone']], [['r_vx_zone']], 'c_vx_zone', 'c_vx_zone', '',
            'Tier',
            split_in_saved_search=False,
            acquiry_field_grouping='| eval c_vx_zone = if(r_vx_zone == "cdn", "cdn", "acquiry")')

    def check_failing_split(self, split, subsplit,
                            error=errors.PropertyValueError,
                            error_code=4004,
                            error_description=None,
                            error_details=None,
                            report_id='traffic'):
        """Error case where a wrong split is given"""

        # Call code under test
        with self.assertRaises(error) as cm:
            obj = report_job_collection.ReportJobCollection(report_id=report_id)
            obj._time_split = True
            obj._generate_split({JsonKeys.SPLIT_BY: {split: subsplit}})
        err = cm.exception
        self.assertEquals(err.code, 400)
        self.assertEquals(err.error_code, error_code)

        # If provided check the error desciption and error details, this check
        # is useful for errors raised within the method under test.
        if error_description:
            self.assertEquals(err.error_description, error_description)
        if error_details:
            self.assertEquals(err.error_details, error_details)

    def test_org_wrong_service_split(self):
        """Error case where an wrong service split is given under org"""
        self.check_failing_split(JsonKeys.SPLIT_BY_ORG, 'wrongService')

    def test_http_status_wrong_status_split(self):
        """Error case where an wrong HTTP Status split is given under httpStatus"""
        self.check_failing_split(JsonKeys.SPLIT_BY_HTTP_STATUS, 'wrongHttpStatus')

    def test_cache_status_wrong_status_split(self):
        """Error case where an wrong Cache Status split is given under cacheStatus"""
        self.check_failing_split(JsonKeys.SPLIT_BY_CACHE_STATUS, 'wrongCacheStatus')

    def test_content_wrong_object_split(self):
        """Error case where an wrong object split is given under content"""
        self.check_failing_split(JsonKeys.SPLIT_BY_CONTENT, 'wrongObject')

    def test_network_wrong_appliance_split(self):
        """Error case where an wrong network split is given by Velocix Admin"""
        self.check_failing_split(JsonKeys.SPLIT_BY_NETWORK, 'wrongAppliance')

    @mock.patch.object(FakeCdnDbForReporting, 'get_username')
    def test_unauthorized_network_da_split(self, mock_db_method):
        """Error case of split by delivery appliance by non Velocix Admin user"""
        mock_db_method.return_value = 'test@cachelogic.com'

        # Sorting values due to exception already has these values sorted
        self.check_failing_split(JsonKeys.SPLIT_BY_NETWORK,
                                 JsonKeys.SPLIT_BY_NETWORK_DELIVERY_APPLIANCE,
                                 error=errors.UnrecognisedProperty,
                                 error_code=4002,
                                 error_description="The split dimension must be one of {0}.".format(
                                     ", ".join(sorted(["'content'", "'org'", "'httpStatus'", "'cacheStatus'"]))
                                 ),
                                 error_details={'property': ['splitBy']})

    def test_unavailable_network_da_split(self):
        """Error case of split by delivery appliance in the CDN Insights pack"""
        self.check_failing_split(JsonKeys.SPLIT_BY_NETWORK,
                                 JsonKeys.SPLIT_BY_NETWORK_DELIVERY_APPLIANCE,
                                 error=errors.PropertyValueError,
                                 error_code=4004,
                                 error_description="The split level must be one of 'networkTier', "
                                     "'networkSendingApplianceHostname', 'networkSendingApplianceIpAddress', "
                                     "'networkReceivingApplianceHostname', 'networkReceivingApplianceIpAddress'.",
                                 error_details={'property': ['splitBy', 'network']},
                                 report_id='tier-analysis')

    @mock.patch.object(FakeCdnDbForReporting, 'get_username')
    def test_unauthorized_network_report_split(self, mock_db_method):
        """Error case of split by delivery appliance by non Velocix Admin user"""
        mock_db_method.return_value = 'test@cachelogic.com'
        self.check_failing_split(
            JsonKeys.SPLIT_BY_NETWORK_RESPONSE,
            JsonKeys.SPLIT_BY_NETWORK_RESPONSE_SUCCESS_FAILURE,
            error=errors.UnrecognisedProperty,
            error_code=4002,
            error_description="The split dimension must be one of "
                              "'cacheStatus', 'content', 'httpStatus', "
                              "'org'.",
            error_details={'property': ['splitBy']},
        )

    def test_unavailable_network_report_split(self):
        """Error case of split by network response success or failure in the CDN Insights pack"""
        self.check_failing_split(
            JsonKeys.SPLIT_BY_NETWORK_RESPONSE, 'wrongNetworkResponseSplit',
            error=errors.PropertyValueError,
            error_code=4004,
            error_description=(
                "The split level must be one of "
                "'networkResponseSuccessFailure', 'networkResponseCodeClass', 'networkResponseCode'."
            ),
            error_details={'property': ['splitBy', 'networkResponse']},
            report_id='tier-analysis')

    def test_invalid_split(self):
        """Error case where an invalid split (in this case borg) is given"""
        self.check_failing_split('borg', 'wrongBorg',
                                 error=errors.UnrecognisedProperty, error_code=4002)


class TestGenerateFilter(SetupFlaskContext):
    """Test the method ReportJobCollection._generate_filter"""

    def check_filter(self, report_id, filter_params,
                     splunk_filter='', filter_lookup=None):
        """Helper method to test working filters.
        @param report_id - report ID for which to check the query parts.
        @param filter_params - filter to pass to _generate_filter
        @param splunk_filter - expected list of splunk filter splits
        @param filter_lookup - expected list of filter lookups
        @return filter optimisation callbacks
        """
        # Used by tier filter to control which zones are queried
        # Setup object
        obj = report_job_collection.ReportJobCollection(report_id=report_id)
        # Invoke code under test
        filter_splits, optimisations, filter_parts = obj._generate_filter(
            filter_params, {})

        # Check output of method is as expected
        self.assertEqual(
            splunk_filter.split('|'), filter_parts['filter'].split('|'),
            'The returned filter is incorrect')
        self.assertEqual(
            set(filter_lookup or ()),
            set(q['split_lookup'] for q in filter_splits
                if 'split_lookup' in q),
            'The returned lookup is incorrect')

        # Return any optimisation callbacks
        return optimisations

    def check_valid_filter(self, http_filter, cache_filter,
                           tier_filter, network_response_filter,
                           splunk_filter, report_id):
        """Helper method to test a working filter.
        @param http_filter - HTTP filter (list) as per the syntax used in JSON
                             message body
        @param cache_filter - Cache Disposition filter (list) as per syntax used
                              in JSON message body
        @param tier_filter - CDN Tier filter (list) as per syntax used in JSON
                             message body
        @param network_reponse_filter - Network Reponse filter (list) as per
                                        syntax used in JSON message body
        @param splunk_filter - expected Splunk query part
        @param report_id - report on which to run the test
        @return filter optimisation callbacks
        """
        filters = {}
        filter_params = {JsonKeys.FILTER_BY: filters}
        filter_lookup = []

        if cache_filter:
            filters[JsonKeys.FILTER_BY_CACHE_DISPOSITION] = cache_filter
            filter_lookup.append(report_job_collection.CACHE_STATUS_LOOKUP)

        if http_filter:
            filters[JsonKeys.FILTER_BY_HTTP_STATUS] = http_filter
            filter_lookup.append(report_job_collection.HTTP_STATUS_LOOKUP)

        if tier_filter:
            filters[JsonKeys.FILTER_BY_CDN_TIER] = tier_filter

        if network_response_filter:
            filters[JsonKeys.FILTER_BY_NETWORK_RESPONSE] = network_response_filter
            filter_lookup.append(report_job_collection.NETWORK_STATUS_LOOKUP)

        # Return any optimisation callbacks
        return self.check_filter(
            report_id, filter_params, splunk_filter=splunk_filter,
            filter_lookup=filter_lookup)

    def check_valid_http_filter(
            self, http_filter, splunk_filter, report_id=ReportIds.TRAFFIC):
        """Helper method to test a working filter.
        @param http_filter - syntax used in JSON message body
        @param splunk_filter - expected Splunk query part
        @param report_id - report on which to run the test (defaults to a
                           supported report)
        @return filter optimisation callbacks
        """
        return self.check_valid_filter(
            http_filter, None, None, None, splunk_filter, report_id)

    def check_valid_cache_disposition_filter(
            self, cache_filter, splunk_filter, report_id=ReportIds.TRAFFIC):
        """Helper method to test a working filter.
        @param cache_filter - syntax used in JSON message body
        @param splunk_filter - expected Splunk query part
        @param report_id - report on which to run the test (defaults to a
                           supported report)
        @return filter optimisation callbacks
        """
        return self.check_valid_filter(
            None, cache_filter, None, None, splunk_filter, report_id)

    def check_valid_cdn_tier_filter(
            self, tier_filter, splunk_filter,
            report_id=ReportIds.TIER_ANALYSIS):
        """Helper method to test a working filter.
        @param tier_filter - syntax used in JSON message body
        @param splunk_filter - expected Splunk query part
        @param report_id - report on which to run the test (defaults to a
                           supported report)
        @return Dictionary mapping sourcetype to selected zones, as modified by
                the filter optimisations.
        """
        callbacks = self.check_valid_filter(
            None, None, tier_filter, None, splunk_filter, report_id)
        # Apply the callbacks to query configuration with zone information.
        query_config = [
            {
                'mi_sourcetype': 'cdn_del_access',
                'mi_zones': ('external',)
            },
            {
                'mi_sourcetype': 'cdn_del_acquisition',
                'mi_zones': ('cdn-origin', 'external', 'cdn')
            },
        ]
        for callback in callbacks:
            query_config = callback(query_config)
        zones_by_sourcetype = {}
        for query in query_config:
            zones_by_sourcetype.setdefault(
                query['mi_sourcetype'], set()).update(query['mi_zones'])
        return zones_by_sourcetype

    def check_valid_network_response_filter(
            self, network_response_filter, splunk_filter,
            report_id=ReportIds.TIER_ANALYSIS):
        """Helper method to test a working filter.
        @param network_response_filter - syntax used in JSON message body
        @param splunk_filter - expected Splunk query part
        @param report_id - report on which to run the test (defaults to a
                           supported report)
        @return optimised tstat queries.
        """
        callbacks = self.check_valid_filter(
            None, None, None, network_response_filter, splunk_filter, report_id)
        # Apply the callbacks to query configuration with zone information.
        query_config = [
            {
                'mi_sourcetype': 'cdn_del_access',
                'network_response_classes': ['http'],
            },
            {
                # include events without an sc_status field; the
                # split_lookup will fill in a default for these.
                'mi_sourcetype': 'cdn_del_access',
                'network_response_classes': ['non_http'],
            },
            {
                'mi_sourcetype': 'cdn_del_acquisition',
                'network_response_classes': ['http', 'network_error'],
            },
            {
                # include events without an s_flags field; the split_lookup
                # will fill in a default for these.
                'mi_sourcetype': 'cdn_del_acquisition',
                'network_response_classes': ['http'],
            },
        ]
        for callback in callbacks:
            query_config = callback(query_config)
        return query_config

    def check_unsupported_filter(self, report_id, filter_json):
        """Check an invalid filter will raise UnrecognisedProperty error.
        @param report_id - report to check.
        @param filter_json - the filter to use
        @return The error description from the exception raised.
        """

        filter_params = {JsonKeys.FILTER_BY: filter_json}
        obj = report_job_collection.ReportJobCollection(report_id=report_id)

        # Check a property error is raised
        with self.assertRaises(errors.UnrecognisedProperty) as cm:
            obj._generate_filter(filter_params, {})
        err = cm.exception
        self.assertEquals(err.code, 400)
        self.assertEquals(err.error_code, 4002)
        self.assertEquals(err.error_details, {'property': ['filterBy']})
        return err.error_description

    def test_unrecognised_filter_cdn_reporting_pack(self):
        """Check an unrecognised filter raises the UnrecognisedProperty error"""
        filter_json = {"DUMMY": ["500"]}
        error_description = self.check_unsupported_filter(
            ReportIds.TRAFFIC, filter_json)
        # Check the exception description which varies between reporting pack
        self.assertEquals(
            error_description,
            "The filter must be one of 'cacheDisposition', 'httpStatus'.")

    def test_unrecognised_filter_insights_pack(self):
        """Check an unrecognised filter raises the UnrecognisedProperty error"""
        filter_json = {"DUMMY": ["500"]}
        error_description = self.check_unsupported_filter(
            ReportIds.TIER_ANALYSIS, filter_json)
        # Check the exception description which varies between reporting pack
        self.assertEquals(
            error_description,
            "The filter must be one of 'cdnTier', 'networkResponse'.")

    def test_unsupported_cdn_tier_filter(self):
        """Check an unsupported filter raises the UnrecognisedProperty error.
        The tier filter is not included in the cdn reporting pack."""
        filter_json = {JsonKeys.FILTER_BY: {JsonKeys.FILTER_BY_CDN_TIER: []}}
        error_description = self.check_unsupported_filter(
            ReportIds.TRAFFIC, filter_json)
        # Check the exception description which varies between reporting pack
        self.assertEquals(
            error_description,
            "The filter must be one of 'cacheDisposition', 'httpStatus'.")

    def test_unsupported_network_response_filter(self):
        """Check an unsupported filter raises the UnrecognisedProperty error.
        The tier filter is not included in the cdn reporting pack."""
        filter_json = {
            JsonKeys.FILTER_BY: {JsonKeys.FILTER_BY_NETWORK_RESPONSE: []}}
        error_description = self.check_unsupported_filter(
            ReportIds.TRAFFIC, filter_json)
        # Check the exception description which varies between reporting pack
        self.assertEquals(
            error_description,
            "The filter must be one of 'cacheDisposition', 'httpStatus'.")

    def test_unsupported_cache_status_filter(self):
        """Check an unsupported filter raises the UnrecognisedProperty error.
        The cache status filter is not included in the cdn insights pack."""
        filter_json = {
            JsonKeys.FILTER_BY: {JsonKeys.FILTER_BY_CACHE_DISPOSITION: []}}
        error_description = self.check_unsupported_filter(
            ReportIds.TIER_ANALYSIS, filter_json)
        # Check the exception description which varies between reporting pack
        self.assertEquals(
            error_description,
            "The filter must be one of 'cdnTier', 'networkResponse'.")

    def test_unsupported_http_status_filter(self):
        """Check an unsupported filter raises the UnrecognisedProperty error.
        The http status filter is not included in the cdn insights pack."""
        filter_json = {
            JsonKeys.FILTER_BY: {JsonKeys.FILTER_BY_HTTP_STATUS: []}}
        error_description = self.check_unsupported_filter(
            ReportIds.TIER_ANALYSIS, filter_json)
        # Check the exception description which varies between reporting pack
        self.assertEquals(
            error_description,
            "The filter must be one of 'cdnTier', 'networkResponse'.")

    def test_invalid_subfilter(self):
        """Check invalid filter value will raise PropertyValueError error"""
        # The "httpStatus" filter is but subfilters > 600 are not in the reporting pack
        filter_params = {
            JsonKeys.FILTER_BY: {
                JsonKeys.FILTER_BY_HTTP_STATUS: [
                    200, "300", "4xx", "success", "failure", 5000, "5000",
                    "6000"]}}
        obj = report_job_collection.ReportJobCollection(report_id=ReportIds.TRAFFIC)
        with self.assertRaises(errors.PropertyValueError) as cm:
            obj._generate_filter(filter_params, {})
        err = cm.exception
        self.assertEquals(err.code, 400)
        self.assertEquals(err.error_code, 4004)  # PropertyValue error
        # Error does not include any of the valid subfilters nor duplicates
        self.assertEquals(
            err.error_description,
            "The filter values not allowed '5000', '6000'.")
        self.assertEquals(
            err.error_details, {'property': ['filterBy', 'httpStatus']})

    def test_traffic_report_default(self):
        """Check the Traffic report has the correct default filter."""
        self.check_filter(ReportIds.TRAFFIC, {})

    def test_request_report_default(self):
        """Check the Request report has the correct default filter."""
        self.check_filter(ReportIds.REQUEST, {})

    def test_cache_hit_report_default(self):
        """Check the cache hit report has the correct default filter."""
        self.check_filter(ReportIds.CACHE_HIT, {})

    def test_http_status_report_default(self):
        """Check the http status report has the correct default filter."""
        self.check_filter(ReportIds.HTTP_STATUS_CODE, {})

    def test_content_popularity_report_default(self):
        """Check the content_popularity report has the correct default filter."""
        self.check_filter(
            ReportIds.CONTENT_POPULARITY, {},
            splunk_filter='| search (status_type = "Successful 2xx") ',
            filter_lookup=[report_job_collection.HTTP_STATUS_LOOKUP])

    def test_tier_analysis_report_default(self):
        """Check the tier analysis report has the correct default filter."""
        self.check_filter(ReportIds.TIER_ANALYSIS, {})

    def test_empty_filter(self):
        """Normal case where an empty filter is given """
        self.check_filter(ReportIds.TRAFFIC, {JsonKeys.FILTER_BY: {}})

    def test_overriding_default_filter(self):
        """Test overriding a default filter"""
        self.check_filter(
            ReportIds.CONTENT_POPULARITY, {JsonKeys.FILTER_BY: {}})

    def test_empty_http_status_filter(self):
        """Normal case where an empty HTTP Status filter is given """
        self.check_filter(
            ReportIds.TRAFFIC, {
                JsonKeys.FILTER_BY: {JsonKeys.FILTER_BY_HTTP_STATUS: []}})

    def test_http_success(self):
        """Filter on HTTP Statuses for Success"""
        self.check_valid_http_filter(
            ['success'], '| search (status_ok = "Success") ')

    def test_http_failure(self):
        """Filter on HTTP Statuses for Failure"""
        self.check_valid_http_filter(
            ['failure'], '| search (status_ok = "Failure") ')

    def test_http_success_failure(self):
        """Filter on HTTP Statuses Success or Failure (nonsense really)"""
        self.check_valid_http_filter(
            ['success', 'failure'],
            '| search (status_ok = "Success" OR status_ok = "Failure") ')

    def test_http_1xx(self):
        """Filter on HTTP Statuses 100 to 199"""
        self.check_valid_http_filter(
            ['1xx'], '| search (status_type = "Informational 1xx") ')

    def test_http_2xx(self):
        """Filter on HTTP Statuses 200 to 299"""
        self.check_valid_http_filter(
            ['2xx'], '| search (status_type = "Successful 2xx") ')

    def test_http_3xx(self):
        """Filter on HTTP Statuses 300 to 399"""
        self.check_valid_http_filter(
            ['3xx'], '| search (status_type = "Redirection 3xx") ')

    def test_http_4xx(self):
        """Filter on HTTP Statuses 400 to 499"""
        self.check_valid_http_filter(
            ['4xx'], '| search (status_type = "Client Error 4xx") ')

    def test_http_5xx(self):
        """Filter on HTTP Statuses 500 to 599"""
        self.check_valid_http_filter(
            ['5xx'], '| search (status_type = "Server Error 5xx") ')

    def test_http_class(self):
        """Filter on all HTTP Status classes (nonsense really)"""
        self.check_valid_http_filter(
            ['1xx', '2xx', '3xx', '4xx', '5xx'],
            '| search (status_type = "Informational 1xx" '
            'OR status_type = "Successful 2xx" '
            'OR status_type = "Redirection 3xx" '
            'OR status_type = "Client Error 4xx" '
            'OR status_type = "Server Error 5xx") ')

    def test_http_status(self):
        """Filter on a HTTP Status Code"""
        self.check_valid_http_filter([100], '| search (sc_status = 100) ')
        self.check_valid_http_filter(["201"], '| search (sc_status = 201) ')

    def test_http_statuses(self):
        """Filter on HTTP Statuses Codes"""
        self.check_valid_http_filter(
            [100, "201", '304', 401, 500],
            '| search (sc_status = 100 '
            'OR sc_status = 201 '
            'OR sc_status = 304 '
            'OR sc_status = 401 '
            'OR sc_status = 500) ')

    def test_http_status_combo(self):
        """Filter on a combinare of HTTP Status success, class and codes."""
        self.check_valid_http_filter(
            ["2xx", "200", 301, "304", "failure"],
            '| search (status_type = "Successful 2xx" '
            'OR sc_status = 200 OR sc_status = 301 '
            'OR sc_status = 304 OR status_ok = "Failure") ')

    def test_empty_cache_disposition_filter(self):
        """Normal case where an empty Cache Disposition filter is given """
        self.check_filter(
            ReportIds.TRAFFIC, {
                JsonKeys.FILTER_BY: {JsonKeys.FILTER_BY_CACHE_DISPOSITION: []}})

    def test_cache_disposition_hit(self):
        """Filter on Cache Disposition for Hit"""
        self.check_valid_cache_disposition_filter(
            [JsonKeys.FILTER_BY_CACHE_DISPOSITION_HIT],
            '| search (cache_hit = 1) ')

    def test_cache_disposition_miss(self):
        """Filter on Cache Disposition for Failure"""
        self.check_valid_cache_disposition_filter(
            [JsonKeys.FILTER_BY_CACHE_DISPOSITION_MISS],
            '| search (cache_hit = 0) ')

    def test_cache_disposition_hit_miss(self):
        """Filter on Cache Disposition Hit or Miss (nonsense really)"""
        self.check_valid_cache_disposition_filter(
            [JsonKeys.FILTER_BY_CACHE_DISPOSITION_HIT,
             JsonKeys.FILTER_BY_CACHE_DISPOSITION_MISS],
            '| search (cache_hit = 1 OR cache_hit = 0) ')

    def test_cache_disposition_mem_hit(self):
        """Filter on Cache Disposition CACHE_MEM_HIT"""
        self.check_valid_cache_disposition_filter(
            [JsonKeys.FILTER_BY_CACHE_MEM_HIT],
            '| search (s_cachestatus = CACHE_MEM_HIT) ')

    def test_cache_disposition_cache_hit(self):
        """Filter on Cache Disposition CACHE_HIT"""
        self.check_valid_cache_disposition_filter(
            [JsonKeys.FILTER_BY_CACHE_HIT],
            '| search (s_cachestatus = CACHE_HIT) ')

    def test_cache_disposition_vxicp_hit(self):
        """Filter on Cache Disposition VXICP_HIT"""
        self.check_valid_cache_disposition_filter(
            [JsonKeys.FILTER_BY_CACHE_VXICP_HIT],
            '| search (s_cachestatus = VXICP_HIT) ')

    def test_cache_disposition_revalidated_mem_hit(self):
        """Filter on Cache Disposition CACHE_REVALIDATED_MEM_HIT"""
        self.check_valid_cache_disposition_filter(
            [JsonKeys.FILTER_BY_CACHE_REVALIDATED_MEM_HIT],
            '| search (s_cachestatus = CACHE_REVALIDATED_MEM_HIT) ')

    def test_cache_disposition_revalidated_hit(self):
        """Filter on Cache Disposition CACHE_REVALIDATED_HIT"""
        self.check_valid_cache_disposition_filter(
            [JsonKeys.FILTER_BY_CACHE_REVALIDATED_HIT],
            '| search (s_cachestatus = CACHE_REVALIDATED_HIT) ')

    def test_cache_disposition_cache_miss(self):
        """Filter on Cache Disposition CACHE_MISS"""
        self.check_valid_cache_disposition_filter(
            [JsonKeys.FILTER_BY_CACHE_MISS],
            '| search (s_cachestatus = CACHE_MISS) ')

    def test_cache_disposition_revalidated_miss(self):
        """Filter on Cache Disposition CACHE_REVALIDATED_MISS"""
        self.check_valid_cache_disposition_filter(
            [JsonKeys.FILTER_BY_CACHE_REVALIDATED_MISS],
            '| search (s_cachestatus = CACHE_REVALIDATED_MISS) ')

    def test_cache_disposition_background_fill(self):
        """Filter on Cache Disposition CACHE_BACKGROUND_FILL"""
        self.check_valid_cache_disposition_filter(
            [JsonKeys.FILTER_BY_CACHE_BACKGROUND_FILL],
            '| search (s_cachestatus = CACHE_BACKGROUND_FILL) ')

    def test_cache_disposition_bad_request(self):
        """Filter on Cache Disposition BAD_REQUEST"""
        self.check_valid_cache_disposition_filter(
            [JsonKeys.FILTER_BY_CACHE_BAD_REQUEST],
            '| search (s_cachestatus = BAD_REQUEST) ')

    def test_cache_disposition_serv_fail(self):
        """Filter on Cache Disposition SERV_FAIL"""
        self.check_valid_cache_disposition_filter(
            [JsonKeys.FILTER_BY_CACHE_SERV_FAIL],
            '| search (s_cachestatus = SERV_FAIL) ')

    def test_cache_disposition_unknown(self):
        """Filter on Cache Disposition unknown (-)"""
        self.check_valid_cache_disposition_filter(
            [JsonKeys.FILTER_BY_CACHE_UNKNOWN], '| search (s_cachestatus = -) ')

    def test_cache_dispositions(self):
        """Filter on Cache Disposition Codes"""
        self.check_valid_cache_disposition_filter(
            [JsonKeys.FILTER_BY_CACHE_HIT,
             JsonKeys.FILTER_BY_CACHE_DISPOSITION_MISS],
            '| search (s_cachestatus = CACHE_HIT OR cache_hit = 0) ')

    def test_cache_hit_and_http_2xx(self):
        """Filter on a combined HTTP Status and Cache Disposition Filter"""
        self.check_valid_filter(
            ['2xx'], [JsonKeys.FILTER_BY_CACHE_HIT], None, None,
            '| search (s_cachestatus = CACHE_HIT) '
            ' (status_type = "Successful 2xx") ',
            report_id=ReportIds.TRAFFIC)

    def test_cdn_tier_delivery(self):
        """Filter on the delivery CDN Tier."""
        zone_config = self.check_valid_cdn_tier_filter(
            [JsonKeys.FILTER_BY_CDN_TIER_DELIVERY], '')
        # Check the the query configuration only contains the delivery tier.
        # Check the the query configuration only contains the delivery tier.
        self.assertEqual(zone_config, {
            'cdn_del_access': set(['external']),
        })

    def test_cdn_tier_intermediate(self):
        """Filter on the intermediate CDN Tier."""
        zone_config = self.check_valid_cdn_tier_filter(
            [JsonKeys.FILTER_BY_CDN_TIER_INTERMEDIATE], '')
        # Check the the query configuration only contains the intermediate tier.
        # Check the the query configuration only contains the delivery tier.
        self.assertEqual(zone_config, {
            'cdn_del_acquisition': set(['cdn']),
        })

    def test_cdn_tier_acquisition(self):
        """Filter on the acquisition CDN Tier."""
        zone_config = self.check_valid_cdn_tier_filter(
            [JsonKeys.FILTER_BY_CDN_TIER_ACQUIRY], '')
        # Check the the query configuration only contains the acquiry tier.
        self.assertEqual(zone_config, {
            'cdn_del_acquisition': set(['cdn-origin', 'external']),
        })

    def test_network_responses_non_http(self):
        """Filter on the Non-HTTP network responses."""
        query_config = self.check_valid_network_response_filter(
            [JsonKeys.FILTER_BY_NETWORK_RESPONSE_NON_HTTP],
            '| search (status_type = "Non-HTTP") ')
        # Check the the query configuration only contains the one rtmp query.
        self.assertEqual(query_config, [{
            'mi_sourcetype': 'cdn_del_access',
            'network_response_classes': ['non_http'],
        }])

    def test_network_responses_network_error(self):
        """Filter on the network error responses."""
        query_config = self.check_valid_network_response_filter(
            [JsonKeys.FILTER_BY_NETWORK_RESPONSE_NETWORK_ERROR],
            '| search (status_type = "Network Error") ')
        # Check the the query configuration only contains the one network error
        # query.
        self.assertEqual(query_config, [{
            'mi_sourcetype': 'cdn_del_acquisition',
            'network_response_classes': ['network_error'],
        }])

    def test_network_responses_http_status_codes(self):
        """Filter on the HTTP status network responses."""
        query_config = self.check_valid_network_response_filter(
            ['1xx', '2xx', '3xx', '4xx', '5xx'],
            '| search (status_type = "Informational 1xx" '
            'OR status_type = "Successful 2xx" '
            'OR status_type = "Redirection 3xx" '
            'OR status_type = "Client Error 4xx" '
            'OR status_type = "Server Error 5xx") ')
        # Check the the query configuration only contains http queries.
        self.assertEqual(query_config, [
            {
                'mi_sourcetype': 'cdn_del_access',
                'network_response_classes': ['http'],
            },
            {
                'mi_sourcetype': 'cdn_del_acquisition',
                'network_response_classes': ['http'],
            },
            {
                'mi_sourcetype': 'cdn_del_acquisition',
                'network_response_classes': ['http'],
            },
        ])

    def test_network_responses_success(self):
        """Filter on the successful network responses."""
        query_config = self.check_valid_network_response_filter(
            [JsonKeys.FILTER_BY_NETWORK_RESPONSE_SUCCESS],
            '| search (status_ok = "Success") ')
        # Check the the query configuration only contains http and non-http
        # queries.
        self.assertEqual(query_config, [
            {
                'mi_sourcetype': 'cdn_del_access',
                'network_response_classes': ['http'],
            },
            {
                'mi_sourcetype': 'cdn_del_access',
                'network_response_classes': ['non_http'],
            },
            {
                'mi_sourcetype': 'cdn_del_acquisition',
                'network_response_classes': ['http'],
            },
            {
                'mi_sourcetype': 'cdn_del_acquisition',
                'network_response_classes': ['http'],
            },
        ])

    def test_network_responses_failure(self):
        """Filter on the failing network responses."""
        query_config = self.check_valid_network_response_filter(
            [JsonKeys.FILTER_BY_NETWORK_RESPONSE_FAILURE],
            '| search (status_ok = "Failure") ')
        # Check the the query configuration only contains everything but the
        # rtmp query.
        self.assertEqual(query_config, [
            {
                'mi_sourcetype': 'cdn_del_access',
                'network_response_classes': ['http'],
            },
            {
                'mi_sourcetype': 'cdn_del_acquisition',
                'network_response_classes': ['http', 'network_error'],
            },
            {
                'mi_sourcetype': 'cdn_del_acquisition',
                'network_response_classes': ['http'],
            },
        ])

    def test_cdn_acquistion_tier_and_network_response_failure(self):
        """Filter on both the CDN acquisition tier and network response failures

        This requires that queries are optimised for *two different aspects*.
        """
        callbacks = self.check_valid_filter(
            None, None, [JsonKeys.FILTER_BY_CDN_TIER_ACQUIRY],
            [JsonKeys.FILTER_BY_NETWORK_RESPONSE_FAILURE],
            '| search (status_ok = "Failure") ',
            report_id=ReportIds.TIER_ANALYSIS)
        # Apply the callbacks to query configuration with zone information.
        query_config = [
            {
                'mi_sourcetype': 'cdn_del_access',
                'mi_zones': ('external',),
                'network_response_classes': ['http'],
            },
            {
                # include events without an sc_status field; the
                # split_lookup will fill in a default for these.
                'mi_sourcetype': 'cdn_del_access',
                'mi_zones': ('external',),
                'network_response_classes': ['non_http'],
            },
            {
                'mi_sourcetype': 'cdn_del_acquisition',
                'mi_zones': ('cdn-origin', 'external', 'cdn'),
                'network_response_classes': ['http', 'network_error'],
            },
            {
                # include events without an s_flags field; the split_lookup
                # will fill in a default for these.
                'mi_sourcetype': 'cdn_del_acquisition',
                'mi_zones': ('cdn-origin', 'external', 'cdn'),
                'network_response_classes': ['http'],
            },
        ]
        for callback in callbacks:
            query_config = callback(query_config)

        # We end up with just acquisition tier queries
        self.assertEqual(query_config, [
            {
                'mi_sourcetype': 'cdn_del_acquisition',
                'mi_zones': ('cdn-origin', 'external'),
                'network_response_classes': ['http', 'network_error'],
            },
            {
                # include events without an s_flags field; the split_lookup
                # will fill in a default for these.
                'mi_sourcetype': 'cdn_del_acquisition',
                'mi_zones': ('cdn-origin', 'external'),
                'network_response_classes': ['http'],
            },
        ])


class TestRemoveZone(unittest.TestCase):
    """Tests for utility method ReportJobCollection._remove_zone.
    This method is used in tier filtering to remove zones which were orginally
    defined by the metric."""

    def test_removing_zone_from_query_config(self):
        """Test the normal case where the zone is successfully removed."""
        tstats_queries = [{
            "mi_sourcetype": 'cdn_del_access',
            'mi_zones': ("zone1", "zone2")
        }]
        report_job_collection.ReportJobCollection._remove_zone(
            tstats_queries, 'cdn_del_access', 'zone1')
        self.assertEqual(tstats_queries[0]['mi_zones'], ("zone2",))

    def test_sourcetype_not_in_query_config(self):
        """Test the error case where the source type is not in the query config."""
        report_job_collection.ReportJobCollection._remove_zone(
            [], "cdn_del_access", "zone")

    def test_zone_not_in_query_config(self):
        """Test the error case where the zone is not in the query config."""
        tstats_queries = [{
            "mi_sourcetype": 'cdn_del_access',
            'mi_zones': ()
        }]
        report_job_collection.ReportJobCollection._remove_zone(
            tstats_queries, "cdn_del_acquisition", "zone")


class BuildSearchQueryValidationMixin(SetupFlaskContext):
    """Base class for testing _build_search_query with different reporting packs"""

    def setUp(self):
        """Setup mocks"""

        # Call normal initialiser for mocks
        super(BuildSearchQueryValidationMixin, self).setUp()

        # Addtional mocks
        patch = mock.patch.object(report_job_collection.ReportJobCollection, 'get_cdn_db',
                                  new=FakeCdnDbForReporting)
        patch.start()
        self.addCleanup(patch.stop)

        patch = mock.patch.object(FakeCdnDbForReporting, 'get_orgs_with_sites',
                                  return_value=[4,5,2])
        patch.start()
        self.addCleanup(patch.stop)

        patch = mock.patch.object(report_job_collection, 'saved_search_available',
                                  return_value=True)
        patch.start()
        self.addCleanup(patch.stop)

    @mock.patch.object(report_job_collection, 'saved_search_available')
    def invoke_build_search_query_no_saved_search(self, metric, start_time, end_time,
                                                  time_resolution, given_use_split, mock_method):
        # Have saved_search_available() return False
        mock_method.return_value = False

        self.invoke_build_search_query(metric, start_time, end_time,
                                       time_resolution, use_split=given_use_split)

    def _invoke_build_search_query(self, metric, start_time, end_time,
                                  time_resolution, split_by, filter_by,
                                  split_limit, report_id):
        """Create a report job collection and call @c _parse_time_parameters.
        This method stores the object and query parts as returned by @c
        _parse_time_parameters in member variables.
        @param metric - parameter for method under test
        @param start_time - parameter for method under test
        @param end_time - parameter for method under test
        @param time_resolution - parameter for method under test
        @param split_by - the split to use if any
        @param filter_by - the filter to use if any
        @param split_limit - the split limit that has to be used
        @param report_id - report to run - defaults to the @c traffic report
        """

        # Create a report job collection object
        self.obj = report_job_collection.ReportJobCollection(report_id=report_id)

        # Use a fixed value for now - 26/03/2014 18:52:45
        # Must be at least 3am for the tests to work
        self.obj._now = datetime(2014, 3, 26, 18, 52, 45)

        # Configure the parameters handled by _parse_time_parameters
        params = {
            JsonKeys.METRIC: metric,
            JsonKeys.START_TIME: start_time,
            JsonKeys.END_TIME: end_time,
            JsonKeys.TIME_RESOLUTION: time_resolution,
            JsonKeys.SPLIT_LIMIT: split_limit
        }

        if filter_by is not None:
            params.update({JsonKeys.FILTER_BY: filter_by})

        if split_by is not None:
            params.update({JsonKeys.SPLIT_BY: split_by})

        # Call the method under test and store its return value
        self.query_string, unused_snap_to_start_time = self.obj._build_search_query(params)

    # Increase the maximum diff size so we can see differences between
    # expected and produced query as a full list element difference.
    # 4k bytes should be plenty for even the most gnarly of queries.
    maxDiff = 4196

    def check_query(self, expected_query_string):
        """Check generated query is as expected.

        @param expected_query_string - expected splunk query string

        The expected and produced query strings are processed to highlight
        differences between them:

        - A | pipe signals a new query section
        - Whitespace outside quotes is normalised to 1 space
        - Individual query parts are wrapped at 70 characters.

        If the produced query doesn't match the expected, differences are shown
        between expected and produced as if the produced query is a newer
        revision; + changes denote parts added relative to the expected, changes
        marked with - were removed relative to the expected.

        """
        def split_query(query, pipe_split=re.compile(r'\|?[^|]+').findall):
            # The query is split first on | pipe symbols (but preserving them
            # in the resulting parts). We then collapse whitespace (except where
            # quoted) and break up lines to separate strings of at most 70
            # characters to ease spotting errors.
            # By parsing the part as a non-POSIX shell string, we get to
            # preserve the quotes in the input.
            return [line for p in pipe_split(query)
                    for line in textwrap.wrap(
                        ' '.join(shlex.split(p, posix=False)))]

        self.assertEqual(
            split_query(expected_query_string),
            split_query(self.query_string))


class TestCDNReportingBuildSearchQuery(BuildSearchQueryValidationMixin):
    """Test for the _build_search_query with the CDN Reporting Pack.
    Tests for the @ref
    unapireporting.datamodel.report_job_collection.ReportJobCollection._build_search_query method.
    This checks the various ways in which the search query can be built up.

    @todo we should check the localisation when we are building search queries
    since they directly impact what is dispalyed on the console.
    """

    def invoke_build_search_query(self, metric,
                                  start_time, end_time, time_resolution,
                                  use_split=False, use_filter=False,
                                  split_limit=DEFAULT_SPLIT_LIMIT,
                                  report_id='traffic'):
        """Create a report job collection and call @c _parse_time_parameters.
        This method stores the object and query parts as returned by @c
        _parse_time_parameters in member variables.
        @param metric - parameter for method under test
        @param start_time - parameter for method under test
        @param end_time - parameter for method under test
        @param time_resolution - parameter for method under test
        @param use_split - whether or not to use a split
        @param use_filter - whether or not to use a filter
        @param split_limit - the split limit that has to be used
        @param report_id - report to run, defaults to the @c traffic report
        """

        # Split by HTTP Status Code Class or None
        split_by = {JsonKeys.SPLIT_BY_HTTP_STATUS: JsonKeys.SPLIT_BY_HTTP_STATUS_CODE_CLASS} if use_split else {}

        # Filter by HTTP Status Code Sucesses or None
        filter_by = {JsonKeys.FILTER_BY_HTTP_STATUS: [JsonKeys.FILTER_BY_HTTP_ALL_SUCCESS]} if use_filter else {}

        self._invoke_build_search_query(metric, start_time, end_time,
                                        time_resolution, split_by, filter_by,
                                        split_limit, report_id)

    def test_peak_query_timechart(self):
        """Test peak query splitting by time only.
        The query can only use the main index, this tests the following
        scenario:
        - <b>Main Index (access)</b>

        This is then combined with a chart query to present the results, as
        presented in the following list. This test checks the highlighted
        scenario:
        - time chart (with split) + time chart postfix + query postfix
        - <b>time chart (no split) + time chart postfix + query postfix</b>
        - chart with split + split chart postfix + query postfix
        - total chart + query postfix
        """
        self.invoke_build_search_query(JsonKeys.METRIC_PEAK_DELIVERED, "2days", "0days", "1hour")
        self.check_query(
            '| tstats count as s sum(sc_bytes) as x where index="cdn_main" '
            'sourcetype=cdn_del_access earliest=03/24/2014:00:00:00 latest=03/26/2014:00:00:00 '
            'c_vx_zone="external" (host=4 OR host=5 OR host=2)  by _time span=5m '
            '| timechart span=5m limit=12  sum(s) as s eval(sum(x)*8/300) as x   '
            '| append [gentimes start=03/24/2014:00:00:00 end=03/26/2014:00:00:00 increment=1h '
            '| rename starttime as _time | fields _time] '
            '| timechart max(x) as "Peak throughput (bps)" sum(s) as #samples span=1h '
            '| convert timeformat="%Y-%m-%dT%H:%M:%SZ" ctime(_time) | sort _time| rename _time as Time'
            '| fillnull | fields - _* NULL VALUE')

    def test_peak_query_timechart_with_split(self):
        """Test peak query splitting by time and http status.
        The query can only use the main index, this tests the following
        scenario:
        - <b>Main Index (access)</b> (with Split)

        This is then combined with a chart query to present the results, as
        presented in the following list. This test checks the highlighted
        scenario:
        - <b>time chart (with split) + time chart postfix + query postfix</b>
        - time chart (no split) + time chart postfix + query postfix
        - chart with split + split chart postfix + query postfix
        - total chart + query postfix
        """
        self.invoke_build_search_query(JsonKeys.METRIC_PEAK_REQUEST_RATE, "10days", "0days", "1day", use_split=True)
        self.check_query(
            '| tstats  count as x where index="cdn_main" sourcetype=cdn_del_access '
            'earliest=03/16/2014:00:00:00 latest=03/26/2014:00:00:00 c_vx_zone="external" '
            '(host=4 OR host=5 OR host=2)  by _time span=5m, sc_status '
            '| lookup http_status_codes status AS sc_status OUTPUTNEW status_type, status_ok '
            '| timechart span=5m limit=12  sum(x) as s eval(sum(x)/300) as x by status_type '
            '| append [gentimes start=03/16/2014:00:00:00 end=03/26/2014:00:00:00 increment=1d '
            '| rename starttime as _time | fields _time] '
            '| timechart max(x: *) as * sum(s: *) as #samples:* span=1d '
            '| convert timeformat="%Y-%m-%dT%H:%M:%SZ" ctime(_time) | sort _time| rename _time as Date'
            '| fillnull | fields - _* NULL VALUE')

    def test_peak_query_timechart_with_filter(self):
        """Test peak query splitting by time only, filtering by http status.
        The query can only use the main index, this tests the following
        scenario:
        - <b>Main Index (access)</b> (with Filter)

        This is then combined with a chart query to present the results, as
        presented in the following list. This test checks the highlighted
        scenario:
        - time chart (with split) + time chart postfix + query postfix
        - <b>time chart (no split) + time chart postfix + query postfix</b>
        - chart with split + split chart postfix + query postfix
        - total chart + query postfix
        """
        self.invoke_build_search_query(JsonKeys.METRIC_PEAK_REQUEST_RATE, "10days", "0days", "1day", use_filter=True)
        # Note tstat query will also have a split so that the filter works.
        self.check_query(
            '| tstats  count as x where index="cdn_main" sourcetype=cdn_del_access '
            'earliest=03/16/2014:00:00:00 latest=03/26/2014:00:00:00 c_vx_zone="external" '
            '(host=4 OR host=5 OR host=2)  by _time span=5m, sc_status '
            '| lookup http_status_codes status AS sc_status OUTPUTNEW status_type, status_ok '
            '| search (status_type = "Successful 2xx") '
            '| timechart span=5m limit=12  sum(x) as s eval(sum(x)/300) as x   '
            '| append [gentimes start=03/16/2014:00:00:00 end=03/26/2014:00:00:00 '
            'increment=1d | rename starttime as _time | fields _time] '
            '| timechart max(x) as "Peak request rate (rps)" sum(s) as #samples span=1d '
            '| convert timeformat="%Y-%m-%dT%H:%M:%SZ" ctime(_time) | sort _time| rename _time as Date'
            '| fillnull | fields - _* NULL VALUE')

    def test_peak_query_timechart_with_split_and_filter(self):
        """Test peak query splitting by time and http status, filtering by http status.
        The query can only use the main index, this tests the following
        scenario:
        - <b>Main Index (access)</b> (with Split and Filter)

        This is then combined with a chart query to present the results, as
        presented in the following list. This test checks the highlighted
        scenario:
        - <b>time chart (with split) + time chart postfix + query postfix</b>
        - time chart (no split) + time chart postfix + query postfix
        - chart with split + split chart postfix + query postfix
        - total chart + query postfix
        """
        self.invoke_build_search_query(
            JsonKeys.METRIC_PEAK_DELIVERED, "10days", "0days", "1day",
            use_split=True, use_filter=True)
        self.check_query(
            '| tstats count as s sum(sc_bytes) as x where index="cdn_main" '
            'sourcetype=cdn_del_access earliest=03/16/2014:00:00:00 latest=03/26/2014:00:00:00 '
            'c_vx_zone="external" (host=4 OR host=5 OR host=2)  by _time span=5m, sc_status '
            '| lookup http_status_codes status AS sc_status OUTPUTNEW status_type, status_ok '
            '| search (status_type = "Successful 2xx") '
            '| timechart span=5m limit=12  sum(s) as s eval(sum(x)*8/300) as x by status_type '
            '| append [gentimes start=03/16/2014:00:00:00 end=03/26/2014:00:00:00 increment=1d '
            '| rename starttime as _time | fields _time] '
            '| timechart max(x: *) as * sum(s: *) as #samples:* span=1d '
            '| convert timeformat="%Y-%m-%dT%H:%M:%SZ" ctime(_time) | sort _time| rename _time as Date'
            '| fillnull | fields - _* NULL VALUE')

    def test_peak_query_splitchart(self):
        """Test peak query splitting by http status only.
        The query can only use the main index, this tests the following
        scenario:
        - <b>Main Index (access)</b> (with Split)

        This is then combined with a chart query to present the results, as
        presented in the following list. This test checks the highlighted
        scenario:
        - time chart (with split) + time chart postfix + query postfix
        - time chart (no split) + time chart postfix + query postfix
        - <b>chart with split + split chart postfix + query postfix</b>
        - total chart + query postfix
        """
        self.invoke_build_search_query(
            JsonKeys.METRIC_PEAK_DELIVERED, "10days", "0days", "noTimeSplit",
            use_split=True, split_limit=7)
        self.check_query(
            '| tstats count as s sum(sc_bytes) as x where index="cdn_main" '
            'sourcetype=cdn_del_access earliest=03/16/2014:00:00:00 latest=03/26/2014:00:00:00 '
            'c_vx_zone="external" (host=4 OR host=5 OR host=2)  by _time span=5m, sc_status '
            '| lookup http_status_codes status AS sc_status OUTPUTNEW status_type, status_ok '
            '| eventstats sum(s) as s sum(x) as x by _time, status_type'
            '| chart limit=7 sum(s) as s eval(max(x)*8/300) as x by status_type'
            '| sort - x | streamstats count as _row_number '
            '| eval status_type=if(0 = 7 or _row_number <= 7, status_type, "Other") '
            '| stats sum(*) as * by status_type | rename x as "Peak throughput (bps)", '
            's as #samples status_type as "HTTP Status Class"'
            '| fillnull | fields - _* NULL VALUE')

    def test_peak_query_totalchart(self):
        """Test peak query without a split.
        The query can only use the main index, this tests the following
        scenario:
        - <b>Main Index (access)</b>

        This is then combined with a chart query to present the results, as
        presented in the following list. This test checks the highlighted
        scenario:
        - time chart (with split) + time chart postfix + query postfix
        - time chart (no split) + time chart postfix + query postfix
        - chart with split + split chart postfix + query postfix
        - <b>total chart + query postfix</b>
        """
        self.invoke_build_search_query(
            JsonKeys.METRIC_PEAK_REQUEST_RATE, "10days", "0days", "noTimeSplit",
            split_limit=8)
        self.check_query(
            '| tstats  count as x where index="cdn_main" sourcetype=cdn_del_access '
            'earliest=03/16/2014:00:00:00 latest=03/26/2014:00:00:00 c_vx_zone="external" '
            '(host=4 OR host=5 OR host=2)  by _time span=5m '
            '| eventstats sum(x) as s sum(x) as x by _time| chart limit=8 sum(x) '
            'as #samples eval(max(x)/300) as "Peak request rate (rps)"'
            '| fillnull | fields - _* NULL VALUE')

    def test_perc95_query_timechart(self):
        """Test 95th percentile query splitting by time only.
        The query can only use the main index, this tests the following
        scenario:
        - <b>Main Index (access)</b>

        This is then combined with a chart query to present the results, as
        presented in the following list. This test checks the highlighted
        scenario:
        - time chart (with split) + time chart postfix + query postfix
        - <b>time chart (no split) + time chart postfix + query postfix</b>
        - chart with split + split chart postfix + query postfix
        - total chart + query postfix
        """
        self.invoke_build_search_query(
            JsonKeys.METRIC_95TH_PERCENTILE_DELIVERED, "2days", "0days", "1hour")
        self.check_query(
            '| tstats count as s sum(sc_bytes) as x where index="cdn_main" '
            'sourcetype=cdn_del_access earliest=03/24/2014:00:00:00 latest=03/26/2014:00:00:00 '
            'c_vx_zone="external" (host=4 OR host=5 OR host=2)  by _time span=5m '
            '| timechart span=5m limit=12  sum(s) as s eval(sum(x)*8/300) as x   '
            '| append [gentimes start=03/24/2014:00:00:00 end=03/26/2014:00:00:00 increment=1h '
            '| rename starttime as _time | fields _time] '
            '| timechart perc95(x) as "95th percentile throughput (bps)" sum(s) as #samples span=1h '
            '| convert timeformat="%Y-%m-%dT%H:%M:%SZ" ctime(_time) | sort _time| rename _time as Time'
            '| fillnull | fields - _* NULL VALUE')

    def test_perc95_query_timechart_with_split(self):
        """Test 95th percentile query splitting by time and http status.
        The query can only use the main index, this tests the following
        scenario:
        - <b>Main Index (access)</b> (with Split)

        This is then combined with a chart query to present the results, as
        presented in the following list. This test checks the highlighted
        scenario:
        - <b>time chart (with split) + time chart postfix + query postfix</b>
        - time chart (no split) + time chart postfix + query postfix
        - chart with split + split chart postfix + query postfix
        - total chart + query postfix
        """
        self.invoke_build_search_query(
            JsonKeys.METRIC_95TH_PERCENTILE_DELIVERED, "10days", "0days", "1day",
            use_split=True)
        self.check_query(
            '| tstats count as s sum(sc_bytes) as x where index="cdn_main" '
            'sourcetype=cdn_del_access earliest=03/16/2014:00:00:00 latest=03/26/2014:00:00:00 '
            'c_vx_zone="external" (host=4 OR host=5 OR host=2)  by _time span=5m, sc_status '
            '| lookup http_status_codes status AS sc_status OUTPUTNEW status_type, status_ok '
            '| timechart span=5m limit=12  sum(s) as s eval(sum(x)*8/300) as x by status_type '
            '| append [gentimes start=03/16/2014:00:00:00 end=03/26/2014:00:00:00 increment=1d '
            '| rename starttime as _time | fields _time] '
            '| timechart perc95(x: *) as * sum(s: *) as #samples:* span=1d '
            '| convert timeformat="%Y-%m-%dT%H:%M:%SZ" ctime(_time) | sort _time| rename _time as Date'
            '| fillnull | fields - _* NULL VALUE')

    def test_perc95_query_timechart_with_filter(self):
        """Test 95th percentile query splitting by time only, filtering by http status.
        The query can only use the main index, this tests the following
        scenario:
        - <b>Main Index (access)</b> (with Filter)

        This is then combined with a chart query to present the results, as
        presented in the following list. This test checks the highlighted
        scenario:
        - time chart (with split) + time chart postfix + query postfix
        - <b>time chart (no split) + time chart postfix + query postfix</b>
        - chart with split + split chart postfix + query postfix
        - total chart + query postfix
        """
        self.invoke_build_search_query(
            JsonKeys.METRIC_95TH_PERCENTILE_DELIVERED, "10days", "0days",
            "1day", use_filter=True)
        # Note tstat query will also have a split so that the filter works.
        self.check_query(
            '| tstats count as s sum(sc_bytes) as x where index="cdn_main" '
            'sourcetype=cdn_del_access earliest=03/16/2014:00:00:00 latest=03/26/2014:00:00:00 '
            'c_vx_zone="external" (host=4 OR host=5 OR host=2)  by _time span=5m, sc_status '
            '| lookup http_status_codes status AS sc_status OUTPUTNEW status_type, status_ok '
            '| search (status_type = "Successful 2xx") '
            '| timechart span=5m limit=12  sum(s) as s eval(sum(x)*8/300) as x   '
            '| append [gentimes start=03/16/2014:00:00:00 end=03/26/2014:00:00:00 increment=1d '
            '| rename starttime as _time | fields _time] '
            '| timechart perc95(x) as "95th percentile throughput (bps)" sum(s) as #samples span=1d '
            '| convert timeformat="%Y-%m-%dT%H:%M:%SZ" ctime(_time) | sort _time| rename _time as Date'
            '| fillnull | fields - _* NULL VALUE')

    def test_perc95_query_timechart_with_split_and_filter(self):
        """Test 95th percentile query splitting by time and http status, filtering by http status.
        The query can only use the main index, this tests the following
        scenario:
        - <b>Main Index (access)</b> (with Split and Filter)

        This is then combined with a chart query to present the results, as
        presented in the following list. This test checks the highlighted
        scenario:
        - <b>time chart (with split) + time chart postfix + query postfix</b>
        - time chart (no split) + time chart postfix + query postfix
        - chart with split + split chart postfix + query postfix
        - total chart + query postfix
        """
        self.invoke_build_search_query(
            JsonKeys.METRIC_95TH_PERCENTILE_DELIVERED, "10days", "0days",
            "1day", use_split=True, use_filter=True)
        self.check_query(
            '| tstats count as s sum(sc_bytes) as x where index="cdn_main" '
            'sourcetype=cdn_del_access earliest=03/16/2014:00:00:00 latest=03/26/2014:00:00:00 '
            'c_vx_zone="external" (host=4 OR host=5 OR host=2)  by _time span=5m, sc_status '
            '| lookup http_status_codes status AS sc_status OUTPUTNEW status_type, status_ok '
            '| search (status_type = "Successful 2xx") '
            '| timechart span=5m limit=12  sum(s) as s eval(sum(x)*8/300) as x by status_type '
            '| append [gentimes start=03/16/2014:00:00:00 end=03/26/2014:00:00:00 increment=1d '
            '| rename starttime as _time | fields _time] '
            '| timechart perc95(x: *) as * sum(s: *) as #samples:* span=1d '
            '| convert timeformat="%Y-%m-%dT%H:%M:%SZ" ctime(_time) | sort _time| rename _time as Date'
            '| fillnull | fields - _* NULL VALUE')

    def test_perc95_query_splitchart(self):
        """Test 95th percentile query splitting by http status only.
        The query can only use the main index, this tests the following
        scenario:
        - <b>Main Index (access)</b> (with Split)

        This is then combined with a chart query to present the results, as
        presented in the following list. This test checks the highlighted
        scenario:
        - time chart (with split) + time chart postfix + query postfix
        - time chart (no split) + time chart postfix + query postfix
        - <b>chart with split + split chart postfix + query postfix</b>
        - total chart + query postfix
        """
        self.invoke_build_search_query(
            JsonKeys.METRIC_95TH_PERCENTILE_DELIVERED, "10days", "0days",
            "noTimeSplit", use_split=True, split_limit=7)
        self.check_query(
            '| tstats count as s sum(sc_bytes) as x where index="cdn_main" '
            'sourcetype=cdn_del_access earliest=03/16/2014:00:00:00 latest=03/26/2014:00:00:00 '
            'c_vx_zone="external" (host=4 OR host=5 OR host=2)  by _time span=5m, sc_status '
            '| lookup http_status_codes status AS sc_status OUTPUTNEW status_type, status_ok '
            '| eventstats sum(s) as s sum(x) as x by _time, status_type'
            '| chart limit=7 sum(s) as s eval(perc95(x)*8/300) as x by status_type'
            '| sort - x | streamstats count as _row_number '
            '| eval status_type=if(0 = 7 or _row_number <= 7, status_type, "Other") '
            '| stats sum(*) as * by status_type | rename x as "95th percentile throughput (bps)", '
            's as #samples status_type as "HTTP Status Class"'
            '| fillnull | fields - _* NULL VALUE')

    def test_perc95_query_totalchart(self):
        """Test 95th percentile query without a split.
        The query can only use the main index, this tests the following
        scenario:
        - <b>Main Index (access)</b>

        This is then combined with a chart query to present the results, as
        presented in the following list. This test checks the highlighted
        scenario:
        - time chart (with split) + time chart postfix + query postfix
        - time chart (no split) + time chart postfix + query postfix
        - chart with split + split chart postfix + query postfix
        - <b>total chart + query postfix</b>
        """
        self.invoke_build_search_query(
            JsonKeys.METRIC_95TH_PERCENTILE_DELIVERED, "10days", "0days",
            "noTimeSplit", split_limit=8)
        self.check_query(
            '| tstats count as s sum(sc_bytes) as x where index="cdn_main" '
            'sourcetype=cdn_del_access earliest=03/16/2014:00:00:00 latest=03/26/2014:00:00:00 '
            'c_vx_zone="external" (host=4 OR host=5 OR host=2)  by _time span=5m '
            '| eventstats sum(s) as s sum(x) as x by _time'
            '| chart limit=8 sum(s) as #samples eval(perc95(x)*8/300) as "95th percentile throughput (bps)"'
            '| fillnull | fields - _* NULL VALUE')

    def test_summing_query_low_res_timechart_using_mi(self):
        """Test summing metric (request count) query splitting by time only.
        The query can be constructed to use different indexes depending on the
        time resolution and range selected as presented in the following list.
        This test checks the highlighted scenario:
        - <b>Main Index (access)</b>
        - Main Index (access) + Saved Search
        - Main Index (access) + Saved Search + Main Index (access)
        - Main Index (access) + Saved Search + Summary Index
        - Main Index (access) + Summary Index
        - Saved Search
        - Saved Search + Summary Index
        - Summary Index

        This is then combined with a chart query to present the results, as
        presented in the following list. This test checks the highlighted
        scenario:
        - time chart (with split) + time chart postfix + query postfix
        - <b>time chart (no split) + time chart postfix + query postfix</b>
        - chart with split + split chart postfix + query postfix
        - total chart + query postfix
        """
        self.invoke_build_search_query(
            JsonKeys.METRIC_REQUESTS_DELIVERED, "2hours", "0hours", "1day")
        self.check_query(
            '| tstats  count as x where index="cdn_main" sourcetype=cdn_del_access '
            'earliest=03/26/2014:16:00:00 latest=03/26/2014:18:00:00 c_vx_zone="external" '
            '(host=4 OR host=5 OR host=2)  by _time span=1d | eval x=x '
            '| append [gentimes start=03/26/2014:00:00:00 end=03/26/2014:18:00:00 increment=1d '
            '| rename starttime as _time | fields _time | eval x=0 ] '
            '| timechart span=1d limit=12 sum(x) as "Request count"   '
            '| convert timeformat="%Y-%m-%dT%H:%M:%SZ" ctime(_time) | sort _time| rename _time as Date'
            '| fillnull | fields - _* NULL VALUE')

    def test_summing_query_high_res_timechart_using_mi(self):
        """Test summing metric (bytes delivered) query splitting by high time resolution time only.
        The query can be constructed to use different indexes depending on the
        time resolution and range selected as presented in the following list.
        This test checks the highlighted scenario:
        - <b>Main Index (access)</b>
        - Main Index (access) + Saved Search
        - Main Index (access) + Saved Search + Main Index (access)
        - Main Index (access) + Saved Search + Summary Index
        - Main Index (access) + Summary Index
        - Saved Search
        - Saved Search + Summary Index
        - Summary Index

        This is then combined with a chart query to present the results, as
        presented in the following list. This test checks the highlighted
        scenario:
        - time chart (with split) + time chart postfix + query postfix
        - <b>time chart (no split) + time chart postfix + query postfix</b>
        - chart with split + split chart postfix + query postfix
        - total chart + query postfix
        """
        self.invoke_build_search_query(
            JsonKeys.METRIC_BYTES_DELIVERED, "5days", "0days", "30minutes")
        self.check_query(
            '| tstats  sum(sc_bytes) as x where index="cdn_main" sourcetype=cdn_del_access '
            'earliest=03/21/2014:00:00:00 latest=03/26/2014:00:00:00 c_vx_zone="external" '
            '(host=4 OR host=5 OR host=2)  by _time span=30min | eval x=x '
            '| append [gentimes start=03/21/2014:00:00:00 end=03/26/2014:00:00:00 increment=30min '
            '| rename starttime as _time | fields _time | eval x=0 ] '
            '| timechart span=30min limit=12 sum(x) as "Bytes delivered"   '
            '| convert timeformat="%Y-%m-%dT%H:%M:%SZ" ctime(_time) | sort _time| rename _time as Time'
            '| fillnull | fields - _* NULL VALUE')

    def test_summing_query_low_res_timechart_using_mi_ss(self):
        """Test summing metric (request count) query splitting by time only.
        The query can be constructed to use different indexes depending on the
        time resolution and range selected as presented in the following list.
        This test checks the highlighted scenario:
        - Main Index (access)
        - <b>Main Index (access) + Saved Search</b>
        - Main Index (access) + Saved Search + Main Index (access)
        - Main Index (access) + Saved Search + Summary Index
        - Main Index (access) + Summary Index
        - Saved Search
        - Saved Search + Summary Index
        - Summary Index

        This is then combined with a chart query to present the results, as
        presented in the following list. This test checks the highlighted
        scenario:
        - time chart (with split) + time chart postfix + query postfix
        - <b>time chart (no split) + time chart postfix + query postfix</b>
        - chart with split + split chart postfix + query postfix
        - total chart + query postfix
        """
        self.invoke_build_search_query(JsonKeys.METRIC_REQUESTS_DELIVERED, "5days", "0days", "1day")
        self.check_query('| loadjob savedsearch="admin:cdn_reporting:Saved Search By Hour 7 Days Ago" | search _time>=1395532800 _time<1395792000 c_vx_zone="external" (host=4 OR host=5 OR host=2)  | stats sum(requests) as x by _time  | eval x=x '
                         '| append [ search index=cdn_summary c_vx_zone="external" (orig_host=4 OR orig_host=5 OR orig_host=2)  earliest=03/21/2014:00:00:00 latest=03/23/2014:00:00:00 | stats sum(requests) as x by _time  | eval x=x | rename orig_host as host  ]'
                         '| append [gentimes start=03/21/2014:00:00:00 end=03/26/2014:00:00:00 increment=1d | rename starttime as _time | fields _time | eval x=0 ] '
                         '| timechart span=1d limit=12 sum(x) as "Request count"   | convert timeformat="%Y-%m-%dT%H:%M:%SZ" ctime(_time) | sort _time| rename _time as Date'
                         '| fillnull | fields - _* NULL VALUE')

    def test_distinct_count_query_low_res_timechart_using_mi(self):
        """Test unique ip address metric query splitting by time only.
        The query can only be constructed using the main index as the IP
        addresses are not available in the saved search or summary index.
        This test checks the highlighted scenario:
        - <b>Main Index (access)</b>
        - Main Index (access) + Saved Search
        - Main Index (access) + Saved Search + Main Index (access)
        - Main Index (access) + Saved Search + Summary Index
        - Main Index (access) + Summary Index
        - Saved Search
        - Saved Search + Summary Index
        - Summary Index

        This is then combined with a chart query to present the results, as
        presented in the following list. This test checks the highlighted
        scenario:
        - time chart (with split) + time chart postfix + query postfix
        - <b>time chart (no split) + time chart postfix + query postfix</b>
        - chart with split + split chart postfix + query postfix
        - total chart + query postfix
        """
        self.invoke_build_search_query(
            JsonKeys.METRIC_UNIQUE_IP_ADDRESS, "5days", "0days", "1day")
        self.check_query(
            '| tstats  values(c_ip) as x where index="cdn_main" sourcetype=cdn_del_access '
            'earliest=03/21/2014:00:00:00 latest=03/26/2014:00:00:00 c_vx_zone="external" '
            '(host=4 OR host=5 OR host=2)  by _time span=1d | eval x=x '
            '| append [gentimes start=03/21/2014:00:00:00 end=03/26/2014:00:00:00 increment=1d '
            '| rename starttime as _time | fields _time | eval x=NULL ] '
            '| timechart span=1d limit=12 dc(x) as "Unique IP address"   '
            '| convert timeformat="%Y-%m-%dT%H:%M:%SZ" ctime(_time) | sort _time| rename _time as Date'
            '| fillnull | fields - _* NULL VALUE')

    def test_distinct_count_query_high_res_timechart_using_mi(self):
        """Test unique ip address metric query splitting by high time resolution time only.
        The query can only be constructed using the main index as the IP
        addresses are not available in the saved search or summary index.
        This test checks the highlighted scenario:
        - <b>Main Index (access)</b>
        - Main Index (access) + Saved Search
        - Main Index (access) + Saved Search + Main Index (access)
        - Main Index (access) + Saved Search + Summary Index
        - Main Index (access) + Summary Index
        - Saved Search
        - Saved Search + Summary Index
        - Summary Index

        This is then combined with a chart query to present the results, as
        presented in the following list. This test checks the highlighted
        scenario:
        - time chart (with split) + time chart postfix + query postfix
        - <b>time chart (no split) + time chart postfix + query postfix</b>
        - chart with split + split chart postfix + query postfix
        - total chart + query postfix
        """
        self.invoke_build_search_query(
            JsonKeys.METRIC_UNIQUE_IP_ADDRESS, "5days", "0days", "30minutes")
        self.check_query(
            '| tstats  values(c_ip) as x where index="cdn_main" sourcetype=cdn_del_access '
            'earliest=03/21/2014:00:00:00 latest=03/26/2014:00:00:00 c_vx_zone="external" '
            '(host=4 OR host=5 OR host=2)  by _time span=30min | eval x=x '
            '| append [gentimes start=03/21/2014:00:00:00 end=03/26/2014:00:00:00 '
            'increment=30min | rename starttime as _time | fields _time | eval x=NULL ] '
            '| timechart span=30min limit=12 dc(x) as "Unique IP address"   '
            '| convert timeformat="%Y-%m-%dT%H:%M:%SZ" ctime(_time) | sort _time| rename _time as Time'
            '| fillnull | fields - _* NULL VALUE')

    def test_summing_query_medium_res_timechart_using_mi_ss_mi(self):
        """Test summing metric (bytes delivered) query splitting by medium time resolution time only.
        The query can be constructed to use different indexes depending on the
        time resolution and range selected as presented in the following list.
        This test checks the highlighted scenario:
        - Main Index (access)
        - Main Index (access) + Saved Search
        - <b>Main Index (access) + Saved Search + Main Index (access)</b>
        - Main Index (access) + Saved Search + Summary Index
        - Main Index (access) + Summary Index
        - Saved Search
        - Saved Search + Summary Index
        - Summary Index

        This is then combined with a chart query to present the results, as
        presented in the following list. This test checks the highlighted
        scenario:
        - time chart (with split) + time chart postfix + query postfix
        - <b>time chart (no split) + time chart postfix + query postfix</b>
        - chart with split + split chart postfix + query postfix
        - total chart + query postfix
        """
        self.invoke_build_search_query(
            JsonKeys.METRIC_BYTES_DELIVERED, "8days", "now", "1hour")
        self.check_query(
            '| tstats  sum(sc_bytes) as x where index="cdn_main" sourcetype=cdn_del_access '
            'earliest=03/26/2014:16:00:00 latest=03/26/2014:18:52:45 c_vx_zone="external" '
            '(host=4 OR host=5 OR host=2)  by _time span=1h | eval x=x '
            '| append [ loadjob savedsearch="admin:cdn_reporting:Saved Search By Hour 7 Days Ago" '
            '| search _time>=1395187200 _time<1395849600 c_vx_zone="external" '
            '(host=4 OR host=5 OR host=2)  | stats sum(sc_bytes) as x by _time  '
            '| eval x=x  ]'
            '| append [ tstats  sum(sc_bytes) as x where index="cdn_main" '
            'sourcetype=cdn_del_access earliest=03/18/2014:00:00:00 latest=03/19/2014:00:00:00 '
            'c_vx_zone="external" (host=4 OR host=5 OR host=2)  by _time span=1h  ]| eval x=x '
            '| append [gentimes start=03/18/2014:00:00:00 end=03/26/2014:18:52:45 increment=1h '
            '| rename starttime as _time | fields _time | eval x=0 ] '
            '| timechart span=1h limit=12 sum(x) as "Bytes delivered"   '
            '| convert timeformat="%Y-%m-%dT%H:%M:%SZ" ctime(_time) | sort _time| rename _time as Time'
            '| fillnull | fields - _* NULL VALUE')

    def test_summing_query_low_res_timechart_using_mi_ss_si(self):
        """Test summing metric (request count) query splitting by time only.
        The query can be constructed to use different indexes depending on the
        time resolution and range selected as presented in the following list.
        This test checks the highlighted scenario:
        - Main Index (access)
        - Main Index (access) + Saved Search
        - Main Index (access) + Saved Search + Main Index (access)
        - <b>Main Index (access) + Saved Search + Summary Index</b>
        - Main Index (access) + Summary Index
        - Saved Search
        - Saved Search + Summary Index
        - Summary Index

        This is then combined with a chart query to present the results, as
        presented in the following list. This test checks the highlighted
        scenario:
        - time chart (with split) + time chart postfix + query postfix
        - <b>time chart (no split) + time chart postfix + query postfix</b>
        - chart with split + split chart postfix + query postfix
        - total chart + query postfix
        """
        self.invoke_build_search_query(
            JsonKeys.METRIC_REQUESTS_DELIVERED, "10days", "now", "1day")
        self.check_query(
            '| tstats  count as x where index="cdn_main" sourcetype=cdn_del_access '
            'earliest=03/26/2014:16:00:00 latest=03/26/2014:18:52:45 c_vx_zone="external" '
            '(host=4 OR host=5 OR host=2)  by _time span=1d | eval x=x '
            '| append [ loadjob savedsearch="admin:cdn_reporting:Saved Search By Hour 7 Days Ago" '
            '| search _time>=1395532800 _time<1395849600 c_vx_zone="external" '
            '(host=4 OR host=5 OR host=2)  | stats sum(requests) as x by _time  | eval x=x  ]'
            '| append [ search index=cdn_summary c_vx_zone="external" '
            '(orig_host=4 OR orig_host=5 OR orig_host=2)  earliest=03/16/2014:00:00:00 '
            'latest=03/23/2014:00:00:00 | stats sum(requests) as x by _time  | eval x=x '
            '| rename orig_host as host  ]'
            '| append [gentimes start=03/16/2014:00:00:00 end=03/26/2014:18:52:45 increment=1d '
            '| rename starttime as _time | fields _time | eval x=0 ] '
            '| timechart span=1d limit=12 sum(x) as "Request count"   '
            '| convert timeformat="%Y-%m-%dT%H:%M:%SZ" ctime(_time) | sort _time| rename _time as Date'
            '| fillnull | fields - _* NULL VALUE')

    def test_summing_query_medium_res_timechart_using_ss(self):
        """Test summing metric (bytes delivered) query splitting by medium time resolution time only.
        The query can be constructed to use different indexes depending on the
        time resolution and range selected as presented in the following list.
        This test checks the highlighted scenario:
        - Main Index (access)
        - Main Index (access) + Saved Search
        - Main Index (access) + Saved Search + Main Index (access)
        - Main Index (access) + Saved Search + Summary Index
        - Main Index (access) + Summary Index
        - <b>Saved Search</b>
        - Saved Search + Summary Index
        - Summary Index

        This is then combined with a chart query to present the results, as
        presented in the following list. This test checks the highlighted
        scenario:
        - time chart (with split) + time chart postfix + query postfix
        - <b>time chart (no split) + time chart postfix + query postfix</b>
        - chart with split + split chart postfix + query postfix
        - total chart + query postfix
        """
        self.invoke_build_search_query(JsonKeys.METRIC_BYTES_DELIVERED, "7days", "0day", "1hour")
        self.check_query('| loadjob savedsearch="admin:cdn_reporting:Saved Search By Hour 7 Days Ago" | search _time>=1395187200 _time<1395792000 c_vx_zone="external" (host=4 OR host=5 OR host=2)  | stats sum(sc_bytes) as x by _time  | eval x=x '
                         '| append [gentimes start=03/19/2014:00:00:00 end=03/26/2014:00:00:00 increment=1h | rename starttime as _time | fields _time | eval x=0 ] '
                         '| timechart span=1h limit=12 sum(x) as "Bytes delivered"   | convert timeformat="%Y-%m-%dT%H:%M:%SZ" ctime(_time) | sort _time| rename _time as Time'
                         '| fillnull | fields - _* NULL VALUE')

    def test_summing_query_low_res_timechart_using_ss_si(self):
        """Test summing metric (bytes delivered) query splitting by low time resolution time only.
        The query can be constructed to use different indexes depending on the
        time resolution and range selected as presented in the following list.
        This test checks the highlighted scenario:
        - Main Index (access)
        - Main Index (access) + Saved Search
        - Main Index (access) + Saved Search + Main Index (access)
        - Main Index (access) + Saved Search + Summary Index
        - Main Index (access) + Summary Index
        - Saved Search
        - <b>Saved Search + Summary Index</b>
        - Summary Index

        This is then combined with a chart query to present the results, as
        presented in the following list. This test checks the highlighted
        scenario:
        - time chart (with split) + time chart postfix + query postfix
        - <b>time chart (no split) + time chart postfix + query postfix</b>
        - chart with split + split chart postfix + query postfix
        - total chart + query postfix
        """
        self.invoke_build_search_query(JsonKeys.METRIC_BYTES_DELIVERED, "10days", "0days", "1day")
        self.check_query('| loadjob savedsearch="admin:cdn_reporting:Saved Search By Hour 7 Days Ago" | search _time>=1395532800 _time<1395792000 c_vx_zone="external" (host=4 OR host=5 OR host=2)  | stats sum(sc_bytes) as x by _time  | eval x=x '
                         '| append [ search index=cdn_summary c_vx_zone="external" (orig_host=4 OR orig_host=5 OR orig_host=2)  earliest=03/16/2014:00:00:00 latest=03/23/2014:00:00:00 | stats sum(sc_bytes) as x by _time  | eval x=x | rename orig_host as host  ]'
                         '| append [gentimes start=03/16/2014:00:00:00 end=03/26/2014:00:00:00 increment=1d | rename starttime as _time | fields _time | eval x=0 ] '
                         '| timechart span=1d limit=12 sum(x) as "Bytes delivered"   | convert timeformat="%Y-%m-%dT%H:%M:%SZ" ctime(_time) | sort _time| rename _time as Date'
                         '| fillnull | fields - _* NULL VALUE')

    def test_summing_query_low_res_timechart_using_si(self):
        """Test summing metric (bytes delivered) query splitting by medium time resolution time only.
        The query can be constructed to use different indexes depending on the
        time resolution and range selected as presented in the following list.
        This test checks the highlighted scenario:
        - Main Index (access)
        - Main Index (access) + Saved Search
        - Main Index (access) + Saved Search + Main Index (access)
        - Main Index (access) + Saved Search + Summary Index
        - Main Index (access) + Summary Index
        - Saved Search
        - Saved Search + Summary Index
        - <b>Summary Index</b>

        This is then combined with a chart query to present the results, as
        presented in the following list. This test checks the highlighted
        scenario:
        - time chart (with split) + time chart postfix + query postfix
        - <b>time chart (no split) + time chart postfix + query postfix</b>
        - chart with split + split chart postfix + query postfix
        - total chart + query postfix
        """
        self.invoke_build_search_query(JsonKeys.METRIC_BYTES_DELIVERED, "14days", "7days", "1day")
        self.check_query('search index=cdn_summary c_vx_zone="external" (orig_host=4 OR orig_host=5 OR orig_host=2)  earliest=03/12/2014:00:00:00 latest=03/19/2014:00:00:00 | stats sum(sc_bytes) as x by _time  | eval x=x | rename orig_host as host '
                         '| append [gentimes start=03/12/2014:00:00:00 end=03/19/2014:00:00:00 increment=1d | rename starttime as _time | fields _time | eval x=0 ] '
                         '| timechart span=1d limit=12 sum(x) as "Bytes delivered"   | convert timeformat="%Y-%m-%dT%H:%M:%SZ" ctime(_time) | sort _time| rename _time as Date'
                         '| fillnull | fields - _* NULL VALUE')

    def test_summing_query_low_res_timechart_with_split_using_mi_ss_si(self):
        """Test summing metric (request count) query splitting by time and http status.
        The query can be constructed to use different indexes depending on the
        time resolution and range selected as presented in the following list.
        This test checks the highlighted scenario:
        - Main Index (access)
        - Main Index (access) + Saved Search
        - Main Index (access) + Saved Search + Main Index (access)
        - <b>Main Index (access) + Saved Search + Summary Index</b> (with Split)
        - Main Index (access) + Summary Index
        - Saved Search
        - Saved Search + Summary Index
        - Summary Index

        This is then combined with a chart query to present the results, as
        presented in the following list. This test checks the highlighted
        scenario:
        - <b>time chart (with split) + time chart postfix + query postfix</b>
        - time chart (no split) + time chart postfix + query postfix
        - chart with split + split chart postfix + query postfix
        - total chart + query postfix
        """
        self.invoke_build_search_query(
            JsonKeys.METRIC_REQUESTS_DELIVERED, "10days", "now", "1day", use_split=True)
        self.check_query(
            '| tstats  count as x where index="cdn_main" sourcetype=cdn_del_access '
            'earliest=03/26/2014:16:00:00 latest=03/26/2014:18:52:45 c_vx_zone="external" '
            '(host=4 OR host=5 OR host=2)  by _time span=1d, sc_status | eval x=x '
            '| lookup http_status_codes status AS sc_status OUTPUTNEW status_type, status_ok '
            '| append [ loadjob savedsearch="admin:cdn_reporting:Saved Search By Hour 7 Days Ago" '
            '| search _time>=1395532800 _time<1395849600 c_vx_zone="external" (host=4 OR host=5 OR host=2)  '
            '| stats sum(requests) as x by _time status_type | eval x=x  ]'
            '| append [ search index=cdn_summary c_vx_zone="external" '
            '(orig_host=4 OR orig_host=5 OR orig_host=2)  '
            'earliest=03/16/2014:00:00:00 latest=03/23/2014:00:00:00 '
            '| stats sum(requests) as x by _time status_type | eval x=x | rename orig_host as host  ]'
            '| append [gentimes start=03/16/2014:00:00:00 end=03/26/2014:18:52:45 increment=1d '
            '| rename starttime as _time | fields _time | eval x=0 | eval status_type=NULL] '
            '| timechart span=1d limit=12 sum(x) as "Request count" by status_type '
            '| convert timeformat="%Y-%m-%dT%H:%M:%SZ" ctime(_time) | sort _time| rename _time as Date'
            '| fillnull | fields - _* NULL VALUE')

    def test_summing_query_low_res_timechart_with_split_using_mi_si_no_saved_search(self):
        """Test summing metric (request count) query splitting by time and http status.
        The query can be constructed to use different indexes depending on the
        time resolution and range selected as presented in the following list.
        This test checks the highlighted scenario:
        - Main Index (access)
        - Main Index (access) + Saved Search
        - Main Index (access) + Saved Search + Main Index (access)
        - Main Index (access) + Saved Search + Summary Index (with Split)
        - <b>Main Index (access) + Summary Index</b>
        - Saved Search
        - Saved Search + Summary Index
        - Summary Index

        The Saved Search is set to be unavailable so the main index start time
        will be changed accordingly.

        This is then combined with a chart query to present the results, as
        presented in the following list. This test checks the highlighted
        scenario:
        - <b>time chart (with split) + time chart postfix + query postfix</b>
        - time chart (no split) + time chart postfix + query postfix
        - chart with split + split chart postfix + query postfix
        - total chart + query postfix
        """
        self.invoke_build_search_query_no_saved_search(
            JsonKeys.METRIC_REQUESTS_DELIVERED, "10days", "now", "1day", True)
        self.check_query(
            '| tstats  count as x where index="cdn_main" sourcetype=cdn_del_access '
            'earliest=03/23/2014:00:00:00 latest=03/26/2014:18:52:45 c_vx_zone="external" '
            '(host=4 OR host=5 OR host=2)  by _time span=1d, sc_status | eval x=x '
            '| lookup http_status_codes status AS sc_status OUTPUTNEW status_type, status_ok '
            '| append [ search index=cdn_summary c_vx_zone="external" '
            '(orig_host=4 OR orig_host=5 OR orig_host=2)  '
            'earliest=03/16/2014:00:00:00 latest=03/23/2014:00:00:00 '
            '| stats sum(requests) as x by _time status_type | eval x=x | rename orig_host as host  ]'
            '| append [gentimes start=03/16/2014:00:00:00 end=03/26/2014:18:52:45 increment=1d '
            '| rename starttime as _time | fields _time | eval x=0 | eval status_type=NULL] '
            '| timechart span=1d limit=12 sum(x) as "Request count" by status_type '
            '| convert timeformat="%Y-%m-%dT%H:%M:%SZ" ctime(_time) | sort _time| rename _time as Date'
            '| fillnull | fields - _* NULL VALUE')

    def test_summing_query_medium_res_timechart_with_split_and_filter_using_mi_ss_mi(self):
        """Test summing metric (bytes delivered) query splitting by time and http status, filtering by http status.
        The query can be constructed to use different indexes depending on the
        time resolution and range selected as presented in the following list.
        This test checks the highlighted scenario:
        - Main Index (access)
        - Main Index (access) + Saved Search
        - <b>Main Index (access) + Saved Search + Main Index (access)</b> (with Split and Filter)
        - Main Index (access) + Saved Search + Summary Index
        - Main Index (access) + Summary Index
        - Saved Search
        - Saved Search + Summary Index
        - Summary Index

        This is then combined with a chart query to present the results, as
        presented in the following list. This test checks the highlighted
        scenario:
        - <b>time chart (with split) + time chart postfix + query postfix</b>
        - time chart (no split) + time chart postfix + query postfix
        - chart with split + split chart postfix + query postfix
        - total chart + query postfix
        """
        self.invoke_build_search_query(
            JsonKeys.METRIC_BYTES_DELIVERED, "10days", "now", "1hour",
            use_split=True, use_filter=True, split_limit=9)
        self.check_query(
            '| tstats  sum(sc_bytes) as x where index="cdn_main" sourcetype=cdn_del_access '
            'earliest=03/26/2014:16:00:00 latest=03/26/2014:18:52:45 c_vx_zone="external" '
            '(host=4 OR host=5 OR host=2)  by _time span=1h, sc_status '
            '| eval x=x '
            '| lookup http_status_codes status AS sc_status OUTPUTNEW status_type, status_ok '
            '| search (status_type = "Successful 2xx") '
            '| append [ loadjob savedsearch="admin:cdn_reporting:Saved Search By Hour 7 Days Ago" '
            '| search _time>=1395187200 _time<1395849600 c_vx_zone="external" '
            '(host=4 OR host=5 OR host=2)  (status_type = "Successful 2xx")  '
            '| stats sum(sc_bytes) as x by _time status_type | eval x=x  ]'
            '| append [ tstats  sum(sc_bytes) as x where index="cdn_main" sourcetype=cdn_del_access '
            'earliest=03/16/2014:00:00:00 latest=03/19/2014:00:00:00 c_vx_zone="external" '
            '(host=4 OR host=5 OR host=2)  by _time span=1h, sc_status  ]'
            '| eval x=x | lookup http_status_codes status AS sc_status OUTPUTNEW status_type, status_ok '
            '| search (status_type = "Successful 2xx") '
            '| append [gentimes start=03/16/2014:00:00:00 end=03/26/2014:18:52:45 increment=1h '
            '| rename starttime as _time | fields _time | eval x=0 | eval status_type=NULL] '
            '| timechart span=1h limit=9 sum(x) as "Bytes delivered" by status_type '
            '| convert timeformat="%Y-%m-%dT%H:%M:%SZ" ctime(_time) | sort _time| rename _time as Time'
            '| fillnull | fields - _* NULL VALUE')

    def test_summing_query_low_res_timechart_with_split_without_split_limit_using_mi_ss_si(self):
        """Test summing metric (request count) query splitting by time and http status and a zero split limit.
        The query can be constructed to use different indexes depending on the
        time resolution and range selected as presented in the following list.
        This test checks the highlighted scenario:
        - Main Index (access)
        - <b>Main Index (access) + Saved Search</b> (with Split)
        - Main Index (access) + Saved Search + Main Index (access)
        - Main Index (access) + Saved Search + Summary Index
        - Main Index (access) + Summary Index
        - Saved Search
        - Saved Search + Summary Index
        - Summary Index

        This is then combined with a chart query to present the results, as
        presented in the following list. This test checks the highlighted
        scenario:
        - <b>time chart (with split) + time chart postfix + query postfix</b>
        - time chart (no split) + time chart postfix + query postfix
        - chart with split + split chart postfix + query postfix
        - total chart + query postfix
        """
        self.invoke_build_search_query(JsonKeys.METRIC_REQUESTS_DELIVERED, "10days", "0days", "1day", use_split=True, split_limit=0)
        self.check_query('| loadjob savedsearch="admin:cdn_reporting:Saved Search By Hour 7 Days Ago" | search _time>=1395532800 _time<1395792000 c_vx_zone="external" (host=4 OR host=5 OR host=2)  | stats sum(requests) as x by _time status_type | eval x=x '
                         '| append [ search index=cdn_summary c_vx_zone="external" (orig_host=4 OR orig_host=5 OR orig_host=2)  earliest=03/16/2014:00:00:00 latest=03/23/2014:00:00:00 | stats sum(requests) as x by _time status_type | eval x=x | rename orig_host as host  ]'
                         '| append [gentimes start=03/16/2014:00:00:00 end=03/26/2014:00:00:00 increment=1d | rename starttime as _time | fields _time | eval x=0 | eval status_type=NULL] '
                         '| timechart span=1d limit=0 sum(x) as "Request count" by status_type | convert timeformat="%Y-%m-%dT%H:%M:%SZ" ctime(_time) | sort _time| rename _time as Date'
                         '| fillnull | fields - _* NULL VALUE')

    def test_summing_query_low_res_timechart_with_filter_using_mi_ss_si(self):
        """Test summing metric (request count) query splitting by time only, filtering by http status.
        The query can be constructed to use different indexes depending on the
        time resolution and range selected as presented in the following list.
        This test checks the highlighted scenario:
        - Main Index (access)
        - Main Index (access) + Saved Search
        - Main Index (access) + Saved Search + Main Index (access)
        - <b>Main Index (access) + Saved Search + Summary Index</b> (with Filter)
        - Main Index (access) + Summary Index
        - Saved Search
        - Saved Search + Summary Index
        - Summary Index

        This is then combined with a chart query to present the results, as
        presented in the following list. This test checks the highlighted
        scenario:
        - <b>time chart (with split) + time chart postfix + query postfix</b>
        - time chart (no split) + time chart postfix + query postfix
        - chart with split + split chart postfix + query postfix
        - total chart + query postfix
        """
        self.invoke_build_search_query(
            JsonKeys.METRIC_REQUESTS_DELIVERED, "10days", "now", "1day",
            use_filter=True)
        self.check_query(
            '| tstats  count as x where index="cdn_main" sourcetype=cdn_del_access '
            'earliest=03/26/2014:16:00:00 latest=03/26/2014:18:52:45 c_vx_zone="external" '
            '(host=4 OR host=5 OR host=2)  by _time span=1d, sc_status | eval x=x '
            '| lookup http_status_codes status AS sc_status OUTPUTNEW status_type, status_ok '
            '| search (status_type = "Successful 2xx") '
            '| append [ loadjob savedsearch="admin:cdn_reporting:Saved Search By Hour 7 Days Ago" '
            '| search _time>=1395532800 _time<1395849600 c_vx_zone="external" (host=4 OR host=5 OR host=2)  '
            '(status_type = "Successful 2xx")  | stats sum(requests) as x by _time  | eval x=x  ]'
            '| append [ search index=cdn_summary c_vx_zone="external" '
            '(orig_host=4 OR orig_host=5 OR orig_host=2)  (status_type = "Successful 2xx")  '
            'earliest=03/16/2014:00:00:00 latest=03/23/2014:00:00:00 | stats sum(requests) as x by _time  '
            '| eval x=x | rename orig_host as host  ]'
            '| append [gentimes start=03/16/2014:00:00:00 end=03/26/2014:18:52:45 increment=1d '
            '| rename starttime as _time | fields _time | eval x=0 ] '
            '| timechart span=1d limit=12 sum(x) as "Request count"   '
            '| convert timeformat="%Y-%m-%dT%H:%M:%SZ" ctime(_time) | sort _time| rename _time as Date'
            '| fillnull | fields - _* NULL VALUE')

    def test_summing_query_low_res_timechart_with_split_and_filter_using_mi_ss_si(self):
        """Test summing metric (bytes delivered) query splitting by time and http status, filtering by http status.
        The query can be constructed to use different indexes depending on the
        time resolution and range selected as presented in the following list.
        This test checks the highlighted scenario:
        - Main Index (access)
        - Main Index (access) + Saved Search
        - Main Index (access) + Saved Search + Main Index (access)
        - <b>Main Index (access) + Saved Search + Summary Index</b> (with Split and Filter)
        - Main Index (access) + Summary Index
        - Saved Search
        - Saved Search + Summary Index
        - Summary Index

        This is then combined with a chart query to present the results, as
        presented in the following list. This test checks the highlighted
        scenario:
        - <b>time chart (with split) + time chart postfix + query postfix</b>
        - time chart (no split) + time chart postfix + query postfix
        - chart with split + split chart postfix + query postfix
        - total chart + query postfix
        """
        self.invoke_build_search_query(
            JsonKeys.METRIC_BYTES_DELIVERED, "10days", "now", "1day",
            use_split=True, use_filter=True, split_limit=9)
        self.check_query(
            '| tstats  sum(sc_bytes) as x where index="cdn_main" sourcetype=cdn_del_access '
            'earliest=03/26/2014:16:00:00 latest=03/26/2014:18:52:45 c_vx_zone="external" '
            '(host=4 OR host=5 OR host=2)  by _time span=1d, sc_status | eval x=x '
            '| lookup http_status_codes status AS sc_status OUTPUTNEW status_type, status_ok '
            '| search (status_type = "Successful 2xx") '
            '| append [ loadjob savedsearch="admin:cdn_reporting:Saved Search By Hour 7 Days Ago" '
            '| search _time>=1395532800 _time<1395849600 c_vx_zone="external" (host=4 OR host=5 OR host=2)  '
            '(status_type = "Successful 2xx")  | stats sum(sc_bytes) as x by _time status_type | eval x=x  ]'
            '| append [ search index=cdn_summary c_vx_zone="external" '
            '(orig_host=4 OR orig_host=5 OR orig_host=2)  (status_type = "Successful 2xx")  '
            'earliest=03/16/2014:00:00:00 latest=03/23/2014:00:00:00 '
            '| stats sum(sc_bytes) as x by _time status_type | eval x=x | rename orig_host as host  ]'
            '| append [gentimes start=03/16/2014:00:00:00 end=03/26/2014:18:52:45 increment=1d '
            '| rename starttime as _time | fields _time | eval x=0 | eval status_type=NULL] '
            '| timechart span=1d limit=9 sum(x) as "Bytes delivered" by status_type '
            '| convert timeformat="%Y-%m-%dT%H:%M:%SZ" ctime(_time) | sort _time| rename _time as Date'
            '| fillnull | fields - _* NULL VALUE')

    def test_summing_query_splitchart_using_mi_ss_si(self):
        """Test summing metric (bytes delivered) query splitting by http status only.
        The query can be constructed to use different indexes depending on the
        time resolution and range selected as presented in the following list.
        This test checks the highlighted scenario:
        - Main Index (access)
        - Main Index (access) + Saved Search
        - Main Index (access) + Saved Search + Main Index (access)
        - <b>Main Index (access) + Saved Search + Summary Index</b> (with Split)
        - Main Index (access) + Summary Index
        - Saved Search
        - Saved Search + Summary Index
        - Summary Index

        This is then combined with a chart query to present the results, as
        presented in the following list. This test checks the highlighted
        scenario:
        - time chart (with split) + time chart postfix + query postfix
        - time chart (no split) + time chart postfix + query postfix
        - <b>chart with split + split chart postfix + query postfix</b>
        - total chart + query postfix
        """
        self.invoke_build_search_query(
            JsonKeys.METRIC_BYTES_DELIVERED, "10days", "now", "noTimeSplit",
            use_split=True, split_limit=50)
        self.check_query(
            '| tstats  sum(sc_bytes) as x where index="cdn_main" sourcetype=cdn_del_access '
            'earliest=03/26/2014:16:00:00 latest=03/26/2014:18:52:45 c_vx_zone="external" '
            '(host=4 OR host=5 OR host=2)  by _time span=1d, sc_status | eval x=x '
            '| lookup http_status_codes status AS sc_status OUTPUTNEW status_type, status_ok '
            '| append [ loadjob savedsearch="admin:cdn_reporting:Saved Search By Hour 7 Days Ago" '
            '| search _time>=1395532800 _time<1395849600 c_vx_zone="external" '
            '(host=4 OR host=5 OR host=2)  | stats sum(sc_bytes) as x by _time status_type | eval x=x  ]'
            '| append [ search index=cdn_summary c_vx_zone="external" '
            '(orig_host=4 OR orig_host=5 OR orig_host=2)  earliest=03/16/2014:00:00:00 '
            'latest=03/23/2014:00:00:00 | stats sum(sc_bytes) as x by _time status_type '
            '| eval x=x | rename orig_host as host  ]'
            '| chart limit=50 sum(x) as x by status_type '
            '| where isnotnull(status_type) and len(tostring(status_type))>0 '
            '| sort - x | streamstats count as _row_number '
            '| eval status_type=if(0 = 50 or _row_number <= 50, status_type, "Other") '
            '| stats sum(*) as * by status_type '
            '| rename x as "Bytes delivered", s as #samples status_type as "HTTP Status Class"'
            '| fillnull | fields - _* NULL VALUE')

    def test_summing_query_totalchart_using_mi_ss_si(self):
        """Test summing metric (request count) query without a split.
        The query can be constructed to use different indexes depending on the
        time resolution and range selected as presented in the following list.
        This test checks the highlighted scenario:
        - Main Index (access)
        - Main Index (access) + Saved Search
        - Main Index (access) + Saved Search + Main Index (access)
        - <b>Main Index (access) + Saved Search + Summary Index</b> (with Split)
        - Main Index (access) + Summary Index
        - Saved Search
        - Saved Search + Summary Index
        - Summary Index

        This is then combined with a chart query to present the results, as
        presented in the following list. This test checks the highlighted
        scenario:
        - time chart (with split) + time chart postfix + query postfix
        - time chart (no split) + time chart postfix + query postfix
        - chart with split + split chart postfix + query postfix
        - <b>total chart + query postfix</b>
        """
        self.invoke_build_search_query(
            JsonKeys.METRIC_REQUESTS_DELIVERED, "10days", "now", "noTimeSplit",
            split_limit=60)
        self.check_query(
            '| tstats  count as x where index="cdn_main" sourcetype=cdn_del_access '
            'earliest=03/26/2014:16:00:00 latest=03/26/2014:18:52:45 c_vx_zone="external" '
            '(host=4 OR host=5 OR host=2)  by _time span=1d | eval x=x '
            '| append [ loadjob savedsearch="admin:cdn_reporting:Saved Search By Hour 7 Days Ago" '
            '| search _time>=1395532800 _time<1395849600 c_vx_zone="external" '
            '(host=4 OR host=5 OR host=2)  | stats sum(requests) as x by _time  | eval x=x  ]'
            '| append [ search index=cdn_summary c_vx_zone="external" '
            '(orig_host=4 OR orig_host=5 OR orig_host=2)  earliest=03/16/2014:00:00:00 '
            'latest=03/23/2014:00:00:00 | stats sum(requests) as x by _time  '
            '| eval x=x | rename orig_host as host  ]'
            '| chart limit=60 sum(x) as "Request count"'
            '| fillnull | fields - _* NULL VALUE')

    def test_peak_unique_ip_address_timechart(self):
        """ Test peak unique IP addresses query splitting by time only.
        The query can only use the main index, this tests the following
        scenario:
        - <b>Main Index (access)</b>

        This is then combined with a chart query to present the results, as
        presented in the following list. This test checks the highlighted
        scenario:
        - <b>time chart (with split) + time chart postfix + query postfix</b>
        - time chart (no split) + time chart postfix + query postfix
        """
        self.invoke_build_search_query(JsonKeys.METRIC_PEAK_UNIQUE_CLIENT_IP_ADDRESS, "2days", "0days", "1hour")
        self.check_query(
            '| tstats dc(c_ip) as x where index="cdn_main" '
            'sourcetype=cdn_del_access earliest=03/24/2014:00:00:00 latest=03/26/2014:00:00:00 '
            'c_vx_zone="external" (host=4 OR host=5 OR host=2) by _time span=5m'
            '| timechart span=5m limit=12 sum(x) as s eval(sum(x)) as x'
            '| append [gentimes start=03/24/2014:00:00:00  end=03/26/2014:00:00:00 increment=1h'
            '| rename starttime as _time | fields _time]'
            '| timechart max(x) as "Peak unique IP addresses" sum(s) as #samples span=1h'
            '| convert timeformat="%Y-%m-%dT%H:%M:%SZ" ctime(_time) | sort _time| rename _time as Time| fillnull'
            '| fields - _* NULL VALUE')

    def test_peak_unique_ip_address_timechart_with_split(self):
        """Test peak unique ip address splitting by time and http status
        The query can only use the main index, this tests the following
        scenario:
        - <b>Main Index (access)</b>

        This is then combined with a chart query to present the results, as
        presented in the following list. This test checks the highlighted
        scenario:
        - <b>time chart (with split) + time chart postfix + query postfix</b>
        - time chart (no split) + time chart postfix + query postfix
        """
        self.invoke_build_search_query(JsonKeys.METRIC_PEAK_UNIQUE_CLIENT_IP_ADDRESS, "2days", "0days", "1hour",
                use_split=True)
        self.check_query(
            '| tstats dc(c_ip) as x where index="cdn_main" '
            'sourcetype=cdn_del_access earliest=03/24/2014:00:00:00 latest=03/26/2014:00:00:00 '
            'c_vx_zone="external" (host=4 OR host=5 OR host=2) by _time span=5m, sc_status '
            '| lookup http_status_codes status AS sc_status OUTPUTNEW status_type, status_ok '
            '| timechart span=5m limit=12 sum(x) as s eval(sum(x)) as x by status_type '
            '| append [gentimes start=03/24/2014:00:00:00 end=03/26/2014:00:00:00 increment=1h '
            '| rename starttime as _time | fields _time] '
            '| timechart max(x: *) as * sum(s: *) as #samples:* span=1h '
            '| convert timeformat="%Y-%m-%dT%H:%M:%SZ" ctime(_time) | sort _time | rename _time as Time '
            '| fillnull | fields - _* NULL VALUE')

    def test_peak_unique_ip_address_timechart_with_filter(self):
        """Test peak unique ip address splitting by time and filtered by http status
        The query can only use the main index, this tests the following
        scenario:
        - <b>Main Index (access)</b>

        This is then combined with a chart query to present the results, as
        presented in the following list. This test checks the highlighted
        scenario:
        - <b>time chart (with split) + time chart postfix + query postfix</b>
        - time chart (no split) + time chart postfix + query postfix
        """
        self.invoke_build_search_query(JsonKeys.METRIC_PEAK_UNIQUE_CLIENT_IP_ADDRESS, "2days", "0days", "1hour",
                use_filter=True)
        self.check_query(
            '| tstats dc(c_ip) as x where '
            'index="cdn_main" sourcetype=cdn_del_access earliest=03/24/2014:00:00:00 latest=03/26/2014:00:00:00 '
            'c_vx_zone="external" (host=4 OR host=5 OR host=2) by _time span=5m, sc_status '
            '| lookup http_status_codes status AS sc_status OUTPUTNEW status_type, status_ok '
            '| search (status_type = "Successful 2xx" ) '
            '| timechart span=5m limit=12 sum(x) as s eval(sum(x)) as x '
            '| append [gentimes start=03/24/2014:00:00:00 end=03/26/2014:00:00:00 increment=1h '
            '| rename starttime as _time | fields _time] '
            '| timechart max(x) as "Peak unique IP addresses" sum(s) as #samples span=1h '
            '| convert timeformat="%Y-%m-%dT%H:%M:%SZ" ctime(_time) | sort _time | rename _time as Time '
            '| fillnull | fields - _* NULL VALUE')

    def test_peak_unique_ip_address_timechart_with_split_and_filter(self):
        """Test peak unique ip address splitting by time and http status, filtering by http status
        The query can only use the main index, this tests the following
        scenario:
        - <b>Main Index (access)</b>

        This is then combined with a chart query to present the results, as
        presented in the following list. This test checks the highlighted
        scenario: 
        - <b>time chart (with split) + time chart postfix + query postfix</b>        
        - time chart (no split) + time chart postfix + query postfix
        """
        self.invoke_build_search_query(JsonKeys.METRIC_PEAK_UNIQUE_CLIENT_IP_ADDRESS, "2days", "0days", "1hour",
                use_split=True, use_filter=True)
        self.check_query(
            '| tstats dc(c_ip) as x where '
            'index="cdn_main" sourcetype=cdn_del_access earliest=03/24/2014:00:00:00 latest=03/26/2014:00:00:00 '
            'c_vx_zone="external" (host=4 OR host=5 OR host=2) by _time span=5m, sc_status '
            '| lookup http_status_codes status AS sc_status OUTPUTNEW status_type, status_ok '
            '| search (status_type = "Successful 2xx" ) '
            '| timechart span=5m limit=12 sum(x) as s eval(sum(x)) as x by status_type '
            '| append [gentimes start=03/24/2014:00:00:00 end=03/26/2014:00:00:00 increment=1h '
            '| rename starttime as _time | fields _time] '
            '| timechart max(x: *) as * sum(s: *) as #samples:* span=1h '
            '| convert timeformat="%Y-%m-%dT%H:%M:%SZ" ctime(_time) '
            '| sort _time | rename _time as Time '
            '| fillnull| fields - _* NULL VALUE')

class TestCDNInsightsBuildSearchQuery(BuildSearchQueryValidationMixin):
    """Test for the _build_search_query with the CDN Insights Pack.
    Tests for the @ref
    unapireporting.datamodel.report_job_collection.ReportJobCollection._build_search_query method.
    This checks the various ways in which the search query can be built up.
    """

    def invoke_build_search_query(self, metric,
                                  start_time, end_time, time_resolution,
                                  split_by=None, tier_filter=None,
                                  network_response_filter=None,
                                  split_limit=DEFAULT_SPLIT_LIMIT,
                                  report_id='tier-analysis'):
        """Create a report job collection and call @c _parse_time_parameters.
        This method stores the object and query parts as returned by @c
        _parse_time_parameters in member variables.
        @param metric - parameter for method under test
        @param start_time - parameter for method under test
        @param end_time - parameter for method under test
        @param time_resolution - parameter for method under test
        @param use_split - whether or not to use a split
        @param tier_filter - optional list if tiers to filter by
        @param network_reponse_filter - optional list if network responses to
                                        filter by
        @param split_limit - the split limit that has to be used
        @param report_id - report to run, defaults to @c tier-analysis report
        """

        # Split by Network Tier if needed
        if split_by is None:
            split_by = {}

        # Both a tier filter and a network response filter is supported by this
        # reporting pack.
        filter_by = {}
        if tier_filter:
            filter_by[JsonKeys.FILTER_BY_CDN_TIER] = tier_filter
        if network_response_filter:
            filter_by[JsonKeys.FILTER_BY_NETWORK_RESPONSE] = network_response_filter

        self._invoke_build_search_query(metric, start_time, end_time,
                                        time_resolution, split_by, filter_by,
                                        split_limit, report_id)

    def test_bytes_transferred_query_timechart(self):
        """Test bytes transferred query splitting by time only.
        The query can only use the main index, but does it twice once for each
        source type @c cdn_del_access and @c cdn_del_acquisition:
        - <b>Main Index (access) + Main Index (acquiry)</b>

        This is then combined with a chart query to present the results, as
        presented in the following list. This test checks the highlighted
        scenario:
        - time chart (with split) + time chart postfix + query postfix
        - <b>time chart (no split) + time chart postfix + query postfix</b>
        - chart with split + split chart postfix + query postfix
        - total chart + query postfix
        """
        self.invoke_build_search_query(
            JsonKeys.METRIC_BYTES_TRANSFERRED, "2days", "0days", "1hour")
        self.check_query(
            '| tstats  sum(sc_bytes) as x where index="cdn_main" sourcetype=cdn_del_access '
            'earliest=03/24/2014:00:00:00 latest=03/26/2014:00:00:00 '
            'c_vx_zone="external" (host=4 OR host=5 OR host=2)  by _time span=1h '
            '| append [ tstats  sum(rs_bytes) as x where index="cdn_main" sourcetype=cdn_del_acquisition '
            'earliest=03/24/2014:00:00:00 latest=03/26/2014:00:00:00 '
            'r_vx_zone="cdn-origin" OR r_vx_zone="external" OR r_vx_zone="cdn" '
            '(host=4 OR host=5 OR host=2)  by _time span=1h '
            ' ]'
            '| eval x=x '
            '| append [gentimes start=03/24/2014:00:00:00 end=03/26/2014:00:00:00 increment=1h '
            '| rename starttime as _time | fields _time | eval x=0 ] '
            '| timechart span=1h limit=12 sum(x) as "Bytes transferred"   '
            '| convert timeformat="%Y-%m-%dT%H:%M:%SZ" ctime(_time) '
            '| sort _time| rename _time as Time| fillnull | fields - _* NULL VALUE')

    def test_bytes_transferred_query_timechart_with_split(self):
        """Test bytes transferred query splitting by time and network tier.
        The query can only use the main index, but does it twice once for each
        source type @c cdn_del_access and @c cdn_del_acquisition:
        - <b>Main Index (access) + Main Index (acquiry)</b> (with Split)

        This is then combined with a chart query to present the results, as
        presented in the following list. This test checks the highlighted
        scenario:
        - <b>time chart (with split) + time chart postfix + query postfix</b>
        - time chart (no split) + time chart postfix + query postfix
        - chart with split + split chart postfix + query postfix
        - total chart + query postfix
        """
        self.invoke_build_search_query(
            JsonKeys.METRIC_BYTES_TRANSFERRED, "10days", "0days", "1day",
            split_by={JsonKeys.SPLIT_BY_NETWORK: JsonKeys.SPLIT_BY_NETWORK_TIER})
        self.check_query(
            '| tstats  sum(sc_bytes) as x where index="cdn_main" sourcetype=cdn_del_access '
            'earliest=03/16/2014:00:00:00 latest=03/26/2014:00:00:00 '
            'c_vx_zone="external" (host=4 OR host=5 OR host=2)  by _time span=1d, c_vx_zone '
            '| append [ tstats  sum(rs_bytes) as x where index="cdn_main" sourcetype=cdn_del_acquisition '
            'earliest=03/16/2014:00:00:00 latest=03/26/2014:00:00:00 '
            'r_vx_zone="cdn-origin" OR r_vx_zone="external" OR r_vx_zone="cdn" '
            '(host=4 OR host=5 OR host=2)  by _time span=1d, r_vx_zone '
            '| eval c_vx_zone = if(r_vx_zone == "cdn", "cdn", "acquiry") ]'
            '| eval x=x '
            '| append [gentimes start=03/16/2014:00:00:00 end=03/26/2014:00:00:00 increment=1d '
            '| rename starttime as _time | fields _time | eval x=0 | eval c_vx_zone=NULL] '
            '| timechart span=1d limit=12 sum(x) as "Bytes transferred" by c_vx_zone '
            '| convert timeformat="%Y-%m-%dT%H:%M:%SZ" ctime(_time) '
            '| sort _time'
            '| rename _time as Date| fillnull | fields - _* NULL VALUE')

    def test_bytes_transferred_query_splitchart(self):
        """Test bytes transferred query splitting by network tier only.
        The query can only use the main index, but does it twice once for each
        source type @c cdn_del_access and @c cdn_del_acquisition:
        - <b>Main Index (access) + Main Index (acquiry)</b>

        This is then combined with a chart query to present the results, as
        presented in the following list. This test checks the highlighted
        scenario:
        - time chart (with split) + time chart postfix + query postfix
        - time chart (no split) + time chart postfix + query postfix
        - <b>chart with split + split chart postfix + query postfix</b>
        - total chart + query postfix
        """
        self.invoke_build_search_query(
            JsonKeys.METRIC_BYTES_TRANSFERRED, "10days", "0days", "noTimeSplit",
            split_by={JsonKeys.SPLIT_BY_NETWORK: JsonKeys.SPLIT_BY_NETWORK_TIER},
            split_limit=9)
        self.check_query(
            '| tstats  sum(sc_bytes) as x where index="cdn_main" sourcetype=cdn_del_access '
            'earliest=03/16/2014:00:00:00 latest=03/26/2014:00:00:00 '
            'c_vx_zone="external" (host=4 OR host=5 OR host=2)  by _time span=1d, c_vx_zone '
            '| append [ tstats  sum(rs_bytes) as x where index="cdn_main" sourcetype=cdn_del_acquisition '
            'earliest=03/16/2014:00:00:00 latest=03/26/2014:00:00:00 '
            'r_vx_zone="cdn-origin" OR r_vx_zone="external" OR r_vx_zone="cdn" (host=4 OR host=5 OR host=2) '
            ' by _time span=1d, r_vx_zone '
            '| eval c_vx_zone = if(r_vx_zone == "cdn", "cdn", "acquiry") ]'
            '| eval x=x '
            '| chart limit=9 sum(x) as x by c_vx_zone '
            '| where isnotnull(c_vx_zone) and len(tostring(c_vx_zone))>0 '
            '| sort - x '
            '| streamstats count as _row_number '
            '| eval c_vx_zone=if(0 = 9 or _row_number <= 9, c_vx_zone, "Other") '
            '| stats sum(*) as * by c_vx_zone '
            '| rename x as "Bytes transferred", s as #samples c_vx_zone as "Tier"'
            '| fillnull | fields - _* NULL VALUE')

    def test_bytes_transferred_query_totalchart(self):
        """Test bytes transferred query without a split.
        The query can only use the main index, but does it twice once for each
        source type @c cdn_del_access and @c cdn_del_acquisition:
        - <b>Main Index (access) + Main Index (acquiry)</b>

        This is then combined with a chart query to present the results, as
        presented in the following list. This test checks the highlighted
        scenario:
        - time chart (with split) + time chart postfix + query postfix
        - time chart (no split) + time chart postfix + query postfix
        - chart with split + split chart postfix + query postfix
        - <b>total chart + query postfix</b>
        """
        self.invoke_build_search_query(
            JsonKeys.METRIC_BYTES_TRANSFERRED, "10days", "0days", "noTimeSplit")
        self.check_query(
            '| tstats  sum(sc_bytes) as x where index="cdn_main" sourcetype=cdn_del_access '
            'earliest=03/16/2014:00:00:00 latest=03/26/2014:00:00:00 '
            'c_vx_zone="external" (host=4 OR host=5 OR host=2)  by _time span=1d '
            '| append [ tstats  sum(rs_bytes) as x where index="cdn_main" sourcetype=cdn_del_acquisition '
            'earliest=03/16/2014:00:00:00 latest=03/26/2014:00:00:00 '
            'r_vx_zone="cdn-origin" OR r_vx_zone="external" OR r_vx_zone="cdn" '
            '(host=4 OR host=5 OR host=2)  by _time span=1d '
            ' ]'
            '| eval x=x '
            '| chart limit=12 sum(x) as "Bytes transferred"| fillnull | fields - _* NULL VALUE')

    def test_peak_bytes_transferred_query_timechart(self):
        """Test peak bytes transferred query splitting by time only.
        The query can only use the main index, but does it twice once for each
        source type @c cdn_del_access and @c cdn_del_acquisition:
        - <b>Main Index (access) + Main Index (acquiry)</b>

        This is then combined with a chart query to present the results, as
        presented in the following list. This test checks the highlighted
        scenario:
        - time chart (with split) + time chart postfix + query postfix
        - <b>time chart (no split) + time chart postfix + query postfix</b>
        - chart with split + split chart postfix + query postfix
        - total chart + query postfix
        """
        self.invoke_build_search_query(
            JsonKeys.METRIC_PEAK_TRANSFERRED, "2days", "0days", "1hour")
        self.check_query(
            '| tstats count as s sum(sc_bytes) as x where index="cdn_main" sourcetype=cdn_del_access '
            'earliest=03/24/2014:00:00:00 latest=03/26/2014:00:00:00 '
            'c_vx_zone="external" (host=4 OR host=5 OR host=2)  by _time span=5m '
            '| append [ tstats count as s sum(rs_bytes) as x where index="cdn_main" sourcetype=cdn_del_acquisition '
            'earliest=03/24/2014:00:00:00 latest=03/26/2014:00:00:00 '
            'r_vx_zone="cdn-origin" OR r_vx_zone="external" OR r_vx_zone="cdn" '
            '(host=4 OR host=5 OR host=2)  by _time span=5m '
            ' ]'
            '| timechart span=5m limit=12  sum(s) as s eval(sum(x)*8/300) as x   '
            '| append [gentimes start=03/24/2014:00:00:00 end=03/26/2014:00:00:00 increment=1h '
            '| rename starttime as _time | fields _time] '
            '| timechart max(x) as "Peak throughput (bps)" sum(s) as #samples span=1h '
            '| convert timeformat="%Y-%m-%dT%H:%M:%SZ" ctime(_time) '
            '| sort _time| rename _time as Time| fillnull | fields - _* NULL VALUE')

    def test_peak_bytes_transferred_query_timechart_with_split(self):
        """Test peak bytes tranferred query splitting by time and network tier.
        The query can only use the main index, but does it twice once for each
        source type @c cdn_del_access and @c cdn_del_acquisition:
        - <b>Main Index (access) + Main Index (acquiry)</b> (with Split)

        This is then combined with a chart query to present the results, as
        presented in the following list. This test checks the highlighted
        scenario:
        - <b>time chart (with split) + time chart postfix + query postfix</b>
        - time chart (no split) + time chart postfix + query postfix
        - chart with split + split chart postfix + query postfix
        - total chart + query postfix
        """
        self.invoke_build_search_query(
            JsonKeys.METRIC_PEAK_TRANSFERRED, "10days", "0days", "1day",
            split_by={JsonKeys.SPLIT_BY_NETWORK: JsonKeys.SPLIT_BY_NETWORK_TIER})
        self.check_query(
            '| tstats count as s sum(sc_bytes) as x where index="cdn_main" sourcetype=cdn_del_access '
            'earliest=03/16/2014:00:00:00 latest=03/26/2014:00:00:00 '
            'c_vx_zone="external" (host=4 OR host=5 OR host=2)  by _time span=5m, c_vx_zone '
            '| append [ tstats count as s sum(rs_bytes) as x where index="cdn_main" sourcetype=cdn_del_acquisition '
            'earliest=03/16/2014:00:00:00 latest=03/26/2014:00:00:00 '
            'r_vx_zone="cdn-origin" OR r_vx_zone="external" OR r_vx_zone="cdn" '
            '(host=4 OR host=5 OR host=2)  by _time span=5m, r_vx_zone '
            '| eval c_vx_zone = if(r_vx_zone == "cdn", "cdn", "acquiry") ]'
            '| timechart span=5m limit=12  sum(s) as s eval(sum(x)*8/300) as x by c_vx_zone '
            '| append [gentimes start=03/16/2014:00:00:00 end=03/26/2014:00:00:00 increment=1d '
            '| rename starttime as _time | fields _time] | timechart max(x: *) as * sum(s: *) as #samples:* span=1d '
            '| convert timeformat="%Y-%m-%dT%H:%M:%SZ" ctime(_time) '
            '| sort _time| rename _time as Date| fillnull | fields - _* NULL VALUE')

    def test_peak_bytes_transferred_query_splitchart(self):
        """Test peak bytes tranferred query splitting by network tier only.
        The query can only use the main index, but does it twice once for each
        source type @c cdn_del_access and @c cdn_del_acquisition:
        - <b>Main Index (access) + Main Index (acquiry)</b>

        This is then combined with a chart query to present the results, as
        presented in the following list. This test checks the highlighted
        scenario:
        - time chart (with split) + time chart postfix + query postfix
        - time chart (no split) + time chart postfix + query postfix
        - <b>chart with split + split chart postfix + query postfix</b>
        - total chart + query postfix
        """
        self.invoke_build_search_query(
            JsonKeys.METRIC_PEAK_TRANSFERRED, "10days", "0days", "noTimeSplit",
            split_by={JsonKeys.SPLIT_BY_NETWORK: JsonKeys.SPLIT_BY_NETWORK_TIER},
            split_limit=9)
        self.check_query(
            '| tstats count as s sum(sc_bytes) as x where index="cdn_main" sourcetype=cdn_del_access '
            'earliest=03/16/2014:00:00:00 latest=03/26/2014:00:00:00 '
            'c_vx_zone="external" (host=4 OR host=5 OR host=2)  by _time span=5m, c_vx_zone '
            '| append [ tstats count as s sum(rs_bytes) as x where index="cdn_main" sourcetype=cdn_del_acquisition '
            'earliest=03/16/2014:00:00:00 latest=03/26/2014:00:00:00 '
            'r_vx_zone="cdn-origin" OR r_vx_zone="external" OR r_vx_zone="cdn" '
            '(host=4 OR host=5 OR host=2)  by _time span=5m, r_vx_zone '
            '| eval c_vx_zone = if(r_vx_zone == "cdn", "cdn", "acquiry") ]'
            '| eventstats sum(s) as s sum(x) as x by _time, c_vx_zone| '
            'chart limit=9 sum(s) as s eval(max(x)*8/300) as x by c_vx_zone'
            '| sort - x | streamstats count as _row_number '
            '| eval c_vx_zone=if(0 = 9 or _row_number <= 9, c_vx_zone, "Other") '
            '| stats sum(*) as * by c_vx_zone '
            '| rename x as "Peak throughput (bps)", s as #samples c_vx_zone as "Tier"'
            '| fillnull | fields - _* NULL VALUE')

    def test_peak_bytes_transferred_query_totalchart(self):
        """Test peak bytes tranferred query without a split.
        The query can only use the main index, but does it twice once for each
        source type @c cdn_del_access and @c cdn_del_acquisition:
        - <b>Main Index (access) + Main Index (acquiry)</b>

        This is then combined with a chart query to present the results, as
        presented in the following list. This test checks the highlighted
        scenario:
        - time chart (with split) + time chart postfix + query postfix
        - time chart (no split) + time chart postfix + query postfix
        - chart with split + split chart postfix + query postfix
        - <b>total chart + query postfix</b>
        """
        self.invoke_build_search_query(
            JsonKeys.METRIC_PEAK_TRANSFERRED, "10days", "0days", "noTimeSplit")
        self.check_query(
            '| tstats count as s sum(sc_bytes) as x where index="cdn_main" sourcetype=cdn_del_access '
            'earliest=03/16/2014:00:00:00 latest=03/26/2014:00:00:00 '
            'c_vx_zone="external" (host=4 OR host=5 OR host=2)  by _time span=5m '
            '| append [ tstats count as s sum(rs_bytes) as x where index="cdn_main" sourcetype=cdn_del_acquisition '
            'earliest=03/16/2014:00:00:00 latest=03/26/2014:00:00:00 '
            'r_vx_zone="cdn-origin" OR r_vx_zone="external" OR r_vx_zone="cdn" '
            '(host=4 OR host=5 OR host=2)  by _time span=5m '
            ' ]'
            '| eventstats sum(s) as s sum(x) as x by _time| '
            'chart limit=12 sum(s) as #samples eval(max(x)*8/300) as "Peak throughput (bps)"'
            '| fillnull | fields - _* NULL VALUE')

    def test_perc95_bytes_transferred_query_timechart(self):
        """Test 95th percentile bytes tranferred query splitting by time only.
        The query can only use the main index, but does it twice once for each
        source type @c cdn_del_access and @c cdn_del_acquisition:
        - <b>Main Index (access) + Main Index (acquiry)</b>

        This is then combined with a chart query to present the results, as
        presented in the following list. This test checks the highlighted
        scenario:
        - time chart (with split) + time chart postfix + query postfix
        - <b>time chart (no split) + time chart postfix + query postfix</b>
        - chart with split + split chart postfix + query postfix
        - total chart + query postfix
        """
        self.invoke_build_search_query(
            JsonKeys.METRIC_95TH_PERCENTILE_TRANSFERRED,
            "2days", "0days", "1hour")
        self.check_query(
            '| tstats count as s sum(sc_bytes) as x where index="cdn_main" sourcetype=cdn_del_access '
            'earliest=03/24/2014:00:00:00 latest=03/26/2014:00:00:00 '
            'c_vx_zone="external" (host=4 OR host=5 OR host=2)  by _time span=5m '
            '| append [ tstats count as s sum(rs_bytes) as x where index="cdn_main" sourcetype=cdn_del_acquisition '
            'earliest=03/24/2014:00:00:00 latest=03/26/2014:00:00:00 '
            'r_vx_zone="cdn-origin" OR r_vx_zone="external" OR r_vx_zone="cdn" '
            '(host=4 OR host=5 OR host=2)  by _time span=5m '
            ' ]'
            '| timechart span=5m limit=12  sum(s) as s eval(sum(x)*8/300) as x   '
            '| append [gentimes start=03/24/2014:00:00:00 end=03/26/2014:00:00:00 increment=1h '
            '| rename starttime as _time | fields _time] '
            '| timechart perc95(x) as "95th percentile throughput (bps)" sum(s) as #samples span=1h '
            '| convert timeformat="%Y-%m-%dT%H:%M:%SZ" ctime(_time) '
            '| sort _time| rename _time as Time| fillnull | fields - _* NULL VALUE')

    def test_perc95_bytes_transferred_query_timechart_with_split(self):
        """Test 95th percentile bytes tranferred query splitting by time and network tier.
        The query can only use the main index, but does it twice once for each
        source type @c cdn_del_access and @c cdn_del_acquisition:
        - <b>Main Index (access) + Main Index (acquiry)</b> (with Split)

        This is then combined with a chart query to present the results, as
        presented in the following list. This test checks the highlighted
        scenario:
        - <b>time chart (with split) + time chart postfix + query postfix</b>
        - time chart (no split) + time chart postfix + query postfix
        - chart with split + split chart postfix + query postfix
        - total chart + query postfix
        """
        self.invoke_build_search_query(
            JsonKeys.METRIC_95TH_PERCENTILE_TRANSFERRED,
            "10days", "0days", "1day",
            split_by={JsonKeys.SPLIT_BY_NETWORK: JsonKeys.SPLIT_BY_NETWORK_TIER})
        self.check_query(
            '| tstats count as s sum(sc_bytes) as x where index="cdn_main" sourcetype=cdn_del_access '
            'earliest=03/16/2014:00:00:00 latest=03/26/2014:00:00:00 '
            'c_vx_zone="external" (host=4 OR host=5 OR host=2)  by _time span=5m, c_vx_zone '
            '| append [ '
            'tstats count as s sum(rs_bytes) as x where index="cdn_main" sourcetype=cdn_del_acquisition '
            'earliest=03/16/2014:00:00:00 latest=03/26/2014:00:00:00 '
            'r_vx_zone="cdn-origin" OR r_vx_zone="external" OR r_vx_zone="cdn" '
            '(host=4 OR host=5 OR host=2)  by _time span=5m, r_vx_zone '
            '| eval c_vx_zone = if(r_vx_zone == "cdn", "cdn", "acquiry") ]'
            '| timechart span=5m limit=12  sum(s) as s eval(sum(x)*8/300) as x by c_vx_zone '
            '| append [gentimes start=03/16/2014:00:00:00 end=03/26/2014:00:00:00 increment=1d '
            '| rename starttime as _time | fields _time] '
            '| timechart perc95(x: *) as * sum(s: *) as #samples:* span=1d '
            '| convert timeformat="%Y-%m-%dT%H:%M:%SZ" ctime(_time) '
            '| sort _time| rename _time as Date| fillnull | fields - _* NULL VALUE')

    def test_perc95_bytes_transferred_query_splitchart(self):
        """Test 95th percentile bytes tranferred query splitting by network tier only.
        The query can only use the main index, but does it twice once for each
        source type @c cdn_del_access and @c cdn_del_acquisition:
        - <b>Main Index (access) + Main Index (acquiry)</b>

        This is then combined with a chart query to present the results, as
        presented in the following list. This test checks the highlighted
        scenario:
        - time chart (with split) + time chart postfix + query postfix
        - time chart (no split) + time chart postfix + query postfix
        - <b>chart with split + split chart postfix + query postfix</b>
        - total chart + query postfix
        """
        self.invoke_build_search_query(
            JsonKeys.METRIC_95TH_PERCENTILE_TRANSFERRED,
            "10days", "0days", "noTimeSplit",
            split_by={JsonKeys.SPLIT_BY_NETWORK: JsonKeys.SPLIT_BY_NETWORK_TIER},
            split_limit=9)
        self.check_query(
            '| tstats count as s sum(sc_bytes) as x where index="cdn_main" sourcetype=cdn_del_access '
            'earliest=03/16/2014:00:00:00 latest=03/26/2014:00:00:00 '
            'c_vx_zone="external" (host=4 OR host=5 OR host=2)  by _time span=5m, c_vx_zone '
            '| append [ tstats count as s sum(rs_bytes) as x where index="cdn_main" sourcetype=cdn_del_acquisition '
            'earliest=03/16/2014:00:00:00 latest=03/26/2014:00:00:00 '
            'r_vx_zone="cdn-origin" OR r_vx_zone="external" OR r_vx_zone="cdn" '
            '(host=4 OR host=5 OR host=2)  by _time span=5m, r_vx_zone '
            '| eval c_vx_zone = if(r_vx_zone == "cdn", "cdn", "acquiry") ]'
            '| eventstats sum(s) as s sum(x) as x by _time, c_vx_zone'
            '| chart limit=9 sum(s) as s eval(perc95(x)*8/300) as x by c_vx_zone'
            '| sort - x | streamstats count as _row_number '
            '| eval c_vx_zone=if(0 = 9 or _row_number <= 9, c_vx_zone, "Other") '
            '| stats sum(*) as * by c_vx_zone '
            '| rename x as "95th percentile throughput (bps)", s as #samples c_vx_zone as "Tier"'
            '| fillnull | fields - _* NULL VALUE')

    def test_perc95_bytes_transferred_query_totalchart(self):
        """Test 95th percentile bytes tranferred query without a split.
        The query can only use the main index, but does it twice once for each
        source type @c cdn_del_access and @c cdn_del_acquisition:
        - <b>Main Index (access) + Main Index (acquiry)</b>

        This is then combined with a chart query to present the results, as
        presented in the following list. This test checks the highlighted
        scenario:
        - time chart (with split) + time chart postfix + query postfix
        - time chart (no split) + time chart postfix + query postfix
        - chart with split + split chart postfix + query postfix
        - <b>total chart + query postfix</b>
        """
        self.invoke_build_search_query(
            JsonKeys.METRIC_95TH_PERCENTILE_TRANSFERRED,
            "10days", "0days", "noTimeSplit")
        self.check_query(
            '| tstats count as s sum(sc_bytes) as x where index="cdn_main" sourcetype=cdn_del_access '
            'earliest=03/16/2014:00:00:00 latest=03/26/2014:00:00:00 '
            'c_vx_zone="external" (host=4 OR host=5 OR host=2)  by _time span=5m '
            '| append [ tstats count as s sum(rs_bytes) as x where index="cdn_main" sourcetype=cdn_del_acquisition '
            'earliest=03/16/2014:00:00:00 latest=03/26/2014:00:00:00 '
            'r_vx_zone="cdn-origin" OR r_vx_zone="external" OR r_vx_zone="cdn" '
            '(host=4 OR host=5 OR host=2)  by _time span=5m '
            ' ]'
            '| eventstats sum(s) as s sum(x) as x by _time'
            '| chart limit=12 sum(s) as #samples eval(perc95(x)*8/300) as "95th percentile throughput (bps)"'
            '| fillnull | fields - _* NULL VALUE')

    def test_requests_transferred_query_timechart(self):
        """Test requests transferred (aka request count) query splitting by time only.
        The query can only use the main index, but does it twice once for each
        source type @c cdn_del_access and @c cdn_del_acquisition:
        - <b>Main Index (access) + Main Index (acquiry)</b>

        This is then combined with a chart query to present the results, as
        presented in the following list. This test checks the highlighted
        scenario:
        - time chart (with split) + time chart postfix + query postfix
        - <b>time chart (no split) + time chart postfix + query postfix</b>
        - chart with split + split chart postfix + query postfix
        - total chart + query postfix
        """
        self.invoke_build_search_query(
            JsonKeys.METRIC_REQUESTS_TRANSFERRED, "2days", "0days", "1hour")
        self.check_query(
            '| tstats  count as x where index="cdn_main" sourcetype=cdn_del_access '
            'earliest=03/24/2014:00:00:00 latest=03/26/2014:00:00:00 '
            'c_vx_zone="external" (host=4 OR host=5 OR host=2)  by _time span=1h '
            '| append [ tstats  count as x where index="cdn_main" sourcetype=cdn_del_acquisition '
            'earliest=03/24/2014:00:00:00 latest=03/26/2014:00:00:00 '
            'r_vx_zone="cdn-origin" OR r_vx_zone="external" OR r_vx_zone="cdn" '
            '(host=4 OR host=5 OR host=2)  by _time span=1h '
            ' ]'
            '| eval x=x '
            '| append [gentimes start=03/24/2014:00:00:00 end=03/26/2014:00:00:00 '
            'increment=1h '
            '| rename starttime as _time '
            '| fields _time '
            '| eval x=0 ] '
            '| timechart span=1h limit=12 sum(x) as "Request count"   '
            '| convert timeformat="%Y-%m-%dT%H:%M:%SZ" ctime(_time) '
            '| sort _time'
            '| rename _time as Time'
            '| fillnull '
            '| fields - _* NULL VALUE')

    def test_requests_transferred_query_timechart_with_split(self):
        """Test requests transferred (aka request count) query splitting by time only.
        The query can only use the main index, but does it twice once for each
        source type @c cdn_del_access and @c cdn_del_acquisition:
        - <b>Main Index (access) + Main Index (acquiry)</b> (with Split)

        This is then combined with a chart query to present the results, as
        presented in the following list. This test checks the highlighted
        scenario:
        - <b>time chart (with split) + time chart postfix + query postfix</b>
        - time chart (no split) + time chart postfix + query postfix
        - chart with split + split chart postfix + query postfix
        - total chart + query postfix
        """
        self.invoke_build_search_query(
            JsonKeys.METRIC_REQUESTS_TRANSFERRED, "10days", "0days", "1day",
            split_by={JsonKeys.SPLIT_BY_NETWORK: JsonKeys.SPLIT_BY_NETWORK_TIER})
        self.check_query(
            '| tstats  count as x where index="cdn_main" sourcetype=cdn_del_access '
            'earliest=03/16/2014:00:00:00 latest=03/26/2014:00:00:00 '
            'c_vx_zone="external" (host=4 OR host=5 OR host=2)  by _time span=1d, c_vx_zone '
            '| append [ tstats  count as x where index="cdn_main" sourcetype=cdn_del_acquisition '
            'earliest=03/16/2014:00:00:00 latest=03/26/2014:00:00:00 '
            'r_vx_zone="cdn-origin" OR r_vx_zone="external" OR r_vx_zone="cdn" '
            '(host=4 OR host=5 OR host=2)  by _time span=1d, r_vx_zone '
            '| eval c_vx_zone = if(r_vx_zone == "cdn", "cdn", "acquiry") ]'
            '| eval x=x '
            '| append [gentimes start=03/16/2014:00:00:00 end=03/26/2014:00:00:00 increment=1d '
            '| rename starttime as _time '
            '| fields _time '
            '| eval x=0 '
            '| eval c_vx_zone=NULL] '
            '| timechart span=1d limit=12 sum(x) as "Request count" by c_vx_zone '
            '| convert timeformat="%Y-%m-%dT%H:%M:%SZ" ctime(_time) '
            '| sort _time'
            '| rename _time as Date'
            '| fillnull '
            '| fields - _* NULL VALUE')

    def test_requests_transferred_query_splitchart(self):
        """Test requests transferred (aka request count) query splitting by time only.
        The query can only use the main index, but does it twice once for each
        source type @c cdn_del_access and @c cdn_del_acquisition:
        - <b>Main Index (access) + Main Index (acquiry)</b>

        This is then combined with a chart query to present the results, as
        presented in the following list. This test checks the highlighted
        scenario:
        - time chart (with split) + time chart postfix + query postfix
        - time chart (no split) + time chart postfix + query postfix
        - <b>chart with split + split chart postfix + query postfix</b>
        - total chart + query postfix
        """
        self.invoke_build_search_query(
            JsonKeys.METRIC_REQUESTS_TRANSFERRED, "10days", "0days", "noTimeSplit",
            split_by={JsonKeys.SPLIT_BY_NETWORK: JsonKeys.SPLIT_BY_NETWORK_TIER},
            split_limit=9)
        self.check_query(
            '| tstats  count as x where index="cdn_main" sourcetype=cdn_del_access '
            'earliest=03/16/2014:00:00:00 latest=03/26/2014:00:00:00 '
            'c_vx_zone="external" (host=4 OR host=5 OR host=2)  by _time span=1d, c_vx_zone '
            '| append [ tstats  count as x where index="cdn_main" sourcetype=cdn_del_acquisition '
            'earliest=03/16/2014:00:00:00 latest=03/26/2014:00:00:00 '
            'r_vx_zone="cdn-origin" OR r_vx_zone="external" OR r_vx_zone="cdn" (host=4 OR host=5 OR host=2)  '
            'by _time span=1d, r_vx_zone '
            '| eval c_vx_zone = if(r_vx_zone == "cdn", "cdn", "acquiry") ]'
            '| eval x=x | chart limit=9 sum(x) as x by c_vx_zone '
            '| where isnotnull(c_vx_zone) and len(tostring(c_vx_zone))>0 '
            '| sort - x '
            '| streamstats count as _row_number '
            '| eval c_vx_zone=if(0 = 9 or _row_number <= 9, c_vx_zone, "Other") '
            '| stats sum(*) as * by c_vx_zone '
            '| rename x as "Request count", s as #samples c_vx_zone as "Tier"'
            '| fillnull '
            '| fields - _* NULL VALUE')

    def test_requests_transferred_query_totalchart(self):
        """Test requests transferred (aka request count) query splitting by time only.
        The query can only use the main index, but does it twice once for each
        source type @c cdn_del_access and @c cdn_del_acquisition:
        - <b>Main Index (access) + Main Index (acquiry)</b>

        This is then combined with a chart query to present the results, as
        presented in the following list. This test checks the highlighted
        scenario:
        - time chart (with split) + time chart postfix + query postfix
        - time chart (no split) + time chart postfix + query postfix
        - chart with split + split chart postfix + query postfix
        - <b>total chart + query postfix</b>
        """
        self.invoke_build_search_query(
            JsonKeys.METRIC_REQUESTS_TRANSFERRED, "10days", "0days", "noTimeSplit")
        self.check_query(
            '| tstats  count as x where index="cdn_main" sourcetype=cdn_del_access '
            'earliest=03/16/2014:00:00:00 latest=03/26/2014:00:00:00 '
            'c_vx_zone="external" (host=4 OR host=5 OR host=2)  by _time span=1d '
            '| append [ tstats  count as x where index="cdn_main" sourcetype=cdn_del_acquisition '
            'earliest=03/16/2014:00:00:00 latest=03/26/2014:00:00:00 '
            'r_vx_zone="cdn-origin" OR r_vx_zone="external" OR r_vx_zone="cdn" (host=4 OR host=5 OR host=2)  '
            'by _time span=1d '
            ' ]'
            '| eval x=x '
            '| chart limit=12 sum(x) as "Request count"'
            '| fillnull '
            '| fields - _* NULL VALUE')

    def test_delivery_tier_filter(self):
        """Test filter by delivery tier.
        The query can only use the main index, but the metric normally uses two
        source types @c cdn_del_access and @c cdn_del_acquisition. However, when
        a filter is applied one of the source types can be excluded. This test
        checks the highlighted scenario:
        - Main Index (access) + Main Index (acquiry)
        - <b>Main Index (access)</b>
        - Main Index (acquiry)

        This is then combined with a chart query to present the results, as
        presented in the following list. This test checks the highlighted
        scenario:
        - time chart (with split) + time chart postfix + query postfix
        - <b>time chart (no split) + time chart postfix + query postfix</b>
        - chart with split + split chart postfix + query postfix
        - total chart + query postfix
        """
        self.invoke_build_search_query(
            JsonKeys.METRIC_BYTES_TRANSFERRED, "2days", "0days", "1hour",
            tier_filter=[JsonKeys.FILTER_BY_CDN_TIER_DELIVERY])
        self.check_query(
            '| tstats  sum(sc_bytes) as x where index="cdn_main" sourcetype=cdn_del_access '
            'earliest=03/24/2014:00:00:00 latest=03/26/2014:00:00:00 '
            'c_vx_zone="external" (host=4 OR host=5 OR host=2)  by _time span=1h '
            '| eval x=x '
            '| append [gentimes start=03/24/2014:00:00:00 end=03/26/2014:00:00:00 increment=1h '
            '| rename starttime as _time | fields _time | eval x=0 ] '
            '| timechart span=1h limit=12 sum(x) as "Bytes transferred"   '
            '| convert timeformat="%Y-%m-%dT%H:%M:%SZ" ctime(_time) '
            '| sort _time| rename _time as Time| fillnull | fields - _* NULL VALUE')

    def test_intermediate_tier_filter(self):
        """Test filter by intermediate tier.
        The query can only use the main index, but the metric normally uses two
        source types @c cdn_del_access and @c cdn_del_acquisition. However, when
        a filter is applied one of the source types can be excluded. This test
        checks the highlighted scenario:
        - Main Index (access) + Main Index (acquiry)
        - Main Index (access)
        - <b>Main Index (acquiry)</b>

        This is then combined with a chart query to present the results, as
        presented in the following list. This test checks the highlighted
        scenario:
        - <b>time chart (with split) + time chart postfix + query postfix</b>
        - time chart (no split) + time chart postfix + query postfix
        - chart with split + split chart postfix + query postfix
        - total chart + query postfix
        """
        self.invoke_build_search_query(
            JsonKeys.METRIC_PEAK_TRANSFERRED, "10days", "0days", "1day",
            split_by={JsonKeys.SPLIT_BY_NETWORK: JsonKeys.SPLIT_BY_NETWORK_TIER},
            tier_filter=[JsonKeys.FILTER_BY_CDN_TIER_INTERMEDIATE])
        self.check_query(
            '| tstats count as s sum(rs_bytes) as x where index="cdn_main" sourcetype=cdn_del_acquisition '
            'earliest=03/16/2014:00:00:00 latest=03/26/2014:00:00:00 '
            'r_vx_zone="cdn" (host=4 OR host=5 OR host=2)  by _time span=5m, r_vx_zone '
            '| eval c_vx_zone = if(r_vx_zone == "cdn", "cdn", "acquiry")'
            '| timechart span=5m limit=12  sum(s) as s eval(sum(x)*8/300) as x by c_vx_zone '
            '| append [gentimes start=03/16/2014:00:00:00 end=03/26/2014:00:00:00 increment=1d '
            '| rename starttime as _time | fields _time] | timechart max(x: *) as * sum(s: *) as #samples:* span=1d '
            '| convert timeformat="%Y-%m-%dT%H:%M:%SZ" ctime(_time) '
            '| sort _time| rename _time as Date| fillnull | fields - _* NULL VALUE')

    def test_acquiry_tier_filter(self):
        """Test filter by acquiry tier.
        The query can only use the main index, but the metric normally uses two
        source types @c cdn_del_access and @c cdn_del_acquisition. However, when
        a filter is applied one of the source types can be excluded. This test
        checks the highlighted scenario:
        - Main Index (access) + Main Index (acquiry)
        - Main Index (access)
        - <b>Main Index (acquiry)</b>

        This is then combined with a chart query to present the results, as
        presented in the following list. This test checks the highlighted
        scenario:
        - time chart (with split) + time chart postfix + query postfix
        - time chart (no split) + time chart postfix + query postfix
        - <b>chart with split + split chart postfix + query postfix</b>
        - total chart + query postfix
        """
        self.invoke_build_search_query(
            JsonKeys.METRIC_95TH_PERCENTILE_TRANSFERRED,
            "10days", "0days", "noTimeSplit",
            split_by={JsonKeys.SPLIT_BY_NETWORK: JsonKeys.SPLIT_BY_NETWORK_TIER},
            tier_filter=[JsonKeys.FILTER_BY_CDN_TIER_ACQUIRY],
            split_limit=9)
        self.check_query(
            '| tstats count as s sum(rs_bytes) as x where index="cdn_main" sourcetype=cdn_del_acquisition '
            'earliest=03/16/2014:00:00:00 latest=03/26/2014:00:00:00 '
            'r_vx_zone="cdn-origin" OR r_vx_zone="external" (host=4 OR host=5 OR host=2)  '
            'by _time span=5m, r_vx_zone '
            '| eval c_vx_zone = if(r_vx_zone == "cdn", "cdn", "acquiry")'
            '| eventstats sum(s) as s sum(x) as x by _time, c_vx_zone'
            '| chart limit=9 sum(s) as s eval(perc95(x)*8/300) as x by c_vx_zone'
            '| sort - x | streamstats count as _row_number '
            '| eval c_vx_zone=if(0 = 9 or _row_number <= 9, c_vx_zone, "Other") '
            '| stats sum(*) as * by c_vx_zone '
            '| rename x as "95th percentile throughput (bps)", s as #samples c_vx_zone as "Tier"'
            '| fillnull | fields - _* NULL VALUE')

    def test_network_response_code_success_failure_split_query_timechart(self):
        """Test network response code success / failure split.
        The query can only use the main index, but results in three separate
        tstats queries, once for the @c cdn_del_access source type, two for
        @c cdn_del_acquisition:
        - <b>Main Index (access) + Main Index (acquiry)</b> (with Split)

        This is then combined with a chart query to present the results, as
        presented in the following list. This test checks the highlighted
        scenario:
        - <b>time chart with split + time chart postfix + query postfix</b>
        """
        self.invoke_build_search_query(
            JsonKeys.METRIC_REQUESTS_TRANSFERRED, "10days", "0days", "1day",
            split_by={JsonKeys.SPLIT_BY_NETWORK_RESPONSE: (
                JsonKeys.SPLIT_BY_NETWORK_RESPONSE_SUCCESS_FAILURE)})
        self.check_query(
            '| tstats count as x where index="cdn_main" sourcetype=cdn_del_access'
            '  earliest=03/16/2014:00:00:00 latest=03/26/2014:00:00:00'
            '  c_vx_zone="external" (host=4 OR host=5 OR host=2) by _time span=1d,'
            '  sc_status'
            '| append [ tstats count as x where index="cdn_main"'
            '  sourcetype=cdn_del_access earliest=03/16/2014:00:00:00'
            '  latest=03/26/2014:00:00:00 c_vx_zone="external" '
            '  (host=4 OR host=5 OR host=2) NOT sc_status="*" by _time span=1d ]'
            '| append [ tstats count as x where index="cdn_main"'
            '  sourcetype=cdn_del_acquisition earliest=03/16/2014:00:00:00'
            '  latest=03/26/2014:00:00:00 r_vx_zone="cdn-origin" OR'
            '  r_vx_zone="external" OR r_vx_zone="cdn" (host=4 OR host=5 OR host=2)'
            '  by _time span=1d, s_flags, rs_status ]'
            '| append [ tstats count as x where index="cdn_main"'
            '  sourcetype=cdn_del_acquisition earliest=03/16/2014:00:00:00'
            '  latest=03/26/2014:00:00:00 r_vx_zone="cdn-origin" OR'
            '  r_vx_zone="external" OR r_vx_zone="cdn" (host=4 OR host=5 OR host=2)'
            '  NOT s_flags="*" by _time span=1d, rs_status ]'
            '| eval x=x'
            '| lookup http_status_codes status AS sc_status OUTPUTNEW status_type'
            '  AS access_status_type, status_ok AS access_status_ok'
            '| lookup http_status_codes status AS rs_status OUTPUTNEW status_type'
            '  AS http_status_type, status_ok AS http_status_ok'
            '| eval http_status_type = coalesce( http_status_type,'
            '  access_status_type, "Non-HTTP")'
            '| eval http_status_ok = coalesce( http_status_ok, access_status_ok,'
            '  "Success")'
            '| eval http_status_code = coalesce( rs_status, sc_status, "Non-HTTP")'
            '| eval receiver_network_status=substr(s_flags, 1, 1)'
            '| fillnull value="-" receiver_network_status'
            '| eval receiver_network_status = if(receiver_network_status == "r",'
            '  "-" , receiver_network_status)'
            '| lookup network_status_codes status AS receiver_network_status'
            '  OUTPUTNEW status_title AS network_status_title, status_type AS'
            '  network_status_type, status_ok AS network_status_ok'
            '| fillnull value="Unrecognised network error" network_status_title'
            '| fillnull value="Network Error" network_status_type'
            '| fillnull value="Failure" network_status_ok'
            '| eval status_type =if(receiver_network_status == "-" ,'
            '  http_status_type, network_status_type)'
            '| eval status_ok =if(receiver_network_status == "-" , http_status_ok,'
            '  network_status_ok)'
            '| eval status_code =if(receiver_network_status == "-", '
            'http_status_code, network_status_title)'
            '| append [gentimes start=03/16/2014:00:00:00 end=03/26/2014:00:00:00'
            '  increment=1d'
            '| rename starttime as _time'
            '| fields _time'
            '| eval x=0'
            '| eval status_ok=NULL]'
            '| timechart span=1d limit=12 sum(x) as "Request count" by status_ok'
            '| convert timeformat="%Y-%m-%dT%H:%M:%SZ" ctime(_time)'
            '| sort _time'
            '| rename _time as Date'
            '| fillnull'
            '| fields - _* NULL VALUE')

    def test_network_response_code_class_split_query_timechart(self):
        """Test network response code class split.
        The query can only use the main index, but results in three separate
        tstats queries, once for the @c cdn_del_access source type, two for
        @c cdn_del_acquisition:
        - <b>Main Index (access) + Main Index (acquiry)</b> (with Split)

        This is then combined with a chart query to present the results, as
        presented in the following list. This test checks the highlighted
        scenario:
        - <b>time chart with split + time chart postfix + query postfix</b>
        """
        self.invoke_build_search_query(
            JsonKeys.METRIC_REQUESTS_TRANSFERRED, "10days", "0days", "1day",
            split_by={JsonKeys.SPLIT_BY_NETWORK_RESPONSE: (
                JsonKeys.SPLIT_BY_NETWORK_RESPONSE_CODE_CLASS)})
        self.check_query(
            '| tstats count as x where index="cdn_main" sourcetype=cdn_del_access'
            '  earliest=03/16/2014:00:00:00 latest=03/26/2014:00:00:00'
            '  c_vx_zone="external" (host=4 OR host=5 OR host=2) by _time span=1d,'
            '  sc_status'
            '| append [ tstats count as x where index="cdn_main"'
            '  sourcetype=cdn_del_access earliest=03/16/2014:00:00:00'
            '  latest=03/26/2014:00:00:00 c_vx_zone="external"'
            '  (host=4 OR host=5 OR host=2) NOT sc_status="*" by _time span=1d ]'
            '| append [ tstats count as x where index="cdn_main"'
            '  sourcetype=cdn_del_acquisition earliest=03/16/2014:00:00:00'
            '  latest=03/26/2014:00:00:00 r_vx_zone="cdn-origin" OR'
            '  r_vx_zone="external" OR r_vx_zone="cdn" (host=4 OR host=5 OR host=2)'
            '  by _time span=1d, s_flags, rs_status ]'
            '| append [ tstats count as x where index="cdn_main"'
            '  sourcetype=cdn_del_acquisition earliest=03/16/2014:00:00:00'
            '  latest=03/26/2014:00:00:00 r_vx_zone="cdn-origin" OR'
            '  r_vx_zone="external" OR r_vx_zone="cdn" (host=4 OR host=5 OR host=2)'
            '  NOT s_flags="*" by _time span=1d, rs_status ]'
            '| eval x=x'
            '| lookup http_status_codes status AS sc_status OUTPUTNEW status_type'
            '  AS access_status_type, status_ok AS access_status_ok'
            '| lookup http_status_codes status AS rs_status OUTPUTNEW status_type'
            '  AS http_status_type, status_ok AS http_status_ok'
            '| eval http_status_type = coalesce( http_status_type,'
            '  access_status_type, "Non-HTTP")'
            '| eval http_status_ok = coalesce( http_status_ok, access_status_ok,'
            '  "Success")'
            '| eval http_status_code = coalesce( rs_status, sc_status, "Non-HTTP")'
            '| eval receiver_network_status=substr(s_flags, 1, 1)'
            '| fillnull value="-" receiver_network_status'
            '| eval receiver_network_status = if(receiver_network_status == "r",'
            '  "-" , receiver_network_status)'
            '| lookup network_status_codes status AS receiver_network_status'
            '  OUTPUTNEW status_title AS network_status_title, status_type AS'
            '  network_status_type, status_ok AS network_status_ok'
            '| fillnull value="Unrecognised network error" network_status_title'
            '| fillnull value="Network Error" network_status_type'
            '| fillnull value="Failure" network_status_ok'
            '| eval status_type =if(receiver_network_status == "-" ,'
            '  http_status_type, network_status_type)'
            '| eval status_ok =if(receiver_network_status == "-" , http_status_ok,'
            '  network_status_ok)'
            '| eval status_code =if(receiver_network_status == "-", '
            'http_status_code, network_status_title)'
            '| append [gentimes start=03/16/2014:00:00:00 end=03/26/2014:00:00:00'
            '  increment=1d'
            '| rename starttime as _time'
            '| fields _time'
            '| eval x=0'
            '| eval status_type=NULL]'
            '| timechart span=1d limit=12 sum(x) as "Request count" by status_type'
            '| convert timeformat="%Y-%m-%dT%H:%M:%SZ" ctime(_time)'
            '| sort _time'
            '| rename _time as Date'
            '| fillnull'
            '| fields - _* NULL VALUE')

    def test_network_response_code_split_query_timechart(self):
        """Test network response code split.
        The query can only use the main index, but results in three separate
        tstats queries, once for the @c cdn_del_access source type, two for
        @c cdn_del_acquisition:
        - <b>Main Index (access) + Main Index (acquiry)</b> (with Split)

        This is then combined with a chart query to present the results, as
        presented in the following list. This test checks the highlighted
        scenario:
        - <b>time chart with split + time chart postfix + query postfix</b>
        """
        self.invoke_build_search_query(
            JsonKeys.METRIC_REQUESTS_TRANSFERRED, "10days", "0days", "1day",
            split_by={JsonKeys.SPLIT_BY_NETWORK_RESPONSE: (
                JsonKeys.SPLIT_BY_NETWORK_RESPONSE_CODE)})
        self.check_query(
            '| tstats count as x where index="cdn_main" sourcetype=cdn_del_access'
            '  earliest=03/16/2014:00:00:00 latest=03/26/2014:00:00:00'
            '  c_vx_zone="external" (host=4 OR host=5 OR host=2) by _time span=1d,'
            '  sc_status'
            '| append [ tstats count as x where index="cdn_main"'
            ' sourcetype=cdn_del_access earliest=03/16/2014:00:00:00'
            ' latest=03/26/2014:00:00:00 c_vx_zone="external" (host=4 OR host=5 OR'
            ' host=2) NOT sc_status="*" by _time span=1d ]'
            '| append [ tstats count as x where index="cdn_main"'
            '  sourcetype=cdn_del_acquisition earliest=03/16/2014:00:00:00'
            '  latest=03/26/2014:00:00:00 r_vx_zone="cdn-origin" OR'
            '  r_vx_zone="external" OR r_vx_zone="cdn" (host=4 OR host=5 OR host=2)'
            '  by _time span=1d, s_flags, rs_status ]'
            '| append [ tstats count as x where index="cdn_main"'
            '  sourcetype=cdn_del_acquisition earliest=03/16/2014:00:00:00'
            '  latest=03/26/2014:00:00:00 r_vx_zone="cdn-origin" OR'
            '  r_vx_zone="external" OR r_vx_zone="cdn" (host=4 OR host=5 OR host=2)'
            '  NOT s_flags="*" by _time span=1d, rs_status ]'
            '| eval x=x'
            '| lookup http_status_codes status AS sc_status OUTPUTNEW status_type'
            '  AS access_status_type, status_ok AS access_status_ok'
            '| lookup http_status_codes status AS rs_status OUTPUTNEW status_type'
            '  AS http_status_type, status_ok AS http_status_ok'
            '| eval http_status_type = coalesce( http_status_type,'
            '  access_status_type, "Non-HTTP")'
            '| eval http_status_ok = coalesce( http_status_ok, access_status_ok,'
            '  "Success")'
            '| eval http_status_code = coalesce( rs_status, sc_status, "Non-HTTP")'
            '| eval receiver_network_status=substr(s_flags, 1, 1)'
            '| fillnull value="-" receiver_network_status'
            '| eval receiver_network_status = if(receiver_network_status == "r",'
            '  "-" , receiver_network_status)'
            '| lookup network_status_codes status AS receiver_network_status'
            '  OUTPUTNEW status_title AS network_status_title, status_type AS'
            '  network_status_type, status_ok AS network_status_ok'
            '| fillnull value="Unrecognised network error" network_status_title'
            '| fillnull value="Network Error" network_status_type'
            '| fillnull value="Failure" network_status_ok'
            '| eval status_type =if(receiver_network_status == "-" ,'
            '  http_status_type, network_status_type)'
            '| eval status_ok =if(receiver_network_status == "-" , http_status_ok,'
            '  network_status_ok)'
            '| eval status_code =if(receiver_network_status == "-", '
            'http_status_code, network_status_title)'
            '| append [gentimes start=03/16/2014:00:00:00 end=03/26/2014:00:00:00'
            '  increment=1d'
            '| rename starttime as _time'
            '| fields _time'
            '| eval x=0'
            '| eval status_code=NULL]'
            '| timechart span=1d limit=12 sum(x) as "Request count" by status_code'
            '| convert timeformat="%Y-%m-%dT%H:%M:%SZ" ctime(_time)'
            '| sort _time'
            '| rename _time as Date'
            '| fillnull'
            '| fields - _* NULL VALUE')

    def test_invalid_filter_combination(self):
        """CDN tier and network response filters can be mutually exclusive.

        This scenario tests using both filters in a way that results in an
        error message; network response errors are only available in the
        acquisition tier so any CDN filter that doesn't include this tier
        raises the error.

        """
        with self.assertRaises(errors.PropertyValueError) as cm:
            self.invoke_build_search_query(
                JsonKeys.METRIC_REQUESTS_TRANSFERRED, "10days", "0days", "1day",
                tier_filter=[JsonKeys.FILTER_BY_CDN_TIER_DELIVERY],
                network_response_filter=[
                    JsonKeys.FILTER_BY_NETWORK_RESPONSE_NETWORK_ERROR])
        err = cm.exception
        self.assertEquals(err.code, 400)
        self.assertEquals(err.error_code, 4004)  # PropertyValue error
        self.assertEquals(
            err.error_description,
            'For this specific combination of filters it is not possible to '
            'return any data and no report can be run. Please consult the '
            'documentation.')
        self.assertEquals(err.error_details, {'property': ['filterBy']})

    def test_network_response_failures_filter(self):
        """Test filter by network response failures.
        The query can only use the main index, but the metric normally uses two
        source types @c cdn_del_access and @c cdn_del_acquisition. However, when
        a filter some of the tstat queries required to split the data can
        be omitted. This test checks the highlighted scenario:
        - <b>Main index with non-HTTP tstats query removed</b>
        - Main index with non-HTTP and HTTP tstats queries removed
        - Main index with HTTP and network error tstats queries removed

        This is then combined with a chart query to present the results, as
        presented in the following list. This test checks the highlighted
        scenario:
        - <b>time chart (no split) + time chart postfix + query postfix</b>
        """
        self.invoke_build_search_query(
            JsonKeys.METRIC_BYTES_TRANSFERRED, "2days", "0days", "1hour",
            network_response_filter=[
                JsonKeys.FILTER_BY_NETWORK_RESPONSE_FAILURE])
        self.check_query(
            '| tstats sum(sc_bytes) as x where index="cdn_main"'
            '  sourcetype=cdn_del_access earliest=03/24/2014:00:00:00'
            '  latest=03/26/2014:00:00:00 c_vx_zone="external" (host=4 OR host=5 OR'
            '  host=2) by _time span=1h, sc_status'
            '| append [ tstats sum(rs_bytes) as x where index="cdn_main"'
            '  sourcetype=cdn_del_acquisition earliest=03/24/2014:00:00:00'
            '  latest=03/26/2014:00:00:00 r_vx_zone="cdn-origin" OR'
            '  r_vx_zone="external" OR r_vx_zone="cdn" (host=4 OR host=5 OR host=2)'
            '  by _time span=1h, s_flags, rs_status ]'
            '| append [ tstats sum(rs_bytes) as x where index="cdn_main"'
            '  sourcetype=cdn_del_acquisition earliest=03/24/2014:00:00:00'
            '  latest=03/26/2014:00:00:00 r_vx_zone="cdn-origin" OR'
            '  r_vx_zone="external" OR r_vx_zone="cdn" (host=4 OR host=5 OR host=2)'
            '  NOT s_flags="*" by _time span=1h, rs_status ]'
            '| eval x=x'
            '| lookup http_status_codes status AS sc_status OUTPUTNEW status_type'
            '  AS access_status_type, status_ok AS access_status_ok'
            '| lookup http_status_codes status AS rs_status OUTPUTNEW status_type'
            '  AS http_status_type, status_ok AS http_status_ok'
            '| eval http_status_type = coalesce( http_status_type,'
            '  access_status_type, "Non-HTTP" )'
            '| eval http_status_ok = coalesce( http_status_ok, access_status_ok,'
            '  "Success" )'
            '| eval http_status_code = coalesce( rs_status, sc_status, "Non-HTTP" )'
            '| eval receiver_network_status=substr(s_flags, 1, 1)'
            '| fillnull value="-" receiver_network_status'
            '| eval receiver_network_status = if(receiver_network_status == "r",'
            '  "-" , receiver_network_status)'
            '| lookup network_status_codes status AS receiver_network_status'
            '  OUTPUTNEW status_title AS network_status_title, status_type AS'
            '  network_status_type, status_ok AS network_status_ok'
            '| fillnull value="Unrecognised network error" network_status_title'
            '| fillnull value="Network Error" network_status_type'
            '| fillnull value="Failure" network_status_ok'
            '| eval status_type =if(receiver_network_status == "-" ,'
            '  http_status_type, network_status_type)'
            '| eval status_ok =if(receiver_network_status == "-" , http_status_ok,'
            '  network_status_ok)'
            '| eval status_code =if(receiver_network_status == "-" ,'
            '  http_status_code, network_status_title)'
            '| search (status_ok = "Failure" )'
            '| append [gentimes start=03/24/2014:00:00:00 end=03/26/2014:00:00:00'
            '  increment=1h'
            '| rename starttime as _time'
            '| fields _time'
            '| eval x=0 ]'
            '| timechart span=1h limit=12 sum(x) as "Bytes transferred"'
            '| convert timeformat="%Y-%m-%dT%H:%M:%SZ" ctime(_time)'
            '| sort _time'
            '| rename _time as Time'
            '| fillnull'
            '| fields - _* NULL VALUE')

    def test_network_response_network_error_filter(self):
        """Test filter by network response network errors.
        The query can only use the main index, but the metric normally uses two
        source types @c cdn_del_access and @c cdn_del_acquisition. However, when
        a filter some of the tstat queries required to split the data can
        be omitted. This test checks the highlighted scenario:
        - Main index with non-HTTP tstats query removed
        - <b>Main index with non-HTTP and HTTP tstats queries removed</b>
        - Main index with HTTP and network error tstats queries removed

        This is then combined with a chart query to present the results, as
        presented in the following list. This test checks the highlighted
        scenario:
        - <b>time chart (no split) + time chart postfix + query postfix</b>
        """
        self.invoke_build_search_query(
            JsonKeys.METRIC_BYTES_TRANSFERRED, "2days", "0days", "1hour",
            network_response_filter=[
                JsonKeys.FILTER_BY_NETWORK_RESPONSE_NETWORK_ERROR])
        self.check_query(
            '| tstats sum(rs_bytes) as x where index="cdn_main"'
            '  sourcetype=cdn_del_acquisition earliest=03/24/2014:00:00:00'
            '  latest=03/26/2014:00:00:00 r_vx_zone="cdn-origin" OR'
            '  r_vx_zone="external" OR r_vx_zone="cdn" (host=4 OR host=5 OR host=2)'
            '  by _time span=1h, s_flags, rs_status'
            '| eval x=x'
            '| lookup http_status_codes status AS sc_status OUTPUTNEW status_type'
            '  AS access_status_type, status_ok AS access_status_ok'
            '| lookup http_status_codes status AS rs_status OUTPUTNEW status_type'
            '  AS http_status_type, status_ok AS http_status_ok'
            '| eval http_status_type = coalesce( http_status_type,'
            '  access_status_type, "Non-HTTP" )'
            '| eval http_status_ok = coalesce( http_status_ok, access_status_ok,'
            '  "Success" )'
            '| eval http_status_code = coalesce( rs_status, sc_status, "Non-HTTP" )'
            '| eval receiver_network_status=substr(s_flags, 1, 1)'
            '| fillnull value="-" receiver_network_status'
            '| eval receiver_network_status = if(receiver_network_status == "r",'
            '  "-" , receiver_network_status)'
            '| lookup network_status_codes status AS receiver_network_status'
            '  OUTPUTNEW status_title AS network_status_title, status_type AS'
            '  network_status_type, status_ok AS network_status_ok'
            '| fillnull value="Unrecognised network error" network_status_title'
            '| fillnull value="Network Error" network_status_type'
            '| fillnull value="Failure" network_status_ok'
            '| eval status_type =if(receiver_network_status == "-" ,'
            '  http_status_type, network_status_type)'
            '| eval status_ok =if(receiver_network_status == "-" , http_status_ok,'
            '  network_status_ok)'
            '| eval status_code =if(receiver_network_status == "-" ,'
            '  http_status_code, network_status_title)'
            '| search (status_type = "Network Error" )'
            '| append [gentimes start=03/24/2014:00:00:00 end=03/26/2014:00:00:00'
            '  increment=1h'
            '| rename starttime as _time'
            '| fields _time'
            '| eval x=0 ]'
            '| timechart span=1h limit=12 sum(x) as "Bytes transferred"'
            '| convert timeformat="%Y-%m-%dT%H:%M:%SZ" ctime(_time)'
            '| sort _time'
            '| rename _time as Time'
            '| fillnull'
            '| fields - _* NULL VALUE')

    def test_network_response_non_http_filter(self):
        """Test filter by network response non-http events.
        The query can only use the main index, but the metric normally uses two
        source types @c cdn_del_access and @c cdn_del_acquisition. However, when
        a filter some of the tstat queries required to split the data can
        be omitted. This test checks the highlighted scenario:
        - Main index with non-HTTP tstats query removed
        - Main index with non-HTTP and HTTP tstats queries removed
        - <b>Main index with HTTP and network error tstats queries removed</b>

        This is then combined with a chart query to present the results, as
        presented in the following list. This test checks the highlighted
        scenario:
        - <b>time chart (no split) + time chart postfix + query postfix</b>
        """
        self.invoke_build_search_query(
            JsonKeys.METRIC_BYTES_TRANSFERRED, "2days", "0days", "1hour",
            network_response_filter=[
                JsonKeys.FILTER_BY_NETWORK_RESPONSE_NON_HTTP])
        self.check_query(
            '| tstats sum(sc_bytes) as x where index="cdn_main" '
            '  sourcetype=cdn_del_access  earliest=03/24/2014:00:00:00 '
            '  latest=03/26/2014:00:00:00 c_vx_zone="external" (host=4 OR host=5 OR'
            '  host=2) NOT sc_status="*" by _time span=1h'
            '| eval x=x'
            '| lookup http_status_codes status AS sc_status OUTPUTNEW status_type'
            '  AS access_status_type, status_ok AS access_status_ok'
            '| lookup http_status_codes status AS rs_status OUTPUTNEW status_type'
            '  AS http_status_type, status_ok AS http_status_ok'
            '| eval http_status_type = coalesce( http_status_type,'
            '  access_status_type, "Non-HTTP" )'
            '| eval http_status_ok = coalesce( http_status_ok, access_status_ok,'
            '  "Success" )'
            '| eval http_status_code = coalesce( rs_status, sc_status, "Non-HTTP" )'
            '| eval receiver_network_status=substr(s_flags, 1, 1)'
            '| fillnull value="-" receiver_network_status'
            '| eval receiver_network_status = if(receiver_network_status == "r",'
            '  "-" , receiver_network_status)'
            '| lookup network_status_codes status AS receiver_network_status'
            '  OUTPUTNEW status_title AS network_status_title, status_type AS'
            '  network_status_type, status_ok AS network_status_ok'
            '| fillnull value="Unrecognised network error" network_status_title'
            '| fillnull value="Network Error" network_status_type'
            '| fillnull value="Failure" network_status_ok'
            '| eval status_type =if(receiver_network_status == "-" ,'
            '  http_status_type, network_status_type)'
            '| eval status_ok =if(receiver_network_status == "-" , http_status_ok,'
            '  network_status_ok)'
            '| eval status_code =if(receiver_network_status == "-" ,'
            '  http_status_code, network_status_title)'
            '| search (status_type = "Non-HTTP" )'
            '| append [gentimes start=03/24/2014:00:00:00 end=03/26/2014:00:00:00'
            '  increment=1h'
            '| rename starttime as _time'
            '| fields _time'
            '| eval x=0 ]'
            '| timechart span=1h limit=12 sum(x) as "Bytes transferred"'
            '| convert timeformat="%Y-%m-%dT%H:%M:%SZ" ctime(_time)'
            '| sort _time'
            '| rename _time as Time'
            '| fillnull'
            '| fields - _* NULL VALUE')


class TestLinkRendering(SetupUnapiFlaskAppForReporting):
    """Test rendering of report jobs."""

    def check_report_job_collection(self, language=None):
        """Check the rendering of the feed, collection and item.
        @param language - language in which to make the request."""

        # Check an empty collection does not own any jobs
        _, json_msg_body = self.http_get(Profiles.REPORT_JOB_COLLECTION, '/reporting/reports/traffic/jobs', language=language)
        self.assertNotIn(LinkRel.OWNS, json_msg_body[JsonKeys.LINKS], "No jobs should be present")
        self.assertNotIn(LinkRel.ITEM, json_msg_body[JsonKeys.LINKS], "No jobs should be present")
        if language is None:
            self.assertEqual(json_msg_body[JsonKeys.TITLE], "Job Collection for the Traffic analysis Report")
        else:
            self.assertEqual(json_msg_body[JsonKeys.TITLE], "!Job Collection for the !Traffic analysis Report")

        # Check an empty feed does not contain any jobs
        _, json_msg_body = self.http_get(Profiles.REPORT_JOB_FEED, '/reporting/jobs', language=language)
        self.assertNotIn(LinkRel.OWNS, json_msg_body[JsonKeys.LINKS], "No jobs should be present")
        self.assertNotIn(LinkRel.ITEM, json_msg_body[JsonKeys.LINKS], "No jobs should be present")
        if language is None:
            self.assertEqual(json_msg_body[JsonKeys.TITLE], "Report Job Feed")
        else:
            self.assertEqual(json_msg_body[JsonKeys.TITLE], "!Report Job Feed")

        # Create a report job item
        _, json_msg_body = self.http_post(Profiles.REPORT_JOB_ITEM, '/reporting/reports/traffic/jobs', {}, language=language)

        # Check the job item has a self link
        self.check_link_present(json_msg_body, LinkRel.SELF, Profiles.REPORT_JOB_ITEM, "http://localhost/reporting/jobs/arbitrary_name")
        # Check the job item has a link to the owning report job collection
        self.check_link_present(json_msg_body, LinkRel.OWNER, Profiles.REPORT_JOB_COLLECTION, "http://localhost/reporting/reports/traffic/jobs")
        # Check the job item has a link to the report job feed
        self.check_link_present(json_msg_body, LinkRel.COLLECTION, Profiles.REPORT_JOB_FEED, "http://localhost/reporting/jobs")


        # Get the collection now it should own a report job item
        _, json_msg_body = self.http_get(Profiles.REPORT_JOB_COLLECTION, '/reporting/reports/traffic/jobs', language=language)

        # Check the collection has the link to schema
        self.check_link_present(json_msg_body, LinkRel.DESCRIBED_BY, Profiles.JSON_SCHEMA_ITEM)
        # Check the collection has a self link
        self.check_link_present(json_msg_body, LinkRel.SELF, Profiles.REPORT_JOB_COLLECTION)
        # Check the collection has a link to an owned report job
        self.check_link_present(json_msg_body, LinkRel.OWNS, Profiles.REPORT_JOB_ITEM, "http://localhost/reporting/jobs/arbitrary_name")
        # Check the collection does not have any feed style links
        self.assertNotIn(LinkRel.ITEM, json_msg_body[JsonKeys.LINKS], "No jobs should be present")

        # Get the feed now it should contain a report job item
        _, json_msg_body = self.http_get(Profiles.REPORT_JOB_FEED, '/reporting/jobs', language=language)

        # Check the feed has a self link
        self.check_link_present(json_msg_body, LinkRel.SELF, Profiles.REPORT_JOB_FEED)
        # Check the feed has a link to the report job
        self.check_link_present(json_msg_body, LinkRel.ITEM, Profiles.REPORT_JOB_ITEM, "http://localhost/reporting/jobs/arbitrary_name")
        # Check the feed still doesn't own anything
        self.assertNotIn(LinkRel.OWNS, json_msg_body[JsonKeys.LINKS], "No jobs should be present")

    def test_report_job_collection(self):
        """Test the rendering of the feed, collection and item."""
        self.check_report_job_collection()

    def test_report_job_collection_localised(self):
        """Test the rendering of the feed, collection and item using the "test" language code."""
        self.check_report_job_collection(TEST_LANG_CODE)


if __name__ == "__main__":
    # Only show warning and errors.
    logging.basicConfig(level=logging.WARNING, format="%(asctime)s %(name)s: %(levelname)s: %(message)s")
    unittest.main(verbosity=2)
